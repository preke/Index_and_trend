,Issue_id_1,Issue_id_2,Resolution,Title_1,Title_2,same_comp,same_prio,same_tw
0,TST-170,SPARK-290,Not A Problem,Increase SVN Limit for Binary Artifacts to 300MB,Use SPARK_MASTER_IP if it is set in start-slaves.sh.,1,0,1
1,TST-171,SPARK-290,Invalid,JavaPairRDD should support subtractByKey,Use SPARK_MASTER_IP if it is set in start-slaves.sh.,1,0,1
2,TST-168,SPARK-290,Fixed,The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution.,Use SPARK_MASTER_IP if it is set in start-slaves.sh.,1,0,1
3,TST-169,SPARK-290,Unresolved,Spark Stats package: supporting common statistical estimators for Big Data,Use SPARK_MASTER_IP if it is set in start-slaves.sh.,1,0,1
4,SPARK-290,SPARK-291,Fixed,Use SPARK_MASTER_IP if it is set in start-slaves.sh.,Removing credentials line in build.,1,1,1
5,SPARK-291,SPARK-292,Fixed,Removing credentials line in build.,Adding dependency repos in quickstart example,1,1,1
6,SPARK-292,SPARK-293,Fixed,Adding dependency repos in quickstart example,SparkContext.newShuffleId should be public or ShuffleDependency should set its own id,1,1,1
7,SPARK-293,SPARK-294,Fixed,SparkContext.newShuffleId should be public or ShuffleDependency should set its own id,Disable gpg by default.,1,1,1
8,SPARK-294,SPARK-295,Fixed,Disable gpg by default.,Access denied for LATEST_AMI_URL on 0.6 EC2 scripts,1,1,1
9,SPARK-295,SPARK-296,Fixed,Access denied for LATEST_AMI_URL on 0.6 EC2 scripts,Let the user specify environment variables to be passed to the Executors,1,1,1
10,SPARK-296,SPARK-297,Fixed,Let the user specify environment variables to be passed to the Executors,Change block manager to accept a ArrayBuffer,1,1,1
11,SPARK-297,SPARK-298,Fixed,Change block manager to accept a ArrayBuffer,Adding Java documentation,1,1,1
12,SPARK-298,SPARK-299,Fixed,Adding Java documentation,Null pointer exception when RDD size is larger than cache size,1,1,1
13,SPARK-299,SPARK-300,Fixed,Null pointer exception when RDD size is larger than cache size,Adding code for publishing to Sonatype.,1,1,1
14,SPARK-300,SPARK-301,Fixed,Adding code for publishing to Sonatype.,Fixed bug when fetching Jar dependencies.,1,1,1
15,SPARK-301,SPARK-302,Fixed,Fixed bug when fetching Jar dependencies.,Making Spark version configurable in docs and updating Bagel doc,1,1,1
16,SPARK-302,SPARK-303,Fixed,Making Spark version configurable in docs and updating Bagel doc,Support for Hadoop 2 distributions such as cdh4,1,1,1
17,SPARK-303,SPARK-304,Fixed,Support for Hadoop 2 distributions such as cdh4,Add m1.medium node option to cluster management script.,1,1,1
18,SPARK-304,SPARK-305,Fixed,Add m1.medium node option to cluster management script.,Document RDD api (i.e. RDD.scala),1,1,1
19,SPARK-305,SPARK-306,Fixed,Document RDD api (i.e. RDD.scala),Adding documentation to public API's.,1,1,1
20,SPARK-306,SPARK-307,Fixed,Adding documentation to public API's.,Updates docs to use the new version num vars and adds Spark version in nav bar,1,1,1
21,SPARK-307,SPARK-308,Fixed,Updates docs to use the new version num vars and adds Spark version in nav bar,Synchronization bug fix in broadcast implementations,1,1,1
22,SPARK-308,SPARK-309,Fixed,Synchronization bug fix in broadcast implementations,Readding yarn-standalone scheduler scheme,1,1,1
23,SPARK-309,SPARK-310,Fixed,Readding yarn-standalone scheduler scheme,Adds special version variables to docs templating system,1,1,1
24,SPARK-310,SPARK-311,Fixed,Adds special version variables to docs templating system,Adding new download instructions,1,1,1
25,SPARK-311,SPARK-312,Fixed,Adding new download instructions,Removes the annoying small gap above the nav menu dropdown boxes,1,1,1
26,SPARK-312,SPARK-313,Fixed,Removes the annoying small gap above the nav menu dropdown boxes,Removing one link in quickstart,1,1,1
27,SPARK-313,SPARK-314,Fixed,Removing one link in quickstart,Changed the println to logInfo in Utils.fetchFile.,1,1,1
28,SPARK-314,SPARK-315,Fixed,Changed the println to logInfo in Utils.fetchFile.,Adding Sonatype releases to SBT.,1,1,1
29,SPARK-315,SPARK-316,Fixed,Adding Sonatype releases to SBT.,Document Dependency classes and make minor interface improvements,1,1,1
30,SPARK-316,SPARK-317,Fixed,Document Dependency classes and make minor interface improvements,"Fixed a bug in addFile that if the file is specified as ""file:///""; the symlink is created incorrectly for local mode.",1,1,1
31,SPARK-317,SPARK-318,Fixed,"Fixed a bug in addFile that if the file is specified as ""file:///""; the symlink is created incorrectly for local mode.",Move RDD classes/files to their own package/directory,1,1,1
32,SPARK-318,SPARK-319,Fixed,Move RDD classes/files to their own package/directory,Dev,1,1,1
33,SPARK-319,SPARK-320,Fixed,Dev,Fix SizeEstimator tests to work with String classes in JDK 6 and 7,1,1,1
34,SPARK-320,SPARK-321,Fixed,Fix SizeEstimator tests to work with String classes in JDK 6 and 7,Some additions to the Tuning Guide.,1,1,1
35,SPARK-321,SPARK-322,Fixed,Some additions to the Tuning Guide.,Made Serializer and JavaSerializer non private.,1,1,1
36,SPARK-322,SPARK-323,Fixed,Made Serializer and JavaSerializer non private.,Removes the included mesos-0.9.0.jar and pulls it from Maven Central instead,1,1,1
37,SPARK-323,SPARK-324,Fixed,Removes the included mesos-0.9.0.jar and pulls it from Maven Central instead,Changing version of Scala in README,1,1,1
38,SPARK-324,SPARK-325,Fixed,Changing version of Scala in README,First cut at adding documentation for GC tuning,1,1,1
39,SPARK-325,SPARK-326,Fixed,First cut at adding documentation for GC tuning,Package-Private Classes,1,1,1
40,SPARK-326,SPARK-327,Fixed,Package-Private Classes,Allow whitespaces in cluster URL configuration for local cluster.,1,1,1
41,SPARK-327,SPARK-328,Fixed,Allow whitespaces in cluster URL configuration for local cluster.,Don't build spark-repl-assembly in the assembly target by default,1,1,1
42,SPARK-328,SPARK-329,Fixed,Don't build spark-repl-assembly in the assembly target by default,"A Spark ""Quick Start"" example",1,1,1
43,SPARK-329,SPARK-330,Fixed,"A Spark ""Quick Start"" example",publish-local should go to maven + ivy by default,1,1,1
44,SPARK-330,SPARK-331,Fixed,publish-local should go to maven + ivy by default,Publish local maven,1,1,1
45,SPARK-331,SPARK-332,Fixed,Publish local maven,Fixed #232: DirectBuffer's cleaner was empty and Spark tried to invoke clean on it.,1,1,1
46,SPARK-332,SPARK-333,Fixed,Fixed #232: DirectBuffer's cleaner was empty and Spark tried to invoke clean on it.,0.6: NPE in spark.storage.BlockManager,1,1,1
47,SPARK-333,SPARK-334,Fixed,0.6: NPE in spark.storage.BlockManager,"Added a new command ""pl"" in sbt to publish to both Maven and Ivy.",1,1,1
48,SPARK-334,SPARK-335,Fixed,"Added a new command ""pl"" in sbt to publish to both Maven and Ivy.",SizeEstimator gives different sizes for Strings on Java 7,1,1,1
49,SPARK-335,SPARK-336,Fixed,SizeEstimator gives different sizes for Strings on Java 7,Added mapPartitionsWithSplit to the programming guide.,1,1,1
50,SPARK-336,SPARK-337,Fixed,Added mapPartitionsWithSplit to the programming guide.,Allow controlling number of splits in distinct().,1,1,1
51,SPARK-337,SPARK-338,Fixed,Allow controlling number of splits in distinct().,Add a CoalescedRDD for decreasing the number of partitions in a map phase,1,1,1
52,SPARK-338,SPARK-339,Fixed,Add a CoalescedRDD for decreasing the number of partitions in a map phase,Log message which records RDD origin,1,1,1
53,SPARK-339,SPARK-340,Fixed,Log message which records RDD origin,Rename StorageLevels to something easier to remember,1,1,1
54,SPARK-340,SPARK-341,Fixed,Rename StorageLevels to something easier to remember,Added MapPartitionsWithSplitRDD.,1,1,1
55,SPARK-341,SPARK-342,Fixed,Added MapPartitionsWithSplitRDD.,Web UI should display memory in a nicer format,1,1,1
56,SPARK-342,SPARK-343,Fixed,Web UI should display memory in a nicer format,One commit that makes nav dropdowns show on hover,1,1,1
57,SPARK-343,SPARK-344,Fixed,One commit that makes nav dropdowns show on hover,Add spark-shell.cmd,1,1,1
58,SPARK-344,SPARK-345,Fixed,Add spark-shell.cmd,Scripts to start Spark under windows,1,1,1
59,SPARK-345,SPARK-346,Fixed,Scripts to start Spark under windows,Added a method to RDD to expose the ClassManifest.,1,1,1
60,SPARK-346,SPARK-347,Fixed,Added a method to RDD to expose the ClassManifest.,"Logs from ""run"" disappear once you run the tests because test-classes are now on classpath",1,1,1
61,SPARK-347,SPARK-348,Fixed,"Logs from ""run"" disappear once you run the tests because test-classes are now on classpath",HTTP File server fixes,1,1,1
62,SPARK-348,SPARK-349,Fixed,HTTP File server fixes,The --ebs-vol-size option doesn't work with the current EC2 AMI,1,1,1
63,SPARK-349,SPARK-350,Fixed,The --ebs-vol-size option doesn't work with the current EC2 AMI,Log where in the user's code each RDD got created,1,1,1
64,SPARK-350,SPARK-351,Fixed,Log where in the user's code each RDD got created,run,1,1,1
65,SPARK-351,SPARK-352,Fixed,run,Set a limited number of retry in standalone deploy mode.,1,1,1
66,SPARK-352,SPARK-353,Fixed,Set a limited number of retry in standalone deploy mode.,Separated ShuffledRDD into multiple classes.,1,1,1
67,SPARK-353,SPARK-354,Fixed,Separated ShuffledRDD into multiple classes.,RDD.takeSample() produces samples biased toward earlier splits,1,1,1
68,SPARK-354,SPARK-355,Fixed,RDD.takeSample() produces samples biased toward earlier splits,SampledRDD produces incorrect samples when sampling with replacement,1,1,1
69,SPARK-355,SPARK-356,Fixed,SampledRDD produces incorrect samples when sampling with replacement,When a file is downloaded; make it executable.,1,1,1
70,SPARK-356,SPARK-357,Fixed,When a file is downloaded; make it executable.,Java Programming Guide,1,1,1
71,SPARK-357,SPARK-358,Fixed,Java Programming Guide,Updates to docs (including nav structure),1,1,1
72,SPARK-358,SPARK-359,Fixed,Updates to docs (including nav structure),Updates to docs,0,1,1
73,SPARK-359,SPARK-360,Fixed,Updates to docs,YARN and standalone documentation,0,1,1
74,SPARK-360,SPARK-361,Fixed,YARN and standalone documentation,Fix links and make things a bit prettier.,1,1,1
75,SPARK-361,SPARK-362,Fixed,Fix links and make things a bit prettier.,"Adds a ""docs"" directory containing existing Spark documentation and doc build instructions",1,1,1
76,SPARK-362,SPARK-363,Fixed,"Adds a ""docs"" directory containing existing Spark documentation and doc build instructions",Log entire exception (including stack trace) in BlockManagerWorker.,1,1,1
77,SPARK-363,SPARK-364,Fixed,Log entire exception (including stack trace) in BlockManagerWorker.,Spark HTTP FileServer,1,1,1
78,SPARK-364,SPARK-365,Fixed,Spark HTTP FileServer,Don't exit from the Examples since that stops the YARN ApplicationMaster.,1,1,1
79,SPARK-365,SPARK-366,Fixed,Don't exit from the Examples since that stops the YARN ApplicationMaster.,Broken build after typesafe ivy repo changes,1,1,1
80,SPARK-366,SPARK-367,Fixed,Broken build after typesafe ivy repo changes,Log cache add/remove messages in block manager.,1,1,1
81,SPARK-367,SPARK-368,Fixed,Log cache add/remove messages in block manager.,Simulating a Spark standalone cluster locally,1,1,1
82,SPARK-368,SPARK-369,Fixed,Simulating a Spark standalone cluster locally,sbin/mesos-start-cluster.sh      ulimit: error setting limit (Operation not permitted),1,1,1
83,SPARK-369,SPARK-370,Fixed,sbin/mesos-start-cluster.sh      ulimit: error setting limit (Operation not permitted),Spark HTTP FileServer,1,1,1
84,SPARK-370,SPARK-371,Fixed,Spark HTTP FileServer,run spark.examples.SparkPi 127.0.1.1:5050  spark gets error,1,1,1
85,SPARK-371,SPARK-372,Fixed,run spark.examples.SparkPi 127.0.1.1:5050  spark gets error,Disable running combiners on map tasks when mergeCombiners function is not specified by the user.,1,1,1
86,SPARK-372,SPARK-373,Fixed,Disable running combiners on map tasks when mergeCombiners function is not specified by the user.,Disable running combiners on map tasks when mergeCombiners function is not specified by the user.,1,1,1
87,SPARK-373,SPARK-374,Fixed,Disable running combiners on map tasks when mergeCombiners function is not specified by the user.,Add a limit on the number of parallel fetches in the reduce stage,1,1,1
88,SPARK-374,SPARK-375,Fixed,Add a limit on the number of parallel fetches in the reduce stage,Removed the deserialization cache for ShuffleMapTask,1,1,1
89,SPARK-375,SPARK-376,Fixed,Removed the deserialization cache for ShuffleMapTask,Cache points in SparkLR example,1,1,1
90,SPARK-376,SPARK-377,Fixed,Cache points in SparkLR example,Replay debugger for Spark,1,1,1
91,SPARK-377,SPARK-378,Fixed,Replay debugger for Spark,Unresolved dependencies when running sbt to install spark,1,1,1
92,SPARK-378,SPARK-379,Fixed,Unresolved dependencies when running sbt to install spark,add accumulators for mutable collections; with correct typing!,1,1,1
93,SPARK-379,SPARK-380,Fixed,add accumulators for mutable collections; with correct typing!,Make spark-ec2 detect and handle VMs that fail to start correctly,1,1,1
94,SPARK-380,SPARK-381,Fixed,Make spark-ec2 detect and handle VMs that fail to start correctly,make accumulator.localValue public; add tests,1,1,1
95,SPARK-381,SPARK-382,Fixed,make accumulator.localValue public; add tests,Rsync root directory in EC2 script,1,1,1
96,SPARK-382,SPARK-383,Fixed,Rsync root directory in EC2 script,Size estimator changes for dev,1,1,1
97,SPARK-383,SPARK-384,Fixed,Size estimator changes for dev,Launching Spark over YARN,1,1,1
98,SPARK-384,SPARK-385,Fixed,Launching Spark over YARN,Changes to SizeEstimator more accurate,1,1,1
99,SPARK-385,SPARK-386,Fixed,Changes to SizeEstimator more accurate,Use JavaConversion to get a scala iterator,1,1,1
100,SPARK-386,SPARK-387,Fixed,Use JavaConversion to get a scala iterator,Detect non-zero exit status from PipedRDD process,1,1,1
101,SPARK-387,SPARK-388,Fixed,Detect non-zero exit status from PipedRDD process,Avoid a copy in ShuffleMapTask,1,1,1
102,SPARK-388,SPARK-389,Fixed,Avoid a copy in ShuffleMapTask,Fix test checkpoint to reuse spark context defined in the class,1,1,1
103,SPARK-389,SPARK-390,Fixed,Fix test checkpoint to reuse spark context defined in the class,Bug fix in RangePartitioner for partitioning when sorting in descending order.,1,1,1
104,SPARK-390,SPARK-391,Fixed,Bug fix in RangePartitioner for partitioning when sorting in descending order.,Use maxMemory to better estimate memory available for BlockManager cache,1,1,1
105,SPARK-391,SPARK-392,Fixed,Use maxMemory to better estimate memory available for BlockManager cache,Use sbt mergeStrategy for reference.conf files.,1,1,1
106,SPARK-392,SPARK-393,Fixed,Use sbt mergeStrategy for reference.conf files.,Standalone cluster scripts,1,1,1
107,SPARK-393,SPARK-394,Fixed,Standalone cluster scripts,Spark WebUI,1,1,1
108,SPARK-394,SPARK-395,Fixed,Spark WebUI,Merge Akka reference.conf files in sbt assembly task,1,1,1
109,SPARK-395,SPARK-396,Fixed,Merge Akka reference.conf files in sbt assembly task,Logging Throwables in Info and Debug,1,1,1
110,SPARK-396,SPARK-397,Fixed,Logging Throwables in Info and Debug,Support for external sorting and hashing,1,1,1
111,SPARK-397,SPARK-398,Fixed,Support for external sorting and hashing,Support for external hashing and sorting,1,1,1
112,SPARK-398,SPARK-399,Fixed,Support for external hashing and sorting,Broadcast UUID cannot be casted to Integer,1,1,1
113,SPARK-399,SPARK-400,Fixed,Broadcast UUID cannot be casted to Integer,Java API,1,1,1
114,SPARK-400,SPARK-401,Fixed,Java API,Always destroy SparkContext in after block for the unit tests.,1,1,1
115,SPARK-401,SPARK-402,Fixed,Always destroy SparkContext in after block for the unit tests.,Examples ship to to cluster,1,1,1
116,SPARK-402,SPARK-403,Fixed,Examples ship to to cluster,Failing Test: FileSuite - Read SequenceFile using new Hadoop API,1,1,1
117,SPARK-403,SPARK-404,Fixed,Failing Test: FileSuite - Read SequenceFile using new Hadoop API,Instantiating custom serializer using user's classpath,1,1,1
118,SPARK-404,SPARK-405,Fixed,Instantiating custom serializer using user's classpath,Use test fixtures or setup/teardown methods in unit tests,1,1,1
119,SPARK-405,SPARK-406,Fixed,Use test fixtures or setup/teardown methods in unit tests,Broadcast refactoring/cleaning up,1,1,1
120,SPARK-406,SPARK-407,Fixed,Broadcast refactoring/cleaning up,add Accumulatable; add corresponding docs & tests for accumulators,1,1,1
121,SPARK-407,SPARK-408,Fixed,add Accumulatable; add corresponding docs & tests for accumulators,User's JARs are not on the classpath when instantiating custom serializer,1,1,1
122,SPARK-408,SPARK-409,Fixed,User's JARs are not on the classpath when instantiating custom serializer,Include Shark on default Spark AMI,1,1,1
123,SPARK-409,SPARK-410,Fixed,Include Shark on default Spark AMI,Scalacheck groupId has changed https://github.com/rickynils/scalacheck/i...,1,1,1
124,SPARK-410,SPARK-411,Fixed,Scalacheck groupId has changed https://github.com/rickynils/scalacheck/i...,The default broadcast implementation should not use HDFS,1,1,1
125,SPARK-411,SPARK-412,Fixed,The default broadcast implementation should not use HDFS,run spark.examples.SparkPi  hangs with no results,1,1,1
126,SPARK-412,SPARK-413,Fixed,run spark.examples.SparkPi  hangs with no results,Standalone deploy mode,1,1,1
127,SPARK-413,SPARK-414,Fixed,Standalone deploy mode,SizeEstimator's sampling should reuse SearchState,1,1,1
128,SPARK-414,SPARK-415,Fixed,SizeEstimator's sampling should reuse SearchState,Make spark.repl.Main.interp_ publicly accessible,1,1,1
129,SPARK-415,SPARK-416,Fixed,Make spark.repl.Main.interp_ publicly accessible,ShuffleManager & RDD refactored. Some unit tests added.,1,1,1
130,SPARK-416,SPARK-417,Fixed,ShuffleManager & RDD refactored. Some unit tests added.,BoundedMemoryCache.put should fail when estimated size of 'value' is larger than cache capacity,1,1,1
131,SPARK-417,SPARK-418,Fixed,BoundedMemoryCache.put should fail when estimated size of 'value' is larger than cache capacity,Little refactoring and unit tests for CacheTrackerActor,1,1,1
132,SPARK-418,SPARK-419,Fixed,Little refactoring and unit tests for CacheTrackerActor,Return size estimation; cache usage; and cache capacity from slave nodes to CacheTracker,1,1,1
133,SPARK-419,SPARK-420,Fixed,Return size estimation; cache usage; and cache capacity from slave nodes to CacheTracker,Force serialize/deserialize task results in local execution mode.,1,1,1
134,SPARK-420,SPARK-421,Fixed,Force serialize/deserialize task results in local execution mode.,Update spark-yarn project to support newest version of YARN,1,1,1
135,SPARK-421,SPARK-422,Fixed,Update spark-yarn project to support newest version of YARN,End task instead of just exiting in LocalScheduler for tasks that throw exceptions,1,1,1
136,SPARK-422,SPARK-423,Fixed,End task instead of just exiting in LocalScheduler for tasks that throw exceptions,Spark executor should use Mesos-provided hostname for itself in cache tracker updates; etc,1,1,1
137,SPARK-423,SPARK-424,Fixed,Spark executor should use Mesos-provided hostname for itself in cache tracker updates; etc,Fix issues with JAR server in mesos-0.9 branch,1,1,1
138,SPARK-424,SPARK-425,Fixed,Fix issues with JAR server in mesos-0.9 branch,Added the ability to set environmental variables in piped rdd.,1,1,1
139,SPARK-425,SPARK-426,Fixed,Added the ability to set environmental variables in piped rdd.,Added an option (spark.closure.serializer) to specify the serializer for closures.,1,1,1
140,SPARK-426,SPARK-427,Fixed,Added an option (spark.closure.serializer) to specify the serializer for closures.,Report entry dropping in BoundedMemoryCache,1,1,1
141,SPARK-427,SPARK-428,Fixed,Report entry dropping in BoundedMemoryCache,Update the examples to show how to ship the job's code to a cluster,1,1,1
142,SPARK-428,SPARK-429,Fixed,Update the examples to show how to ship the job's code to a cluster,spark.master.host should be set to local IP address instead of hostname,1,1,1
143,SPARK-429,SPARK-430,Fixed,spark.master.host should be set to local IP address instead of hostname,Arthur: Replay debugger for Spark,1,1,1
144,SPARK-430,SPARK-431,Fixed,Arthur: Replay debugger for Spark,Update to work with Mesos 0.9 / 1.0 API,1,1,1
145,SPARK-431,SPARK-432,Fixed,Update to work with Mesos 0.9 / 1.0 API,Changed HadoopRDD to get key and value containers from the RecordReader instead of through reflection,1,1,1
146,SPARK-432,SPARK-433,Fixed,Changed HadoopRDD to get key and value containers from the RecordReader instead of through reflection,Adding sorting to RDDs,1,1,1
147,SPARK-433,SPARK-434,Fixed,Adding sorting to RDDs,Bad behavior when saving to S3,1,1,1
148,SPARK-434,SPARK-435,Fixed,Bad behavior when saving to S3,LocalFileShuffle should not be an object,1,1,1
149,SPARK-435,SPARK-436,Fixed,LocalFileShuffle should not be an object,Added immutable map registration in kryo serializer,1,1,1
150,SPARK-436,SPARK-437,Fixed,Added immutable map registration in kryo serializer,Add support for fixed sized samples,1,1,1
151,SPARK-437,SPARK-438,Fixed,Add support for fixed sized samples,Using 0 partition RDD in a shuffle causes crashes,1,1,1
152,SPARK-438,SPARK-439,Fixed,Using 0 partition RDD in a shuffle causes crashes,Made improvements to takeSample. Also changed SparkLocalKMeans to SparkKMeans,1,1,1
153,SPARK-439,SPARK-440,Fixed,Made improvements to takeSample. Also changed SparkLocalKMeans to SparkKMeans,"Why ""./run spark.examples.SparkPi 1@localhost:5050"" can't get result of Pi",1,1,1
154,SPARK-440,SPARK-441,Fixed,"Why ""./run spark.examples.SparkPi 1@localhost:5050"" can't get result of Pi",Update documentation for building with Scala 2.8,1,1,1
155,SPARK-441,SPARK-442,Fixed,Update documentation for building with Scala 2.8,Add logging/notification when RDDs are evicted from cache,1,1,1
156,SPARK-442,SPARK-444,Fixed,Add logging/notification when RDDs are evicted from cache,Add an accumulating fold operator,1,1,1
157,SPARK-444,SPARK-445,Fixed,Add an accumulating fold operator,Add a version of sample() that returns a fixed number of elements,1,1,1
158,SPARK-445,SPARK-446,Fixed,Add a version of sample() that returns a fixed number of elements,Spark doesn't return offers if it doesn't have any tasks for them,1,1,1
159,SPARK-446,SPARK-447,Fixed,Spark doesn't return offers if it doesn't have any tasks for them,Bagel unit tests broken after API change,1,1,1
160,SPARK-447,SPARK-448,Fixed,Bagel unit tests broken after API change,Use Akka actors for communication,1,1,1
161,SPARK-448,SPARK-449,Fixed,Use Akka actors for communication,Tab completion doesn't work in REPL with Scala 2.9.1,1,1,1
162,SPARK-449,SPARK-450,Fixed,Tab completion doesn't work in REPL with Scala 2.9.1,Implement a sort() operation / RDD,1,1,1
163,SPARK-450,SPARK-451,Fixed,Implement a sort() operation / RDD,Ranges of Longs don't get split properly by SparkContext.parallelize,1,1,1
164,SPARK-451,SPARK-452,Fixed,Ranges of Longs don't get split properly by SparkContext.parallelize,Wrong documentation for Mesos+Spark integration,1,1,1
165,SPARK-452,SPARK-453,Fixed,Wrong documentation for Mesos+Spark integration,Report exceptions in tasks back to master to simplify debugging,1,1,1
166,SPARK-453,SPARK-454,Fixed,Report exceptions in tasks back to master to simplify debugging,Upgrade to SBT 0.11.0,1,1,1
167,SPARK-454,SPARK-455,Fixed,Upgrade to SBT 0.11.0,Upgrade to Scala 2.9.1.,1,1,1
168,SPARK-455,SPARK-456,Fixed,Upgrade to Scala 2.9.1.,Save RDDs should work for s3n paths,1,1,1
169,SPARK-456,SPARK-457,Fixed,Save RDDs should work for s3n paths,Delete scala-2.9 branch,1,1,1
170,SPARK-457,SPARK-458,Fixed,Delete scala-2.9 branch,Replace DepJar with sbt-assembly plugin,1,1,1
171,SPARK-458,SPARK-459,Fixed,Replace DepJar with sbt-assembly plugin,Enable -optimize in the build,1,1,1
172,SPARK-459,SPARK-460,Fixed,Enable -optimize in the build,Fix code not to use code deprecated in Scala 2.9,1,1,1
173,SPARK-460,SPARK-461,Fixed,Fix code not to use code deprecated in Scala 2.9,Workaround for scalac bug and publishTo configuration,1,1,1
174,SPARK-461,SPARK-462,Fixed,Workaround for scalac bug and publishTo configuration,Add APIs to Serializer for streams of objects of the same type,1,1,1
175,SPARK-462,SPARK-463,Fixed,Add APIs to Serializer for streams of objects of the same type,Kryo serializer sometimes fails to use no-argument constructor,1,1,1
176,SPARK-463,SPARK-464,Fixed,Kryo serializer sometimes fails to use no-argument constructor,ClosureCleaner fails when instantiating with outer,1,1,1
177,SPARK-464,SPARK-465,Fixed,ClosureCleaner fails when instantiating with outer,Port SBT build to SBT 0.10,1,1,1
178,SPARK-465,SPARK-466,Fixed,Port SBT build to SBT 0.10,Enable -optimize in the build,1,1,1
179,SPARK-466,SPARK-467,Fixed,Enable -optimize in the build,Add API for controlling the number of splits on a Hadoop file,1,1,1
180,SPARK-467,SPARK-468,Fixed,Add API for controlling the number of splits on a Hadoop file,Change @serializable to extends Serializable in 2.9 branch,1,1,1
181,SPARK-468,SPARK-469,Fixed,Change @serializable to extends Serializable in 2.9 branch,Functionality to save RDDs to Hadoop files,1,1,1
182,SPARK-469,SPARK-470,Fixed,Functionality to save RDDs to Hadoop files,Implemented RDD.leftOuterJoin and RDD.rightOuterJoin,1,1,1
183,SPARK-470,SPARK-471,Fixed,Implemented RDD.leftOuterJoin and RDD.rightOuterJoin,Add missing test for RDD.groupWith,1,1,1
184,SPARK-471,SPARK-472,Fixed,Add missing test for RDD.groupWith,Better readme,1,1,1
185,SPARK-472,SPARK-473,Fixed,Better readme,Move managedStyle to SparkProject,1,1,1
186,SPARK-473,SPARK-474,Fixed,Move managedStyle to SparkProject,Remove unnecessary toStream calls,1,1,1
187,SPARK-474,SPARK-475,Fixed,Remove unnecessary toStream calls,LocalScheduler should catch Throwable to avoid silent failures,1,1,1
188,SPARK-475,SPARK-476,Fixed,LocalScheduler should catch Throwable to avoid silent failures,Make SparkContext.runJob public,1,1,1
189,SPARK-476,SPARK-477,Fixed,Make SparkContext.runJob public,Remove merged branches,1,1,1
190,SPARK-477,SPARK-478,Fixed,Remove merged branches,Use explicit asInstanceOf instead of misleading unchecked pattern matching.,1,1,1
191,SPARK-478,SPARK-479,Fixed,Use explicit asInstanceOf instead of misleading unchecked pattern matching.,Fix deprecations when compiled with Scala 2.8.1,1,1,1
192,SPARK-479,SPARK-480,Fixed,Fix deprecations when compiled with Scala 2.8.1,Fix deprecations when compiled with Scala 2.8.1,1,1,1
193,SPARK-480,SPARK-481,Fixed,Fix deprecations when compiled with Scala 2.8.1,Support Scala 2.9.x,1,1,1
194,SPARK-481,SPARK-482,Fixed,Support Scala 2.9.x,Bagel: Large-scale graph processing on Spark,1,1,1
195,SPARK-482,SPARK-483,Fixed,Bagel: Large-scale graph processing on Spark,Cache to disk,1,1,1
196,SPARK-483,SPARK-484,Fixed,Cache to disk,Make sure slf4j is initialized before any piece of code tries to use it,1,1,1
197,SPARK-484,SPARK-485,Fixed,Make sure slf4j is initialized before any piece of code tries to use it,Concurrency bug in BlockedShuffle implementations in mos-shuffle-tracked branch,1,1,1
198,SPARK-485,SPARK-486,Fixed,Concurrency bug in BlockedShuffle implementations in mos-shuffle-tracked branch,Passing masterHostAddress to workers is not working,1,1,1
199,SPARK-486,SPARK-487,Fixed,Passing masterHostAddress to workers is not working,Serialization issue,1,1,1
200,SPARK-487,SPARK-488,Fixed,Serialization issue,Replacing the native lzf compression code with ning's lzf library,1,1,1
201,SPARK-488,SPARK-489,Fixed,Replacing the native lzf compression code with ning's lzf library,Pluggable broadcast implementations,1,1,1
202,SPARK-489,SPARK-490,Fixed,Pluggable broadcast implementations,Clean up temporary files after LocalFileShuffle,1,1,1
203,SPARK-490,SPARK-491,Fixed,Clean up temporary files after LocalFileShuffle,Support other serialization mechanisms than Java Serialization,1,1,1
204,SPARK-491,SPARK-492,Fixed,Support other serialization mechanisms than Java Serialization,Add unit tests for shuffle operations,1,1,1
205,SPARK-492,SPARK-493,Fixed,Add unit tests for shuffle operations,Organize broadcast implementations,1,1,1
206,SPARK-493,SPARK-494,Fixed,Organize broadcast implementations,Print milliseconds in the log timestamps,1,1,1
207,SPARK-494,SPARK-495,Fixed,Print milliseconds in the log timestamps,Spark/Mesos crashes after completion of jobs,1,1,1
208,SPARK-495,SPARK-496,Fixed,Spark/Mesos crashes after completion of jobs,java crashes when trying to run spark+EC2,1,1,1
209,SPARK-496,SPARK-497,Fixed,java crashes when trying to run spark+EC2,Increase default locality wait from 1s to 3-5s,1,1,1
210,SPARK-497,SPARK-498,Fixed,Increase default locality wait from 1s to 3-5s,Make Spark use log4j / slf4j for logging,1,1,1
211,SPARK-498,SPARK-499,Fixed,Make Spark use log4j / slf4j for logging,Write up a Spark tutorial for wiki,1,1,1
212,SPARK-499,SPARK-500,Fixed,Write up a Spark tutorial for wiki,Add a single spark.shared.fs option to use for REPL classes; broadcast; etc,1,1,1
213,SPARK-500,SPARK-501,Fixed,Add a single spark.shared.fs option to use for REPL classes; broadcast; etc,Don't keep trying to run tasks on a slave if they all fail,1,1,1
214,SPARK-501,SPARK-502,Fixed,Don't keep trying to run tasks on a slave if they all fail,Make per-task CPU and memory configurable,1,1,1
215,SPARK-502,SPARK-503,Fixed,Make per-task CPU and memory configurable,Make it possible to run spark-shell without a shared NFS,1,1,1
216,SPARK-503,SPARK-504,Fixed,Make it possible to run spark-shell without a shared NFS,Add support for Hadoop InputFormats other than TextInputFormat,1,1,1
217,SPARK-504,SPARK-505,Fixed,Add support for Hadoop InputFormats other than TextInputFormat,Consider using a better build tool than make,1,1,1
218,SPARK-505,SPARK-506,Fixed,Consider using a better build tool than make,Load-balance tasks better across nodes,1,1,1
219,SPARK-506,SPARK-507,Fixed,Load-balance tasks better across nodes,Add save() operation,1,1,1
220,SPARK-507,SPARK-508,Fixed,Add save() operation,Union operation on RDDs,1,1,1
221,SPARK-508,SPARK-509,Fixed,Union operation on RDDs,Merge Mosharaf's broadcast code into master branch,1,1,1
222,SPARK-509,SPARK-510,Fixed,Merge Mosharaf's broadcast code into master branch,Save operation,1,1,1
223,SPARK-510,SPARK-511,Fixed,Save operation,Unify the API and implementation of ParallelArrays and HdfsFiles,1,1,1
224,SPARK-511,SPARK-512,Fixed,Unify the API and implementation of ParallelArrays and HdfsFiles,Shuffle operation,1,1,1
225,SPARK-512,SPARK-513,Fixed,Shuffle operation,Make NexusScheduler more efficient by keeping a list of tasks for each node,1,1,1
226,SPARK-513,SPARK-514,Fixed,Make NexusScheduler more efficient by keeping a list of tasks for each node,Merge fault tolerance code into master branch,1,1,1
227,SPARK-514,SPARK-515,Fixed,Merge fault tolerance code into master branch,Make it easier to run external code on a Nexus cluster,1,1,1
228,SPARK-515,SPARK-516,Fixed,Make it easier to run external code on a Nexus cluster,Improve error reporting when slaves fail to start,0,0,1
229,SPARK-516,SPARK-517,Not A Problem,Improve error reporting when slaves fail to start,Exiting spark-ec2 with unfulfilled spot instance request does not cancel request,0,0,1
230,SPARK-517,SPARK-518,Fixed,Exiting spark-ec2 with unfulfilled spot instance request does not cancel request,Allow EC2 script to stop/destroy cluster after master/slave failures,1,1,1
231,SPARK-518,SPARK-519,Fixed,Allow EC2 script to stop/destroy cluster after master/slave failures,Support for Hadoop 2 distributions such as cdh4,1,1,1
232,SPARK-519,SPARK-520,Fixed,Support for Hadoop 2 distributions such as cdh4,Structure SBT build file into modules.,1,1,1
233,SPARK-520,SPARK-521,Won't Fix,Structure SBT build file into modules.,Create repository for /root/mesos-ec2 scripts,0,1,1
234,SPARK-521,SPARK-522,Fixed,Create repository for /root/mesos-ec2 scripts,Updated Kryo to version 2.20,0,1,1
235,SPARK-522,SPARK-523,Fixed,Updated Kryo to version 2.20,Added a method to report slave memory status; force serialize accumulator update in local mode.,1,1,1
236,SPARK-523,SPARK-524,Fixed,Added a method to report slave memory status; force serialize accumulator update in local mode.,spark integration issue with Cloudera hadoop,1,1,1
237,SPARK-524,SPARK-525,Won't Fix,spark integration issue with Cloudera hadoop,Refactoring of shuffling-related classes,1,1,1
238,SPARK-525,SPARK-526,Fixed,Refactoring of shuffling-related classes,sbt/sbt  --> eclipse won't work now,1,1,1
239,SPARK-526,SPARK-527,Cannot Reproduce,sbt/sbt  --> eclipse won't work now,Support spark-shell when running on YARN,0,1,1
240,SPARK-527,SPARK-528,Fixed,Support spark-shell when running on YARN,Provide a dist-like target that builds a binary distribution (JARs + scripts),0,1,1
241,SPARK-528,SPARK-529,Fixed,Provide a dist-like target that builds a binary distribution (JARs + scripts),Have a single file that controls the environmental variables and spark config options,0,0,1
242,SPARK-529,SPARK-530,Fixed,Have a single file that controls the environmental variables and spark config options,Spark driver process doesn't exit after finishing,0,0,1
243,SPARK-530,SPARK-531,Fixed,Spark driver process doesn't exit after finishing,Create more explicit logs for cache registration and task completion,1,1,1
244,SPARK-531,SPARK-532,Won't Fix,Create more explicit logs for cache registration and task completion,Log something when no resources have been offered in the cluster,1,1,1
245,SPARK-532,SPARK-533,Fixed,Log something when no resources have been offered in the cluster,Killing tasks in spark - request for comment,1,1,1
246,SPARK-534,SPARK-535,Invalid,Make SparkContext thread-safe,Error with technique to find hostname in bin/start-slaves.sh in dev branch,0,0,1
247,SPARK-535,SPARK-536,Fixed,Error with technique to find hostname in bin/start-slaves.sh in dev branch,Set SPARK_MEM based on instance type in EC2 scripts,1,1,1
248,SPARK-536,SPARK-537,Fixed,Set SPARK_MEM based on instance type in EC2 scripts,driver.run() returned with code DRIVER_ABORTED,1,1,1
249,SPARK-537,SPARK-538,Fixed,driver.run() returned with code DRIVER_ABORTED,INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone,1,1,1
250,SPARK-538,SPARK-539,Done,INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone,Permission denied(publickey),1,1,1
251,SPARK-539,SPARK-540,Fixed,Permission denied(publickey),Add API to customize in-memory representation of RDDs,0,0,1
252,SPARK-540,SPARK-541,Not A Problem,Add API to customize in-memory representation of RDDs,Passing bad master address to SparkContext results in unhelpful Mesos error message,0,0,1
253,SPARK-541,SPARK-542,Fixed,Passing bad master address to SparkContext results in unhelpful Mesos error message,Cache Miss when machine have multiple hostname,0,0,1
254,SPARK-542,SPARK-543,Won't Fix,Cache Miss when machine have multiple hostname,Spark for Python,0,0,1
255,SPARK-543,SPARK-544,Fixed,Spark for Python,Provide a Configuration class in addition to system properties,1,1,1
256,SPARK-544,SPARK-545,Fixed,Provide a Configuration class in addition to system properties,support external sort,1,1,1
257,SPARK-546,SPARK-547,Fixed,Support full outer join and multiple join in a single shuffle,Provide a means to package Spark's executor into a tgz,0,1,1
258,SPARK-547,SPARK-548,Fixed,Provide a means to package Spark's executor into a tgz,Update Kryo to use 2.x,1,1,1
259,SPARK-548,SPARK-549,Fixed,Update Kryo to use 2.x,Move mesos.jar to a maven repository,1,1,1
260,SPARK-549,SPARK-550,Fixed,Move mesos.jar to a maven repository,Hiding the default spark context in the spark shell creates serialization issues,1,1,1
261,SPARK-550,SPARK-551,Done,Hiding the default spark context in the spark shell creates serialization issues,slave disconnected,1,1,1
262,SPARK-551,SPARK-552,Incomplete,slave disconnected,spark api doc,1,1,1
263,SPARK-552,SPARK-553,Fixed,spark api doc,Write a tutorial for mining Wikipedia interactively on EC2,0,1,1
264,SPARK-553,SPARK-554,Fixed,Write a tutorial for mining Wikipedia interactively on EC2,Add aggregateByKey,0,1,1
265,SPARK-554,SPARK-555,Fixed,Add aggregateByKey,"Write a ""Spark internals"" wiki page",1,1,1
266,SPARK-555,SPARK-556,Fixed,"Write a ""Spark internals"" wiki page",Results from tasks should be returned through our own sockets rather than Mesos framework messages,1,1,1
267,SPARK-556,SPARK-557,Fixed,Results from tasks should be returned through our own sockets rather than Mesos framework messages,Provide scripts for launching Spark through Amazon Elastic MapReduce,1,1,1
268,SPARK-557,SPARK-558,Fixed,Provide scripts for launching Spark through Amazon Elastic MapReduce,Simplify run script by relying on sbt to launch app,1,1,1
269,SPARK-558,SPARK-559,Won't Fix,Simplify run script by relying on sbt to launch app,Automatically register all classes used in fields of a class with Kryo,1,1,1
270,SPARK-559,SPARK-560,Done,Automatically register all classes used in fields of a class with Kryo,Specialize RDDs / iterators,0,1,1
271,SPARK-560,SPARK-561,Won't Fix,Specialize RDDs / iterators,Implement unit tests for RDD save functionality,0,1,1
272,SPARK-561,SPARK-562,Fixed,Implement unit tests for RDD save functionality,Add a way to ship files with as Spark job,1,1,1
273,SPARK-562,SPARK-563,Fixed,Add a way to ship files with as Spark job,Run findBugs and IDEA inspections in the codebase,1,1,1
274,SPARK-563,SPARK-564,Won't Fix,Run findBugs and IDEA inspections in the codebase,Publish to maven repository,1,1,1
275,SPARK-564,SPARK-565,Fixed,Publish to maven repository,Integrate spark in scala standard collection API,1,1,1
276,SPARK-565,SPARK-566,Won't Fix,Integrate spark in scala standard collection API,Replace polling+sleeping with semaphores in broadcast and shuffle,1,1,1
277,SPARK-566,SPARK-567,Not A Problem,Replace polling+sleeping with semaphores in broadcast and shuffle,Unified directory structure for temporary data,1,1,1
278,SPARK-567,SPARK-568,Incomplete,Unified directory structure for temporary data,Document configuration options,1,1,1
279,SPARK-568,SPARK-569,Fixed,Document configuration options,Consolidate/reorganize configuration options,1,1,1
280,SPARK-570,SPARK-571,Fixed,Easy way to set remote node memory,Forbid return statements when cleaning closures,1,1,1
281,SPARK-571,SPARK-572,Fixed,Forbid return statements when cleaning closures,Forbid update of static mutable variables,1,1,1
282,SPARK-572,SPARK-573,Won't Fix,Forbid update of static mutable variables,Clarify semantics of the parallelized closures,0,0,1
283,SPARK-573,SPARK-574,Won't Fix,Clarify semantics of the parallelized closures,Put license notice and copyrights headers to all files,0,0,1
284,SPARK-574,SPARK-575,Fixed,Put license notice and copyrights headers to all files,Maintain a cache of JARs on each node to avoid unnecessary copying,1,1,1
285,SPARK-575,SPARK-576,Incomplete,Maintain a cache of JARs on each node to avoid unnecessary copying,Design and develop a more precise progress estimator,1,1,1
286,SPARK-576,SPARK-577,Won't Fix,Design and develop a more precise progress estimator,Rename Split to Partition,1,1,1
287,SPARK-577,SPARK-578,Fixed,Rename Split to Partition,Fix interpreter code generation to only capture needed dependencies,0,0,1
288,SPARK-578,SPARK-579,Not A Problem,Fix interpreter code generation to only capture needed dependencies,An anonymous submission to Spark,0,1,1
289,SPARK-579,SPARK-580,Won't Fix,An anonymous submission to Spark,Use Spark local directory as PySpark tmp directory,0,0,1
290,SPARK-580,SPARK-581,Fixed,Use Spark local directory as PySpark tmp directory,Try using a smart commit and seeing if it works,0,0,1
291,SPARK-581,SPARK-582,Fixed,Try using a smart commit and seeing if it works,Add unit test for complex column (lazy column),1,1,1
292,SPARK-582,SPARK-583,Won't Fix,Add unit test for complex column (lazy column),Failures in BlockStore may lead to infinite loops of task failures,0,1,1
293,SPARK-583,SPARK-584,Fixed,Failures in BlockStore may lead to infinite loops of task failures,Pass slave ip address when starting a cluster ,1,0,1
294,SPARK-584,SPARK-585,Incomplete,Pass slave ip address when starting a cluster ,Mesos may not work with mesos:// URLs,0,0,1
295,SPARK-585,SPARK-586,Fixed,Mesos may not work with mesos:// URLs,Update docs to say that you need to set MESOS_NATIVE_LIBRARY when running standalone apps on Mesos,1,1,1
296,SPARK-587,SPARK-588,Fixed,Test issue,Have assembly example in quick start; or elsewhere in docs,0,1,1
297,SPARK-588,SPARK-589,Won't Fix,Have assembly example in quick start; or elsewhere in docs,MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos,1,1,1
298,SPARK-589,SPARK-590,Fixed,MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos,Log task size when it's too large on master,0,1,1
299,SPARK-590,SPARK-591,Fixed,Log task size when it's too large on master,Configuration System,0,0,1
300,SPARK-592,SPARK-593,Fixed,Total memory size on per-RDD basis,Send last few lines of failed standalone mode or Mesos task to master,1,1,1
301,SPARK-593,SPARK-594,Won't Fix,Send last few lines of failed standalone mode or Mesos task to master,Update examples to pass JARs when building a SparkContext in 0.6 and master,0,0,1
302,SPARK-595,SPARK-596,Won't Fix,"Document ""local-cluster"" mode",Memory Dashboard,0,0,1
303,SPARK-596,SPARK-597,Fixed,Memory Dashboard,Unimplemented configuration options in spark-daemon[s].sh,1,1,1
304,SPARK-597,SPARK-598,Fixed,Unimplemented configuration options in spark-daemon[s].sh,Test PySpark's Java-side pickling of arrays of key-value pairs,0,1,1
305,SPARK-598,SPARK-599,Cannot Reproduce,Test PySpark's Java-side pickling of arrays of key-value pairs,OutOfMemoryErrors can cause workers to hang indefinitely,0,1,1
306,SPARK-599,SPARK-600,Fixed,OutOfMemoryErrors can cause workers to hang indefinitely,SparkContext.stop and clearJars delete local JAR files,1,1,1
307,SPARK-600,SPARK-601,Fixed,SparkContext.stop and clearJars delete local JAR files,PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None,0,0,1
308,SPARK-601,SPARK-602,Fixed,PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None,Pass integer hashcodes to Java during PySpark hash partitioning,0,0,1
309,SPARK-602,SPARK-603,Fixed,Pass integer hashcodes to Java during PySpark hash partitioning,add simple Counter API,0,0,1
310,SPARK-603,SPARK-604,Won't Fix,add simple Counter API,reconnect if mesos slaves dies,0,0,1
311,SPARK-604,SPARK-605,Cannot Reproduce,reconnect if mesos slaves dies,Add support for new m3.xlarge and m3.2xlarge EC2 instance types,0,0,1
312,SPARK-605,SPARK-606,Fixed,Add support for new m3.xlarge and m3.2xlarge EC2 instance types,Add mapSideCombine setting to Java API partitionBy() method.,0,1,1
313,SPARK-606,SPARK-607,Won't Fix,Add mapSideCombine setting to Java API partitionBy() method.,Timeout while fetching map statuses may cause job to hang,0,0,1
314,SPARK-607,SPARK-608,Fixed,Timeout while fetching map statuses may cause job to hang,Allow showing worker STDOUT/STDERR log tail (e.g. last 512KB),0,1,1
315,SPARK-608,SPARK-609,Fixed,Allow showing worker STDOUT/STDERR log tail (e.g. last 512KB),Add instructions for enabling Akka debug logging,0,0,1
316,SPARK-609,SPARK-610,Incomplete,Add instructions for enabling Akka debug logging,Support master failover in standalone mode,0,0,1
317,SPARK-610,SPARK-611,Fixed,Support master failover in standalone mode,Allow JStack to be run from web UI,0,1,1
318,SPARK-611,SPARK-612,Fixed,Allow JStack to be run from web UI,Update examples to pass JAR file to SparkContext in master and 0.6 branches,0,1,1
319,SPARK-613,SPARK-614,Fixed,Standalone web UI links to internal IPs when running on EC2,Make last 4 digits of framework id in standalone mode logging monotonically increasing,0,1,1
320,SPARK-614,SPARK-615,Unresolved,Make last 4 digits of framework id in standalone mode logging monotonically increasing,Add mapPartitionsWithIndex() to the Java API,0,0,1
321,SPARK-615,SPARK-616,Fixed,Add mapPartitionsWithIndex() to the Java API,Show dead workers in the standalone web UI,0,1,1
322,SPARK-616,SPARK-617,Fixed,Show dead workers in the standalone web UI,Driver program can crash when a standalone worker is lost,1,0,1
323,SPARK-617,SPARK-618,Fixed,Driver program can crash when a standalone worker is lost,start-mesos and stop-mesos scripts should check for running processes,0,0,1
324,SPARK-618,SPARK-619,Won't Fix,start-mesos and stop-mesos scripts should check for running processes,Hadoop MapReduce should be configured to use all local disks for shuffle on AMI,0,1,1
325,SPARK-619,SPARK-620,Fixed,Hadoop MapReduce should be configured to use all local disks for shuffle on AMI,Default SPARK_MEM on AMI too high,1,1,1
326,SPARK-621,SPARK-622,Fixed,Provide an API to manually throw RDDs out of the cache,spark-ec2 launch command hangs if instances fail while starting up.,1,1,1
327,SPARK-622,SPARK-623,Cannot Reproduce,spark-ec2 launch command hangs if instances fail while starting up.,Don't hardcode log location for standalone UI,1,1,1
328,SPARK-623,SPARK-624,Fixed,Don't hardcode log location for standalone UI,The local IP address to bind to should be configurable,1,1,1
329,SPARK-624,SPARK-625,Fixed,The local IP address to bind to should be configurable,Client hangs when connecting to standalone cluster using wrong address,0,1,1
330,SPARK-625,SPARK-626,Fixed,Client hangs when connecting to standalone cluster using wrong address,deleting security groups gives me a 400 error,0,0,1
331,SPARK-626,SPARK-627,Fixed,deleting security groups gives me a 400 error,All references to [K; V] in JavaDStreamLike should be changed to [K2; V2],1,0,1
332,SPARK-627,SPARK-628,Fixed,All references to [K; V] in JavaDStreamLike should be changed to [K2; V2],Make deletion of EC2 security groups optional,1,0,1
333,SPARK-628,SPARK-629,Fixed,Make deletion of EC2 security groups optional,Standalone job details page has strange value for number of cores,1,0,1
334,SPARK-629,SPARK-630,Fixed,Standalone job details page has strange value for number of cores,Master web UI shows some finished/killed executors as running,1,1,1
335,SPARK-630,SPARK-631,Fixed,Master web UI shows some finished/killed executors as running,SPARK_LOCAL_IP environment variable should also affect spark.master.host,1,0,1
336,SPARK-631,SPARK-632,Fixed,SPARK_LOCAL_IP environment variable should also affect spark.master.host,Akka system names need to be normalized (since they are case-sensitive),1,1,1
337,SPARK-632,SPARK-633,Fixed,Akka system names need to be normalized (since they are case-sensitive),Support dropping blocks and RDDs from block manager,1,1,1
338,SPARK-633,SPARK-634,Fixed,Support dropping blocks and RDDs from block manager,Track and display a read count for each block replica in BlockManager,0,1,1
339,SPARK-634,SPARK-635,Won't Fix,Track and display a read count for each block replica in BlockManager,Pass a TaskContext object to compute() interface,0,1,1
340,SPARK-635,SPARK-636,Fixed,Pass a TaskContext object to compute() interface,Add mechanism to run system management/configuration tasks on all workers,0,1,1
341,SPARK-637,SPARK-638,Later,Create troubleshooting checklist,Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting,0,1,1
342,SPARK-638,SPARK-639,Fixed,Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting,Standalone cluster should report executor exit codes more nicely to clients,1,1,1
343,SPARK-639,SPARK-640,Fixed,Standalone cluster should report executor exit codes more nicely to clients,Update Hadoop 1 version to 1.1.0 (especially on AMIs),1,1,1
344,SPARK-640,SPARK-641,Fixed,Update Hadoop 1 version to 1.1.0 (especially on AMIs),spark-ec2 standalone launch should create ~/mesos-ec2/slaves,0,0,1
345,SPARK-641,SPARK-642,Fixed,spark-ec2 standalone launch should create ~/mesos-ec2/slaves,spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS,1,1,1
346,SPARK-642,SPARK-643,Fixed,spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS,Standalone master crashes during actor restart,0,0,1
347,SPARK-643,SPARK-644,Fixed,Standalone master crashes during actor restart,Jobs canceled due to repeated executor failures may hang,1,1,1
348,SPARK-644,SPARK-645,Fixed,Jobs canceled due to repeated executor failures may hang,Calling distinct() without parentheses fails,1,0,1
349,SPARK-645,SPARK-646,Fixed,Calling distinct() without parentheses fails,Floating point overflow/underflow in LR examples,0,0,1
350,SPARK-646,SPARK-647,Fixed,Floating point overflow/underflow in LR examples,ConnectionManager.sendMessage may create too many unnecessary connections,0,1,1
351,SPARK-647,SPARK-648,Fixed,ConnectionManager.sendMessage may create too many unnecessary connections,takeSample() with repetitions should be able to return more items than an RDD contains,0,0,1
352,SPARK-648,SPARK-649,Fixed,takeSample() with repetitions should be able to return more items than an RDD contains,Windows support for PySpark,0,0,1
353,SPARK-649,SPARK-650,Fixed,Windows support for PySpark,"Add a ""setup hook"" API for running initialization code on each executor",0,0,1
354,SPARK-651,SPARK-652,Fixed,Port sample()/takeSample() to PySpark,Propagate exceptions from PySpark workers to the driver,1,1,1
355,SPARK-653,SPARK-654,Fixed,Add accumulators to PySpark,Use ID of hash function when comparing Python partitioner objects in equals(),1,1,1
356,SPARK-654,SPARK-655,Fixed,Use ID of hash function when comparing Python partitioner objects in equals(),Implement co-partitioning aware joins in PySpark,1,1,1
357,SPARK-656,SPARK-657,Fixed,Let Amazon choose our EC2 clusters' availability zone if the user does not specify one,Don't use multiple loopback IP addresses in unit tests,0,0,1
358,SPARK-657,SPARK-658,Fixed,Don't use multiple loopback IP addresses in unit tests,Make Spark execution time logging more obvious and easier to read,1,0,1
359,SPARK-658,SPARK-659,Fixed,Make Spark execution time logging more obvious and easier to read,The master web interface is broken for Scala 2.10,1,1,1
360,SPARK-659,SPARK-660,Fixed,The master web interface is broken for Scala 2.10,Add StorageLevel support in Python,0,1,1
361,SPARK-660,SPARK-661,Fixed,Add StorageLevel support in Python,Java unit tests don't seem to run with Maven,0,1,1
362,SPARK-661,SPARK-662,Cannot Reproduce,Java unit tests don't seem to run with Maven,Executor should only download files & jars once,1,0,1
363,SPARK-662,SPARK-663,Fixed,Executor should only download files & jars once,Implement Fair scheduler within ClusterScheduler,0,0,1
364,SPARK-663,SPARK-664,Fixed,Implement Fair scheduler within ClusterScheduler,Accumulator updates should get locally merged before sent to the driver,1,0,1
365,SPARK-664,SPARK-665,Won't Fix,Accumulator updates should get locally merged before sent to the driver,Create RPM packages for Spark,0,0,1
366,SPARK-665,SPARK-666,Won't Fix,Create RPM packages for Spark,Make Spark's master debug level logging consumable,0,1,1
367,SPARK-666,SPARK-667,Fixed,Make Spark's master debug level logging consumable,IntelliJ may insert stubs for inherited methods in Java Function classes,0,0,1
368,SPARK-667,SPARK-668,Cannot Reproduce,IntelliJ may insert stubs for inherited methods in Java Function classes,JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors,1,0,1
369,SPARK-668,SPARK-669,Fixed,JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors,Send back task results through BlockManager instead of Akka messages,0,1,1
370,SPARK-669,SPARK-670,Fixed,Send back task results through BlockManager instead of Akka messages,"spark-ec2 should warn if you use the ""start"" command without passing a SSH key file",0,0,1
371,SPARK-670,SPARK-671,Fixed,"spark-ec2 should warn if you use the ""start"" command without passing a SSH key file",Spark runs out of memory on fork/exec (affects both pipes and python),0,0,1
372,SPARK-673,SPARK-674,Fixed,PySpark should capture and re-throw Python exceptions,Gateway JVM's should not be launched on slave,1,1,1
373,SPARK-674,SPARK-675,Fixed,Gateway JVM's should not be launched on slave,Gateway JVM should ask for less than SPARK_MEM memory,1,0,1
374,SPARK-675,SPARK-676,Invalid,Gateway JVM should ask for less than SPARK_MEM memory,Abbreviation in SPARK_MEM but not in SPARK_WORKER_MEMORY,0,0,1
375,SPARK-676,SPARK-677,Won't Fix,Abbreviation in SPARK_MEM but not in SPARK_WORKER_MEMORY,PySpark should not collect results through local filesystem,0,0,1
376,SPARK-677,SPARK-678,Fixed,PySpark should not collect results through local filesystem,Add an example that does a roll-up on log data,0,1,1
377,SPARK-678,SPARK-679,Fixed,Add an example that does a roll-up on log data,Have a DSL or other language support for OLAP expressions,1,1,1
378,SPARK-680,SPARK-681,Cannot Reproduce,broadcast hangs spark cluster,Optimize hashtables used in Spark,1,1,1
379,SPARK-681,SPARK-682,Fixed,Optimize hashtables used in Spark,Memoize results of getPreferredLocations,1,0,1
380,SPARK-683,SPARK-684,Fixed,Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation,Move downloads links on Spark website away from GitHub,0,1,1
381,SPARK-684,SPARK-685,Fixed,Move downloads links on Spark website away from GitHub,Add an environment variable to launch PySpark with ipython,0,0,1
382,SPARK-685,SPARK-686,Fixed,Add an environment variable to launch PySpark with ipython,Port FT heartbeat and fixes from 0.6 branch to master,0,0,1
383,SPARK-686,SPARK-687,Fixed,Port FT heartbeat and fixes from 0.6 branch to master,Use separate SPARK_DAEMON_MEMORY setting in Windows run script too,0,0,1
384,SPARK-687,SPARK-688,Fixed,Use separate SPARK_DAEMON_MEMORY setting in Windows run script too,Task crashed when I do spark stress test,0,0,1
385,SPARK-688,SPARK-689,Incomplete,Task crashed when I do spark stress test,Task will crash when setting SPARK_WORKER_CORES> 128,0,1,1
386,SPARK-689,SPARK-690,Cannot Reproduce,Task will crash when setting SPARK_WORKER_CORES> 128,Stack overflow when running pagerank more than 10000 iterators,1,1,1
387,SPARK-690,SPARK-691,Unresolved,Stack overflow when running pagerank more than 10000 iterators,Infinite recursion in doCheckpoint when running Bagel,0,1,1
388,SPARK-691,SPARK-692,Fixed,Infinite recursion in doCheckpoint when running Bagel,spark-shell doesn't start in 0.6.2,1,0,1
389,SPARK-692,SPARK-693,Done,spark-shell doesn't start in 0.6.2,Let deploy scripts set alternate conf; work directories,1,1,1
390,SPARK-694,SPARK-695,Fixed,[pyspark] operator.getattr not serialized,Exponential recursion in getPreferredLocations,1,1,1
391,SPARK-696,SPARK-697,Fixed,sortByKey(ascending: Boolean) ignores ascending parameter,RDD should be covariant in T,0,1,1
392,SPARK-698,SPARK-699,Fixed,"Spark Standalone Mode is leaving a java process ""spark.executor.StandaloneExecutorBackend"" open on Windows",Add example of reading from HBase,0,1,1
393,SPARK-699,SPARK-700,Fixed,Add example of reading from HBase,Add example of reading from Cassandra,1,1,1
394,SPARK-700,SPARK-701,Fixed,Add example of reading from Cassandra,Wrong SPARK_MEM setting with different EC2 master and worker machine types,0,1,1
395,SPARK-701,SPARK-702,Fixed,Wrong SPARK_MEM setting with different EC2 master and worker machine types,All PairRDDFunctions should accept JFunction (not Function),0,1,1
396,SPARK-702,SPARK-703,Fixed,All PairRDDFunctions should accept JFunction (not Function),KafkaWordCount example crashes with java.lang.ArrayIndexOutOfBoundsException in CheckpointRDD.scala,0,1,1
397,SPARK-703,SPARK-704,Not A Problem,KafkaWordCount example crashes with java.lang.ArrayIndexOutOfBoundsException in CheckpointRDD.scala,ConnectionManager sometimes cannot detect loss of sending connections,1,1,1
398,SPARK-704,SPARK-705,Cannot Reproduce,ConnectionManager sometimes cannot detect loss of sending connections,Implement sortByKey() in PySpark,0,1,1
399,SPARK-705,SPARK-706,Fixed,Implement sortByKey() in PySpark,Failures in block manager put leads to task hanging,0,1,1
400,SPARK-706,SPARK-707,Cannot Reproduce,Failures in block manager put leads to task hanging,Port more of the Scala example programs to Java,0,0,1
401,SPARK-707,SPARK-708,Fixed,Port more of the Scala example programs to Java,add a JobLogger for Spark,0,0,1
402,SPARK-708,SPARK-709,Fixed,add a JobLogger for Spark,Dropping a block reports 0 bytes,0,1,1
403,SPARK-709,SPARK-710,Incomplete,Dropping a block reports 0 bytes,Remove dependency on Twitter4J repository,1,0,1
404,SPARK-710,SPARK-711,Fixed,Remove dependency on Twitter4J repository,Spark Streaming 0.7.0: ArrayIndexOutOfBoundsException in KafkaWordCount Example,0,0,1
405,SPARK-712,SPARK-713,Fixed,Kafka OutOfMemoryError,Broken link in quick start guide,0,0,1
406,SPARK-713,SPARK-714,Fixed,Broken link in quick start guide,"Link to YARN document broken in ""Launching Spark on YARN"" doc",1,1,1
407,SPARK-714,SPARK-715,Fixed,"Link to YARN document broken in ""Launching Spark on YARN"" doc",Add documentation for using Maven to build Spark using,1,1,1
408,SPARK-715,SPARK-716,Fixed,Add documentation for using Maven to build Spark using,Unit tests fail out of the box,0,1,1
409,SPARK-716,SPARK-717,Fixed,Unit tests fail out of the box,Refactor Programming Guides in Documentation,0,1,1
410,SPARK-717,SPARK-718,Fixed,Refactor Programming Guides in Documentation,NPE when performing action during transformation,0,1,1
411,SPARK-718,SPARK-720,Done,NPE when performing action during transformation,Statically guarantee serialization will succeed,0,1,1
412,SPARK-720,SPARK-721,Won't Fix,Statically guarantee serialization will succeed,Fix remaining deprecation warnings,0,0,1
413,SPARK-721,SPARK-722,Fixed,Fix remaining deprecation warnings,"Prefix current Spark package names with ""org.spark-project.""",1,0,1
414,SPARK-722,SPARK-723,Fixed,"Prefix current Spark package names with ""org.spark-project.""",Upgrade from Scala 2.9.2 to 2.9.3,0,1,1
415,SPARK-723,SPARK-724,Fixed,Upgrade from Scala 2.9.2 to 2.9.3,Have Akka logging enabled by default for standalone daemons,0,1,1
416,SPARK-724,SPARK-725,Fixed,Have Akka logging enabled by default for standalone daemons,Ran out of disk space on EC2 master due to Ganglia logs,0,1,1
417,SPARK-725,SPARK-726,Not A Problem,Ran out of disk space on EC2 master due to Ganglia logs,Possible bugs in zip() transformation,0,1,1
418,SPARK-726,SPARK-727,Fixed,Possible bugs in zip() transformation,Various improvements to Spark EC2 Scripts,0,1,1
419,SPARK-727,SPARK-728,Fixed,Various improvements to Spark EC2 Scripts,Automatically infer AMI ids based on Region; HVM/Non-HVM etc.,1,1,1
420,SPARK-728,SPARK-729,Fixed,Automatically infer AMI ids based on Region; HVM/Non-HVM etc.,Closures not always serialized at capture time,0,1,1
421,SPARK-729,SPARK-730,Won't Fix,Closures not always serialized at capture time,Unit testing failure,0,0,1
422,SPARK-730,SPARK-731,Fixed,Unit testing failure,CheckpointRDD with zero partitions test failure,0,1,1
423,SPARK-732,SPARK-733,Won't Fix,Recomputation of RDDs may result in duplicated accumulator updates,Add documentation on use of accumulators in lazy transformation,0,0,1
424,SPARK-733,SPARK-734,Fixed,Add documentation on use of accumulators in lazy transformation,Adjust default timeouts to be more sane,0,1,1
425,SPARK-734,SPARK-735,Fixed,Adjust default timeouts to be more sane,memory leak in KryoSerializer,1,1,1
426,SPARK-735,SPARK-736,Fixed,memory leak in KryoSerializer,Killing tasks in spark,1,0,1
427,SPARK-737,SPARK-738,Fixed,Silence expected exceptions for unit tests,Spark should detect and squash nonserializable exceptions,1,0,1
428,SPARK-738,SPARK-739,Fixed,Spark should detect and squash nonserializable exceptions,Have quickstart standalone read README instead of log file,1,0,1
429,SPARK-739,SPARK-740,Fixed,Have quickstart standalone read README instead of log file,Spark block manager UI has bug when enabling Spark Streaming,0,0,1
430,SPARK-740,SPARK-741,Fixed,Spark block manager UI has bug when enabling Spark Streaming,DiskStore should use > 8kB buffer when doing writes,0,0,1
431,SPARK-741,SPARK-742,Fixed,DiskStore should use > 8kB buffer when doing writes,Task Metrics should not employ per-record timing by default,1,0,1
432,SPARK-742,SPARK-743,Fixed,Task Metrics should not employ per-record timing by default,An Akka-cluster based masterless standalone mode for Spark.,0,0,1
433,SPARK-743,SPARK-744,Won't Fix,An Akka-cluster based masterless standalone mode for Spark.,BlockManagerUI with no RDD: java.lang.UnsupportedOperationException: empty.reduceLeft,0,0,1
434,SPARK-744,SPARK-745,Incomplete,BlockManagerUI with no RDD: java.lang.UnsupportedOperationException: empty.reduceLeft,Document Scala environment configuration when Scala is installed from RPM,0,0,1
435,SPARK-745,SPARK-746,Fixed,Document Scala environment configuration when Scala is installed from RPM,Automatically Use Avro Serialization for Avro Objects,0,0,1
436,SPARK-746,SPARK-747,Fixed,Automatically Use Avro Serialization for Avro Objects,Throw an exception on slaves if a message is sent that is larger than akka's max frame size,0,1,1
437,SPARK-747,SPARK-748,Fixed,Throw an exception on slaves if a message is sent that is larger than akka's max frame size,Add documentation page describing interoperability with other software (e.g. HBase; JDBC; Kafka; etc.),0,1,1
438,SPARK-748,SPARK-749,Won't Fix,Add documentation page describing interoperability with other software (e.g. HBase; JDBC; Kafka; etc.),spark-ec2 fails to detect cluster after ssh error during launch,0,1,1
439,SPARK-749,SPARK-750,Fixed,spark-ec2 fails to detect cluster after ssh error during launch,LocalSparkContext should be included in Spark JAR,0,0,1
440,SPARK-751,SPARK-752,Fixed,Consolidate shuffle files,CoalescedRDD should maximize locality,1,1,1
441,SPARK-752,SPARK-753,Fixed,CoalescedRDD should maximize locality,ClusterSchedulerSuite unit test will failed in some scenarios,0,0,1
442,SPARK-753,SPARK-754,Fixed,ClusterSchedulerSuite unit test will failed in some scenarios,Multiple Spark Contexts active in a single Spark Context,1,0,1
443,SPARK-755,SPARK-756,Cannot Reproduce,Kryo serialization failing - MLbase,JAR file appears corrupt to workers. - MLbase,0,1,1
444,SPARK-756,SPARK-757,Cannot Reproduce,JAR file appears corrupt to workers. - MLbase,Deserialization Exception partway into long running job with Netty - MLbase,1,1,1
445,SPARK-757,SPARK-758,Not A Problem,Deserialization Exception partway into long running job with Netty - MLbase,ALS scalability for MLbase,0,1,1
446,SPARK-758,SPARK-759,Incomplete,ALS scalability for MLbase,Change how we track AMI ids in the EC2 scripts,0,1,1
447,SPARK-759,SPARK-760,Fixed,Change how we track AMI ids in the EC2 scripts,Include a simple PageRank example in Spark,0,0,1
448,SPARK-760,SPARK-761,Fixed,Include a simple PageRank example in Spark,Print a nicer error message when incompatible Spark binaries try to talk,0,1,1
449,SPARK-762,SPARK-763,Fixed,The simple tutorial doesn't say anything about including the Spark jar or the Spark codepath,Built-in support for saving with compression,0,1,1
450,SPARK-763,SPARK-764,Fixed,Built-in support for saving with compression,Fix SPARK_EXAMPLES_JAR in 0.7.2,0,1,1
451,SPARK-764,SPARK-765,Fixed,Fix SPARK_EXAMPLES_JAR in 0.7.2,Test suite should run Spark example programs,1,1,1
452,SPARK-765,SPARK-766,Unresolved,Test suite should run Spark example programs,Add executor info for the task completed message,0,1,1
453,SPARK-766,SPARK-767,Fixed,Add executor info for the task completed message,Standardize web ui port,1,0,1
454,SPARK-767,SPARK-768,Fixed,Standardize web ui port,Fail a task when the remote block it is fetching is not serializable,1,0,1
455,SPARK-768,SPARK-769,Cannot Reproduce,Fail a task when the remote block it is fetching is not serializable,block manager UI should contain the locations for each RDD block,1,1,1
456,SPARK-769,SPARK-770,Fixed,block manager UI should contain the locations for each RDD block,Extend and Consolidate Spark Web UI,1,1,1
457,SPARK-770,SPARK-771,Fixed,Extend and Consolidate Spark Web UI,Throw a more meaning message when SparkContext cannot connect to the master,1,1,1
458,SPARK-771,SPARK-772,Fixed,Throw a more meaning message when SparkContext cannot connect to the master,groupByKey should disable map side combine,1,1,1
459,SPARK-772,SPARK-773,Fixed,groupByKey should disable map side combine,Add fair scheduler pool information UI similar with hadoop,0,0,1
460,SPARK-773,SPARK-774,Fixed,Add fair scheduler pool information UI similar with hadoop,cogroup should also disable map side combine by default,0,0,1
461,SPARK-774,SPARK-776,Fixed,cogroup should also disable map side combine by default,Support adding jars to Spark shell,1,1,1
462,SPARK-776,SPARK-777,Fixed,Support adding jars to Spark shell,spark.util.AkkaUtils not usable out of spark package,0,0,1
463,SPARK-777,SPARK-778,Won't Fix,spark.util.AkkaUtils not usable out of spark package,run script should try java executable from JAVA_HOME first,0,0,1
464,SPARK-778,SPARK-779,Fixed,run script should try java executable from JAVA_HOME first,Monitoring and logging improvement,1,1,1
465,SPARK-779,SPARK-780,Fixed,Monitoring and logging improvement,stdout log should contain the command to launch the worker JVM,1,1,1
466,SPARK-780,SPARK-781,Fixed,stdout log should contain the command to launch the worker JVM,"Log the temp directory path when Spark says ""Failed to create temp directory""",1,1,1
467,SPARK-781,SPARK-782,Fixed,"Log the temp directory path when Spark says ""Failed to create temp directory""",Multiple versions of ASM being put on classpath,1,1,1
468,SPARK-782,SPARK-783,Fixed,Multiple versions of ASM being put on classpath,Web UI should report whether a job/task has failed,1,1,1
469,SPARK-783,SPARK-784,Fixed,Web UI should report whether a job/task has failed,"foldByKey does not clone the ""zero value"" for each key; leading to overwriting",0,1,1
470,SPARK-784,SPARK-785,Fixed,"foldByKey does not clone the ""zero value"" for each key; leading to overwriting",ClosureCleaner not invoked on most PairRDDFunctions,1,1,1
471,SPARK-785,SPARK-786,Fixed,ClosureCleaner not invoked on most PairRDDFunctions,Clean up old work directories in standalone worker,0,1,1
472,SPARK-787,SPARK-788,Fixed,Add EC2 Script Option to Push EC2 Credentials to Spark Nodes,Add spark metrics system,0,1,1
473,SPARK-788,SPARK-789,Fixed,Add spark metrics system,ApplicationRemoved message shouldn't be sent to terminated application driver actor,0,0,1
474,SPARK-789,SPARK-790,Fixed,ApplicationRemoved message shouldn't be sent to terminated application driver actor,Implement the reregistered() callback in MesosScheduler to support master failover,0,0,1
475,SPARK-790,SPARK-791,Unresolved,Implement the reregistered() callback in MesosScheduler to support master failover,add DenseVector and SparseVector to mllib; and replace all Array[Double] with Vectors,0,0,1
476,SPARK-792,SPARK-793,Fixed,PairRDDFunctions should expect Product2 instead of Tuple2,DAGScheduler doesn't release ActiveJob inside idToActiveJob,0,1,1
477,SPARK-793,SPARK-794,Fixed,DAGScheduler doesn't release ActiveJob inside idToActiveJob,Remove sleep() in ClusterScheduler.stop,0,1,1
478,SPARK-794,SPARK-795,Fixed,Remove sleep() in ClusterScheduler.stop,Numerous tests failing in maven build,0,0,1
479,SPARK-795,SPARK-796,Fixed,Numerous tests failing in maven build,Jobs are always marked as SUCCEEDED even it's actually failed on Yarn.,0,1,1
480,SPARK-796,SPARK-797,Fixed,Jobs are always marked as SUCCEEDED even it's actually failed on Yarn.,ML failures if libfortran is not installed,0,0,1
481,SPARK-797,SPARK-798,Fixed,ML failures if libfortran is not installed,AMI: ami-530f7a3a and Mesos,0,1,1
482,SPARK-798,SPARK-799,Won't Fix,AMI: ami-530f7a3a and Mesos,Windows versions of the deploy scripts,0,1,1
483,SPARK-799,SPARK-800,Unresolved,Windows versions of the deploy scripts,Improve Quickstart Docs to Make Full Deployment More Clear,0,1,1
484,SPARK-800,SPARK-801,Fixed,Improve Quickstart Docs to Make Full Deployment More Clear,Time columns in web UI tables don't sort properly,0,0,1
485,SPARK-801,SPARK-802,Fixed,Time columns in web UI tables don't sort properly,Link to job UI from standalone deploy cluster web UI,0,1,1
486,SPARK-802,SPARK-803,Fixed,Link to job UI from standalone deploy cluster web UI,Open ports 33000-33010 on EC2 clusters for accessing job UI,0,0,1
487,SPARK-803,SPARK-804,Fixed,Open ports 33000-33010 on EC2 clusters for accessing job UI,Show task status in the Stage Details table on job UI,0,0,1
488,SPARK-804,SPARK-805,Fixed,Show task status in the Stage Details table on job UI,When determining the Spark method for a stage or RDD; look only in spark.* classes,0,1,1
489,SPARK-805,SPARK-806,Fixed,When determining the Spark method for a stage or RDD; look only in spark.* classes,Show application name in HTML page titles on job web UI,0,0,1
490,SPARK-806,SPARK-807,Fixed,Show application name in HTML page titles on job web UI,Show totals for shuffle data and CPU time in Stage pages of UI,1,0,1
491,SPARK-807,SPARK-808,Fixed,Show totals for shuffle data and CPU time in Stage pages of UI,"Add an ""executors"" tab to job UI that shows executor stats",1,0,1
492,SPARK-808,SPARK-809,Fixed,"Add an ""executors"" tab to job UI that shows executor stats",Give newly registered apps a set of executors right away,0,0,1
493,SPARK-809,SPARK-810,Won't Fix,Give newly registered apps a set of executors right away,Ensure thread safety of Spark UI,0,0,1
494,SPARK-810,SPARK-811,Fixed,Ensure thread safety of Spark UI,Job UI should show running tasks,0,1,1
495,SPARK-811,SPARK-812,Fixed,Job UI should show running tasks,Netty shuffle creates a lot of open file handles,0,1,1
496,SPARK-812,SPARK-813,Invalid,Netty shuffle creates a lot of open file handles,Poor locality in master due to ClusterScheduler changes,0,1,1
497,SPARK-813,SPARK-814,Fixed,Poor locality in master due to ClusterScheduler changes,Result stages should be named after the action that invoked them,1,0,1
498,SPARK-814,SPARK-815,Fixed,Result stages should be named after the action that invoked them,PySpark's parallelize() should batch objects after partitioning (instead of before),0,0,1
499,SPARK-815,SPARK-816,Fixed,PySpark's parallelize() should batch objects after partitioning (instead of before),Add Documentation Page for Building and Deploying with a CDH/HDP Cluster,0,1,1
500,SPARK-816,SPARK-817,Fixed,Add Documentation Page for Building and Deploying with a CDH/HDP Cluster,Consistently invoke bash with /usr/bin/env bash in scripts,0,0,1
501,SPARK-817,SPARK-818,Fixed,Consistently invoke bash with /usr/bin/env bash in scripts,Design Spark Job Server,1,0,1
502,SPARK-818,SPARK-819,Won't Fix,Design Spark Job Server,netty: ChannelInboundByteHandlerAdapter no longer exist in 4.0.3.Final,0,1,1
503,SPARK-819,SPARK-820,Fixed,netty: ChannelInboundByteHandlerAdapter no longer exist in 4.0.3.Final,Use Bootstrap progress bars in web UI,0,0,1
504,SPARK-820,SPARK-821,Fixed,Use Bootstrap progress bars in web UI,Driver program should not put a block in memory,0,0,1
505,SPARK-821,SPARK-822,Fixed,Driver program should not put a block in memory,defaultMinSplits can't be set higher than 2,0,0,1
506,SPARK-822,SPARK-823,Won't Fix,defaultMinSplits can't be set higher than 2,spark.default.parallelism's default is inconsistent across scheduler backends,0,0,1
507,SPARK-823,SPARK-824,Fixed,spark.default.parallelism's default is inconsistent across scheduler backends,Make less copies of blocks during remote reads,0,0,1
508,SPARK-824,SPARK-825,Fixed,Make less copies of blocks during remote reads,DoubleRDDFunctions.sampleStdev() actually computes regular stdev(),0,0,1
509,SPARK-825,SPARK-826,Fixed,DoubleRDDFunctions.sampleStdev() actually computes regular stdev(),fold(); reduce(); collect() always attempt to use java serialization,0,0,1
510,SPARK-826,SPARK-827,Fixed,fold(); reduce(); collect() always attempt to use java serialization,MAX_TASK_FAILURES not configurable,0,1,1
511,SPARK-827,SPARK-828,Fixed,MAX_TASK_FAILURES not configurable,Clean up web UI page headers,0,0,1
512,SPARK-828,SPARK-829,Fixed,Clean up web UI page headers,scheduler shouldn't hang if a task contains unserializable objects in its closure,0,0,1
513,SPARK-829,SPARK-830,Fixed,scheduler shouldn't hang if a task contains unserializable objects in its closure,Annotate developer and experimental API's [Core],0,0,1
514,SPARK-830,SPARK-831,Fixed,Annotate developer and experimental API's [Core],Move certain classes into more appropriate packages,0,1,1
515,SPARK-831,SPARK-832,Fixed,Move certain classes into more appropriate packages,PySpark should set worker PYTHONPATH from SPARK_HOME instead of inheriting it from the master,0,1,1
516,SPARK-832,SPARK-833,Fixed,PySpark should set worker PYTHONPATH from SPARK_HOME instead of inheriting it from the master,"Web UI's ""Application Detail UI"" link may use internal EC2 hostname",0,1,1
517,SPARK-833,SPARK-834,Fixed,"Web UI's ""Application Detail UI"" link may use internal EC2 hostname",TaskMetrics should report compressed (not uncompressed) shuffle read bytes.,1,1,1
518,SPARK-834,SPARK-835,Fixed,TaskMetrics should report compressed (not uncompressed) shuffle read bytes.,RDD$parallelize() should use object serializer (not closure serializer) for collection objects,0,1,1
519,SPARK-835,SPARK-836,Fixed,RDD$parallelize() should use object serializer (not closure serializer) for collection objects,ResultTask's serialization forget to handle generation,1,1,1
520,SPARK-838,SPARK-839,Fixed,Add DoubleRDDFunctions methods to PySpark,Bug in how failed executors are removed by ID from standalone cluster,0,0,1
521,SPARK-839,SPARK-840,Fixed,Bug in how failed executors are removed by ID from standalone cluster,Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.,0,0,1
522,SPARK-840,SPARK-841,Fixed,Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.,Use a smaller job UI port than 33000 by default,0,0,1
523,SPARK-841,SPARK-842,Fixed,Use a smaller job UI port than 33000 by default,Maven assembly is including examples libs and dependencies,0,0,1
524,SPARK-842,SPARK-843,Fixed,Maven assembly is including examples libs and dependencies,Show time the app has been running for in job UI,0,0,1
525,SPARK-843,SPARK-844,Fixed,Show time the app has been running for in job UI,Occasional hang on shuffle fetches in master branch,0,0,1
526,SPARK-844,SPARK-845,Fixed,Occasional hang on shuffle fetches in master branch,Removing an executor can result in a negative number of cores used ,0,1,1
527,SPARK-845,SPARK-846,Fixed,Removing an executor can result in a negative number of cores used ,Set `spark.job.annotation` and display it in the web UI.,0,1,1
528,SPARK-846,SPARK-847,Fixed,Set `spark.job.annotation` and display it in the web UI.,Zombie workers,0,1,1
529,SPARK-847,SPARK-848,Fixed,Zombie workers,The StageTable should sort by submitted time by default,0,1,1
530,SPARK-848,SPARK-849,Fixed,The StageTable should sort by submitted time by default,Variety of small fixes in the Web UI,1,1,1
531,SPARK-849,SPARK-850,Fixed,Variety of small fixes in the Web UI,WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered,0,1,1
532,SPARK-850,SPARK-851,Fixed,WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered,Summary count at the top for active completed stages should link to corresponding sections in page,0,1,1
533,SPARK-851,SPARK-852,Fixed,Summary count at the top for active completed stages should link to corresponding sections in page,Remove `Stored RDD` Column,1,1,1
534,SPARK-852,SPARK-853,Fixed,Remove `Stored RDD` Column,"Should say ""Running/Succeeded"" instead of ""Running/Completed""",1,1,1
535,SPARK-853,SPARK-854,Fixed,"Should say ""Running/Succeeded"" instead of ""Running/Completed""","For some reason; header says ""Jobs"" in RDD storage page",1,1,1
536,SPARK-854,SPARK-855,Fixed,"For some reason; header says ""Jobs"" in RDD storage page","Page should have two titles: ""Data Distribution Summary"" (top part); ""Partitions"" (bottom part)",1,1,1
537,SPARK-855,SPARK-856,Fixed,"Page should have two titles: ""Data Distribution Summary"" (top part); ""Partitions"" (bottom part)",Sort all tables by the first column (A -> Z) by default,1,1,1
538,SPARK-856,SPARK-857,Fixed,Sort all tables by the first column (A -> Z) by default,Move Jobs tab before storage,1,1,1
539,SPARK-857,SPARK-858,Fixed,Move Jobs tab before storage,"Right-align the ""Application name"" part if possible",1,1,1
540,SPARK-858,SPARK-859,Fixed,"Right-align the ""Application name"" part if possible",Remove the executor count from the header,1,1,1
541,SPARK-859,SPARK-860,Fixed,Remove the executor count from the header,Clicking Spark logo should go to default page,1,1,1
542,SPARK-860,SPARK-861,Fixed,Clicking Spark logo should go to default page,Task progress should be overlaid with progress bar,1,1,1
543,SPARK-861,SPARK-862,Fixed,Task progress should be overlaid with progress bar,Could not use spark-ec2 to launch clusters with instance type 'cc1.4xlarge',0,1,1
544,SPARK-863,SPARK-864,Fixed,Have Synchronous Versions of `stop-all.sh` and `start-all.sh`,DAGScheduler Exception if A Node is Added then Deleted,0,1,1
545,SPARK-864,SPARK-865,Cannot Reproduce,DAGScheduler Exception if A Node is Added then Deleted,Add the equivalent of ADD_JARS to PySpark,0,0,1
546,SPARK-865,SPARK-866,Fixed,Add the equivalent of ADD_JARS to PySpark,Add the equivalent of ADD_JARS to PySpark,1,1,1
547,SPARK-866,SPARK-867,Duplicate,Add the equivalent of ADD_JARS to PySpark,Add a native Python way to create input RDDs in PySpark,1,0,1
548,SPARK-867,SPARK-868,Won't Fix,Add a native Python way to create input RDDs in PySpark,Document fair scheduler,0,1,1
549,SPARK-869,SPARK-870,Done,Retrofit rest of RDD api to use proper serializer type,Jobs UI shows incorrect task count if #tasks is not #partitions,0,1,1
550,SPARK-870,SPARK-871,Fixed,Jobs UI shows incorrect task count if #tasks is not #partitions,Add a link to the stdout/stderr log on the job level web ui,0,1,1
551,SPARK-871,SPARK-872,Fixed,Add a link to the stdout/stderr log on the job level web ui,Should revive offer after tasks finish in Mesos fine-grained mode ,0,1,1
552,SPARK-872,SPARK-873,Won't Fix,Should revive offer after tasks finish in Mesos fine-grained mode ,Add a way to specify rack topology in Mesos and standalone modes,0,1,1
553,SPARK-873,SPARK-874,Won't Fix,Add a way to specify rack topology in Mesos and standalone modes,Have a --wait flag in ./sbin/stop-all.sh that polls until Worker's are finished,0,0,1
554,SPARK-874,SPARK-875,Fixed,Have a --wait flag in ./sbin/stop-all.sh that polls until Worker's are finished,Disk mounts can be wonky on EC2,0,0,1
555,SPARK-875,SPARK-876,Fixed,Disk mounts can be wonky on EC2,Install Missing Features on AMI,1,1,1
556,SPARK-876,SPARK-877,Fixed,Install Missing Features on AMI,java.lang.UnsupportedOperationException: empty.reduceLeft in UI,0,1,1
557,SPARK-877,SPARK-878,Fixed,java.lang.UnsupportedOperationException: empty.reduceLeft in UI,PySpark does not add Python *.zip and *.egg files to PYTHONPATH,0,0,1
558,SPARK-878,SPARK-879,Fixed,PySpark does not add Python *.zip and *.egg files to PYTHONPATH,"Typo in slaves file: ""listes"" instead of ""listed""",0,0,1
559,SPARK-879,SPARK-880,Fixed,"Typo in slaves file: ""listes"" instead of ""listed""",When built with Hadoop2; spark-shell and examples don't initialize log4j properly,0,0,1
560,SPARK-880,SPARK-881,Fixed,When built with Hadoop2; spark-shell and examples don't initialize log4j properly,Add documentation for new monitoring capabilities,0,1,1
561,SPARK-881,SPARK-882,Fixed,Add documentation for new monitoring capabilities,Have link for feedback/suggestions in docs,1,0,1
562,SPARK-882,SPARK-883,Not A Problem,Have link for feedback/suggestions in docs,Remove dependency on Scala json library,0,1,1
563,SPARK-883,SPARK-884,Fixed,Remove dependency on Scala json library,unit test to validate various Spark json output,1,0,1
564,SPARK-884,SPARK-885,Fixed,unit test to validate various Spark json output,PySpark shell should capture ctrl-c to prevent users from accidentally killing the Java gateway,0,1,1
565,SPARK-885,SPARK-886,Fixed,PySpark shell should capture ctrl-c to prevent users from accidentally killing the Java gateway,NullpointerExceptions in InputFormatInfo.computePreferredLocations/SparkHDFSLR ,0,0,1
566,SPARK-886,SPARK-887,Fixed,NullpointerExceptions in InputFormatInfo.computePreferredLocations/SparkHDFSLR ,mvn package doesn't include yarn in the repl-bin shaded jar,0,0,1
567,SPARK-887,SPARK-888,Fixed,mvn package doesn't include yarn in the repl-bin shaded jar,Killing jobs on standalone cluster,0,1,1
568,SPARK-889,SPARK-890,Won't Fix,Bring back DFS broadcast,Allow multiple parallel commands in spark-shell,1,0,1
569,SPARK-890,SPARK-891,Fixed,Allow multiple parallel commands in spark-shell,Update Windows launch scripts to use assembly,0,1,1
570,SPARK-891,SPARK-892,Fixed,Update Windows launch scripts to use assembly,Add docs page for fair scheduler,0,1,1
571,SPARK-893,SPARK-894,Fixed,Create a docs page on monitoring and metrics,Not all WebUI fields delivered VIA JSON,0,0,1
572,SPARK-894,SPARK-895,Fixed,Not all WebUI fields delivered VIA JSON,Add a Ganglia sink to Spark Metrics,0,0,1
573,SPARK-895,SPARK-896,Fixed,Add a Ganglia sink to Spark Metrics,ADD_JARS does not add all classes to classpath in the spark-shell for cluster on Mesos.,0,0,1
574,SPARK-896,SPARK-897,Won't Fix,ADD_JARS does not add all classes to classpath in the spark-shell for cluster on Mesos.,Preemptively serialize closures to help users identify non-serializable errors early on,0,1,1
575,SPARK-897,SPARK-898,Fixed,Preemptively serialize closures to help users identify non-serializable errors early on,Add jets3t dependency to Spark Build,1,1,1
576,SPARK-898,SPARK-899,Fixed,Add jets3t dependency to Spark Build,Outdated Bagel documentation,0,1,1
577,SPARK-899,SPARK-900,Won't Fix,Outdated Bagel documentation,Use coarser grained naming for metrics,0,0,1
578,SPARK-900,SPARK-901,Fixed,Use coarser grained naming for metrics,"UISuite ""jetty port increases under contention"" fails if startPort is in use",0,1,1
579,SPARK-901,SPARK-902,Fixed,"UISuite ""jetty port increases under contention"" fails if startPort is in use",java.lang.AbstractMethodError when using FlatMapFunction from Java,0,0,1
580,SPARK-902,SPARK-903,Fixed,java.lang.AbstractMethodError when using FlatMapFunction from Java,Inconsistent spark assembly,0,0,1
581,SPARK-903,SPARK-904,Fixed,Inconsistent spark assembly,Not able to Start/Stop Spark Worker from Remote Machine,0,0,1
582,SPARK-904,SPARK-905,Not A Problem,Not able to Start/Stop Spark Worker from Remote Machine,Not able to run Job on  remote machine,1,1,1
583,SPARK-905,SPARK-906,Cannot Reproduce,Not able to run Job on  remote machine,How to recover Spark Master in case of machine failure; where Spark Master was running,1,1,1
584,SPARK-906,SPARK-907,Fixed,How to recover Spark Master in case of machine failure; where Spark Master was running,Add JSON endpoints to SparkUI,0,0,1
585,SPARK-908,SPARK-909,Fixed,local metrics test has race condition due to new SparkListener architecture,add task serialization footprint (time and size) into TaskMetrics,0,0,1
586,SPARK-909,SPARK-910,Fixed,add task serialization footprint (time and size) into TaskMetrics,hadoopFile creates RecordReader key and value at the wrong scope,1,0,1
587,SPARK-910,SPARK-911,Not A Problem,hadoopFile creates RecordReader key and value at the wrong scope,Support map pruning on sorted (K; V) RDD's,1,1,1
588,SPARK-911,SPARK-912,Fixed,Support map pruning on sorted (K; V) RDD's,Take stage breakdown functionality and runLocally out of the main event loop in DAGScheduler,0,1,1
589,SPARK-912,SPARK-913,Fixed,Take stage breakdown functionality and runLocally out of the main event loop in DAGScheduler,log the size of each shuffle block in block manager,0,0,1
590,SPARK-913,SPARK-914,Not A Problem,log the size of each shuffle block in block manager,Make RDD implement Scala and Java Iterable interfaces,0,1,1
591,SPARK-914,SPARK-915,Fixed,Make RDD implement Scala and Java Iterable interfaces,Tidy up the scripts,0,0,1
592,SPARK-915,SPARK-916,Fixed,Tidy up the scripts,Better Support for Flat/Tabular RDD's,0,1,1
593,SPARK-916,SPARK-917,Fixed,Better Support for Flat/Tabular RDD's,API docs for Spark/MLLib/Streaming should point to package URL,0,1,1
594,SPARK-917,SPARK-918,Fixed,API docs for Spark/MLLib/Streaming should point to package URL,hadoop-client dependency should be explained for Scala in addition to Java in quickstart,1,1,1
595,SPARK-918,SPARK-919,Won't Fix,hadoop-client dependency should be explained for Scala in addition to Java in quickstart,spark-ec2 launch --resume doesn't re-initialize all modules,0,1,1
596,SPARK-919,SPARK-920,Fixed,spark-ec2 launch --resume doesn't re-initialize all modules,JSON endpoint URI scheme part (spark://) duplicated,0,0,1
597,SPARK-920,SPARK-921,Fixed,JSON endpoint URI scheme part (spark://) duplicated,Add Application UI URL to ApplicationInfo Json output,1,0,1
598,SPARK-921,SPARK-922,Fixed,Add Application UI URL to ApplicationInfo Json output,Update Spark AMI to Python 2.7,0,0,1
599,SPARK-922,SPARK-923,Not A Problem,Update Spark AMI to Python 2.7,Bytes columns in web UI tables don't sort properly,0,0,1
600,SPARK-923,SPARK-924,Fixed,Bytes columns in web UI tables don't sort properly,Nested RDD,0,0,1
601,SPARK-924,SPARK-925,Won't Fix,Nested RDD,Allow ec2 scripts to load default options from a json file,0,0,1
602,SPARK-925,SPARK-926,Won't Fix,Allow ec2 scripts to load default options from a json file,spark_ec2 script when ssh/scp-ing should pipe UserknowHostFile to /dev/null,1,0,1
603,SPARK-927,SPARK-928,Fixed,PySpark sample() doesn't work if numpy is installed on master but not on workers,Add support for Unsafe-based serializer in Kryo 2.22,0,1,1
604,SPARK-928,SPARK-929,Fixed,Add support for Unsafe-based serializer in Kryo 2.22,Deprecate SPARK_MEM,1,0,1
605,SPARK-929,SPARK-930,Fixed,Deprecate SPARK_MEM,Unicode failing in pyspark - UnicodeEncodeError: 'ascii' codec can't encode character u'\ufffd' in position 440: ordinal not in range(128),0,1,1
606,SPARK-930,SPARK-931,Cannot Reproduce,Unicode failing in pyspark - UnicodeEncodeError: 'ascii' codec can't encode character u'\ufffd' in position 440: ordinal not in range(128),Potential bug with Spark streaming example,0,1,1
607,SPARK-931,SPARK-932,Fixed,Potential bug with Spark streaming example,Consolidate local scheduler and cluster scheduler,0,1,1
608,SPARK-932,SPARK-933,Fixed,Consolidate local scheduler and cluster scheduler,Doesn't compile,0,1,1
609,SPARK-933,SPARK-934,Won't Fix,Doesn't compile,spark-mllib occasionally throw java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress; input offset 51381; output offset 57509),0,1,1
610,SPARK-934,SPARK-935,Fixed,spark-mllib occasionally throw java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress; input offset 51381; output offset 57509),Typo in documentation,1,0,1
611,SPARK-935,SPARK-936,Fixed,Typo in documentation,Please publish jars for Scala 2.10,0,0,1
612,SPARK-938,SPARK-939,Fixed,OpenStack Swift Storage Support,Allow user jars to take precedence over Spark jars; if desired,0,0,1
613,SPARK-939,SPARK-940,Fixed,Allow user jars to take precedence over Spark jars; if desired,Do not directly pass Stage objects to SparkListener,0,0,1
614,SPARK-940,SPARK-941,Fixed,Do not directly pass Stage objects to SparkListener,Add javadoc and user docs for JobLogger,0,1,1
615,SPARK-941,SPARK-942,Won't Fix,Add javadoc and user docs for JobLogger,Do not materialize partitions when DISK_ONLY storage level is used,0,1,1
616,SPARK-942,SPARK-943,Fixed,Do not materialize partitions when DISK_ONLY storage level is used,Add `coalesce` and `repartition` to the streaming API,0,1,1
617,SPARK-943,SPARK-944,Fixed,Add `coalesce` and `repartition` to the streaming API,Give example of writing to HBase from Spark Streaming,1,1,1
618,SPARK-944,SPARK-945,Not A Problem,Give example of writing to HBase from Spark Streaming,Fix confusing behavior when assembly jars already exist,0,1,1
619,SPARK-945,SPARK-946,Fixed,Fix confusing behavior when assembly jars already exist,Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap,1,1,1
620,SPARK-946,SPARK-947,Fixed,Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap,Class path issue in case assembled with specific hadoop version,0,0,1
621,SPARK-947,SPARK-948,Fixed,Class path issue in case assembled with specific hadoop version,"Move ""Classpath Entries"" in WebUI",0,1,1
622,SPARK-948,SPARK-949,Won't Fix,"Move ""Classpath Entries"" in WebUI",Turn DAGScheduler into an Actor,0,0,1
623,SPARK-949,SPARK-950,Fixed,Turn DAGScheduler into an Actor,Use faster random number generator for sampling in K-means,0,1,1
624,SPARK-950,SPARK-951,Fixed,Use faster random number generator for sampling in K-means,Gaussian Mixture Model,0,0,1
625,SPARK-954,SPARK-955,Won't Fix,One repeated sampling; and I am not sure if it is correct.,Paritions increase expotentially when doing cartisian product in loop program,1,1,1
626,SPARK-955,SPARK-956,Won't Fix,Paritions increase expotentially when doing cartisian product in loop program,The Spark python program for Lasso,0,1,1
627,SPARK-956,SPARK-957,Won't Fix,The Spark python program for Lasso,The problem that repeated computation among iterations,1,1,1
628,SPARK-957,SPARK-958,Not A Problem,The problem that repeated computation among iterations,When iteration in ALS increases to 10 running in local mode; spark throws out error of StackOverflowError,0,1,1
629,SPARK-959,SPARK-960,Fixed,Ivy fails to download javax.servlet.orbit dependency,"JobCancellationSuite ""two jobs sharing the same stage"" is broken",0,1,1
630,SPARK-960,SPARK-961,Fixed,"JobCancellationSuite ""two jobs sharing the same stage"" is broken",Add a Vector.random() method or make our website examples show something else,0,0,1
631,SPARK-961,SPARK-962,Fixed,Add a Vector.random() method or make our website examples show something else,debian package contains old version of executable scripts,0,0,1
632,SPARK-962,SPARK-963,Fixed,debian package contains old version of executable scripts,Races in JobLoggerSuite,0,0,1
633,SPARK-963,SPARK-964,Fixed,Races in JobLoggerSuite,Investigate the potential for using JDK 8 lambda expressions for the Java/Scala APIs,0,0,1
634,SPARK-964,SPARK-965,Fixed,Investigate the potential for using JDK 8 lambda expressions for the Java/Scala APIs,Race in DAGSchedulerSuite,0,0,1
635,SPARK-965,SPARK-966,Fixed,Race in DAGSchedulerSuite,Sometimes DAGScheduler throws NullPointerException ,0,0,1
636,SPARK-966,SPARK-967,Fixed,Sometimes DAGScheduler throws NullPointerException ,start-slaves.sh uses local path from master on remote slave nodes,0,0,1
637,SPARK-967,SPARK-968,Not A Problem,start-slaves.sh uses local path from master on remote slave nodes,In stage UI; add an overview section that shows task stats grouped by executor id,0,0,1
638,SPARK-968,SPARK-969,Fixed,In stage UI; add an overview section that shows task stats grouped by executor id,Persistent web ui,1,1,1
639,SPARK-970,SPARK-971,Fixed,PySpark's saveAsTextFile() throws UnicodeEncodeError when saving unicode strings,Link to Confluence wiki from project website / documentation,0,1,1
640,SPARK-971,SPARK-972,Fixed,Link to Confluence wiki from project website / documentation,"PySpark's ""cannot run multiple SparkContexts at once"" message should give source locations",0,0,1
641,SPARK-972,SPARK-973,Fixed,"PySpark's ""cannot run multiple SparkContexts at once"" message should give source locations",Mark fields in RDD class that are not used in workers as @transient to reduce task size,0,0,1
642,SPARK-973,SPARK-974,Fixed,Mark fields in RDD class that are not used in workers as @transient to reduce task size,error in script spark-0.8.0-incubating/make-distribution.sh,0,0,1
643,SPARK-974,SPARK-975,Fixed,error in script spark-0.8.0-incubating/make-distribution.sh,Spark Replay Debugger,0,0,1
644,SPARK-975,SPARK-976,Won't Fix,Spark Replay Debugger,WikipediaPageRand doesn't work anymore,0,1,1
645,SPARK-976,SPARK-977,Won't Fix,WikipediaPageRand doesn't work anymore,Add ZippedRDD / zip to PySpark,0,1,1
646,SPARK-977,SPARK-978,Fixed,Add ZippedRDD / zip to PySpark,PySpark's cartesian method throws ClassCastException exception,1,1,1
647,SPARK-978,SPARK-979,Fixed,PySpark's cartesian method throws ClassCastException exception,Add some randomization to scheduler to better balance in-memory partition distributions,0,1,1
648,SPARK-979,SPARK-980,Fixed,Add some randomization to scheduler to better balance in-memory partition distributions,NullPointerException for single-host setup with S3 URLs,0,1,1
649,SPARK-980,SPARK-981,Fixed,NullPointerException for single-host setup with S3 URLs,"Seemingly spurious ""Duplicate worker ID"" error messages",0,1,1
650,SPARK-982,SPARK-983,Fixed,Typo on Hadoop third-party page,Support external sorting for RDD#sortByKey(),0,0,1
651,SPARK-984,SPARK-985,Fixed,SPARK_TOOLS_JAR not set if multiple tools jars exists,Support Job Cancellation on Mesos Scheduler,0,0,1
652,SPARK-985,SPARK-986,Fixed,Support Job Cancellation on Mesos Scheduler,Add job cancellation to PySpark,0,1,1
653,SPARK-986,SPARK-987,Fixed,Add job cancellation to PySpark,Cannot start workers successfully with hadoop 2.2.0,0,0,1
654,SPARK-987,SPARK-988,Not A Problem,Cannot start workers successfully with hadoop 2.2.0,Write PySpark profiling guide,0,0,1
655,SPARK-988,SPARK-989,Fixed,Write PySpark profiling guide,Executors table in application web ui is wrong,0,1,1
656,SPARK-989,SPARK-990,Fixed,Executors table in application web ui is wrong,JavaPairRDD.top produces ClassNotFoundException,0,1,1
657,SPARK-990,SPARK-991,Incomplete,JavaPairRDD.top produces ClassNotFoundException,Report call sites of operators in Python,0,1,1
658,SPARK-991,SPARK-992,Fixed,Report call sites of operators in Python,Implement toString for Python and Java RDDs,0,0,1
659,SPARK-992,SPARK-993,Fixed,Implement toString for Python and Java RDDs,Don't reuse Writable objects in HadoopRDDs by default,1,0,1
660,SPARK-995,SPARK-996,Fixed,Improve Support for YARN 2.2,Delete Underlying Blocks When Cleaning Shuffle Meta-Data,0,1,1
661,SPARK-996,SPARK-997,Fixed,Delete Underlying Blocks When Cleaning Shuffle Meta-Data,Write integration tests for HDFS-based recovery,0,1,1
662,SPARK-997,SPARK-998,Fixed,Write integration tests for HDFS-based recovery,Support Launching Driver Inside of Standalone Mode,0,1,1
663,SPARK-998,SPARK-999,Fixed,Support Launching Driver Inside of Standalone Mode,Report More Instrumentation for Task Execution Time in UI,0,1,1
664,SPARK-999,SPARK-1000,Fixed,Report More Instrumentation for Task Execution Time in UI,Crash when running SparkPi example with local-cluster,0,1,1
665,SPARK-1000,SPARK-1001,Cannot Reproduce,Crash when running SparkPi example with local-cluster,Memory leak when reading sequence file and then sorting,0,1,1
666,SPARK-1001,SPARK-1002,Cannot Reproduce,Memory leak when reading sequence file and then sorting,Remove binary artifacts from build,0,0,1
667,SPARK-1002,SPARK-1003,Fixed,Remove binary artifacts from build,Spark application is blocked when running on yarn,0,0,1
668,SPARK-1003,SPARK-1004,Fixed,Spark application is blocked when running on yarn,PySpark on YARN,0,0,1
669,SPARK-1004,SPARK-1005,Fixed,PySpark on YARN,Update Ning compress package to 1.0.0,0,0,1
670,SPARK-1005,SPARK-1006,Fixed,Update Ning compress package to 1.0.0,MLlib ALS gets stack overflow with too many iterations,0,0,1
671,SPARK-1007,SPARK-1008,Fixed,spark-class2.cmd should change SCALA_VERSION to be 2.10,Provide good default logging if log4j properties is not present,0,0,1
672,SPARK-1008,SPARK-1009,Fixed,Provide good default logging if log4j properties is not present,Updated MLlib docs to show how to use it in Python,0,1,1
673,SPARK-1009,SPARK-1010,Fixed,Updated MLlib docs to show how to use it in Python,Update all unit tests to use SparkConf instead of system properties,0,0,1
674,SPARK-1011,SPARK-1012,Fixed,MatrixFactorizationModel in pyspark throws serialization error,DAGScheduler Exception,0,1,1
675,SPARK-1012,SPARK-1013,Fixed,DAGScheduler Exception,Have DEVELOPERS.txt file with documentation for developers,0,1,1
676,SPARK-1015,SPARK-1016,Won't Fix,Visualize the DAG of RDD ,When running examples jar (compiled with maven) logs don't initialize properly,0,1,1
677,SPARK-1016,SPARK-1017,Cannot Reproduce,When running examples jar (compiled with maven) logs don't initialize properly,Set the permgen even if we are calling the users sbt (via SBT_OPTS),0,1,1
678,SPARK-1017,SPARK-1018,Won't Fix,Set the permgen even if we are calling the users sbt (via SBT_OPTS),take and collect don't work on HadoopRDD,0,1,1
679,SPARK-1019,SPARK-1020,Fixed,pyspark RDD take() throws NPE,SparkListener interfaces should not expose internal types/objects,0,1,1
680,SPARK-1021,SPARK-1022,Won't Fix,sortByKey() launches a cluster job when it shouldn't,Add unit tests for kafka streaming,0,1,1
681,SPARK-1022,SPARK-1023,Fixed,Add unit tests for kafka streaming,Remove Thread.sleep(5000) from TaskSchedulerImpl,1,1,1
682,SPARK-1023,SPARK-1024,Fixed,Remove Thread.sleep(5000) from TaskSchedulerImpl,-XX:+UseCompressedStrings is actually dropped in jdk7,0,0,1
683,SPARK-1024,SPARK-1025,Fixed,-XX:+UseCompressedStrings is actually dropped in jdk7,pyspark hangs when parent base file is unavailable,0,0,1
684,SPARK-1025,SPARK-1026,Fixed,pyspark hangs when parent base file is unavailable,PySpark using deprecated mapPartitionsWithSplit,0,0,1
685,SPARK-1026,SPARK-1027,Fixed,PySpark using deprecated mapPartitionsWithSplit,Standalone cluster should use default spark home if not specified by user,0,0,1
686,SPARK-1027,SPARK-1028,Fixed,Standalone cluster should use default spark home if not specified by user,spark-shell automatically set MASTER  fails,0,1,1
687,SPARK-1028,SPARK-1029,Fixed,spark-shell automatically set MASTER  fails,spark Window shell script errors regarding shell script location reference,0,0,1
688,SPARK-1029,SPARK-1030,Fixed,spark Window shell script errors regarding shell script location reference,unneeded file required when running pyspark program using yarn-client,0,0,1
689,SPARK-1030,SPARK-1031,Fixed,unneeded file required when running pyspark program using yarn-client,FileNotFoundException when running simple Spark app on Yarn,0,1,1
690,SPARK-1031,SPARK-1032,Fixed,FileNotFoundException when running simple Spark app on Yarn,If Yarn app fails before registering; app master stays around long after,0,1,1
691,SPARK-1032,SPARK-1033,Fixed,If Yarn app fails before registering; app master stays around long after,Ask for cores in Yarn container requests ,1,1,1
692,SPARK-1033,SPARK-1034,Fixed,Ask for cores in Yarn container requests ,Py4JException on PySpark Cartesian Result,0,1,1
693,SPARK-1034,SPARK-1035,Fixed,Py4JException on PySpark Cartesian Result,Use a single mechanism for distributing jars on Yarn,0,1,1
694,SPARK-1035,SPARK-1036,Won't Fix,Use a single mechanism for distributing jars on Yarn,.gitignore is overly aggressive,0,1,1
695,SPARK-1036,SPARK-1037,Fixed,.gitignore is overly aggressive,the name of findTaskFromList & findTask in TaskSetManager.scala is confusing,0,0,1
696,SPARK-1037,SPARK-1038,Fixed,the name of findTaskFromList & findTask in TaskSetManager.scala is confusing,Add more fields in JsonProtocol and add tests that verify the JSON itself,0,0,1
697,SPARK-1038,SPARK-1039,Fixed,Add more fields in JsonProtocol and add tests that verify the JSON itself,In-cluster driver will retry infinitely when failed to start unless the user kill it manually,1,1,1
698,SPARK-1039,SPARK-1040,Won't Fix,In-cluster driver will retry infinitely when failed to start unless the user kill it manually,Collect as Map throws a casting exception when run on a JavaPairRDD object,0,0,1
699,SPARK-1040,SPARK-1041,Fixed,Collect as Map throws a casting exception when run on a JavaPairRDD object,ec2-related lines in start-*.sh no longer work ,0,0,1
700,SPARK-1041,SPARK-1042,Fixed,ec2-related lines in start-*.sh no longer work ,spark cleans all java broadcast variables when it hits the spark.cleaner.ttl ,0,0,1
701,SPARK-1042,SPARK-1043,Fixed,spark cleans all java broadcast variables when it hits the spark.cleaner.ttl ,Pyspark RDD's cannot deal with strings greater than 64K bytes.,0,0,1
702,SPARK-1043,SPARK-1044,Fixed,Pyspark RDD's cannot deal with strings greater than 64K bytes.,Default spark logs location in EC2 AMI leads to out-of-disk space pretty soon,0,0,1
703,SPARK-1044,SPARK-1045,Won't Fix,Default spark logs location in EC2 AMI leads to out-of-disk space pretty soon,ExternalAppendOnlyMap Iterator throw no such element on joining two large rdd,0,0,1
704,SPARK-1046,SPARK-1047,Fixed,Enable to build behind a proxy.,Ability to disable the spark ui server (unit tests),0,1,1
705,SPARK-1048,SPARK-1049,Duplicate,Create spark-site.xml or spark-site.yaml for configuration,spark on yarn - yarn-client mode doesn't always exit properly,0,0,1
706,SPARK-1049,SPARK-1050,Fixed,spark on yarn - yarn-client mode doesn't always exit properly,Investigate AnyRefMap,0,0,1
707,SPARK-1050,SPARK-1051,Won't Fix,Investigate AnyRefMap,On Yarn; executors don't doAs as submitting user,0,0,1
708,SPARK-1051,SPARK-1052,Fixed,On Yarn; executors don't doAs as submitting user,Spark on Mesos with CDH4.5.0 cannot start the Tasks properly,0,1,1
709,SPARK-1052,SPARK-1053,Fixed,Spark on Mesos with CDH4.5.0 cannot start the Tasks properly,Should not require SPARK_YARN_APP_JAR when running on YARN,0,1,1
710,SPARK-1053,SPARK-1054,Fixed,Should not require SPARK_YARN_APP_JAR when running on YARN,Get Cassandra support in Spark Core/Spark Cassandra Module,0,1,1
711,SPARK-1054,SPARK-1055,Won't Fix,Get Cassandra support in Spark Core/Spark Cassandra Module,Spark version on Dockerfile does not match release,0,1,1
712,SPARK-1055,SPARK-1056,Fixed,Spark version on Dockerfile does not match release,Header comment in Executor incorrectly implies it's not used for YARN,0,0,1
713,SPARK-1056,SPARK-1057,Fixed,Header comment in Executor incorrectly implies it's not used for YARN,Remove Fastutil,0,0,1
714,SPARK-1057,SPARK-1058,Fixed,Remove Fastutil,Fix Style Errors and Add Scala Style to Spark Build,0,1,1
715,SPARK-1058,SPARK-1059,Fixed,Fix Style Errors and Add Scala Style to Spark Build,Now that we submit core requests to YARN; fix usage message in ClientArguments,0,0,1
716,SPARK-1059,SPARK-1060,Duplicate,Now that we submit core requests to YARN; fix usage message in ClientArguments,JettyUtil is not using host information to start server,0,0,1
717,SPARK-1060,SPARK-1061,Fixed,JettyUtil is not using host information to start server,allow Hadoop RDDs to be read w/ a partitioner,0,1,1
718,SPARK-1061,SPARK-1062,Won't Fix,allow Hadoop RDDs to be read w/ a partitioner,Add rdd.intersection(otherRdd) method,1,1,1
719,SPARK-1062,SPARK-1063,Fixed,Add rdd.intersection(otherRdd) method,Add .sortBy(f) method on RDD,1,1,1
720,SPARK-1063,SPARK-1064,Fixed,Add .sortBy(f) method on RDD,Make it possible to use cluster's Hadoop jars when running against YARN,0,1,1
721,SPARK-1064,SPARK-1065,Fixed,Make it possible to use cluster's Hadoop jars when running against YARN,PySpark runs out of memory with large broadcast variables,0,1,1
722,SPARK-1065,SPARK-1066,Fixed,PySpark runs out of memory with large broadcast variables,Improve Developer Documentation,0,1,1
723,SPARK-1068,SPARK-1069,Fixed,Worker.scala should kill drivers in method postStop(),Provide binary compatibility in Spark 1.X releases,0,1,1
724,SPARK-1069,SPARK-1070,Fixed,Provide binary compatibility in Spark 1.X releases,Add check for JIRA ticket in the Github pull request title/summary with CI,0,0,1
725,SPARK-1070,SPARK-1071,Won't Fix,Add check for JIRA ticket in the Github pull request title/summary with CI,Tidy logging strategy and use of log4j,0,1,1
726,SPARK-1072,SPARK-1073,Fixed,Use binary search for RangePartitioner when there is more than 1000 partitions,GitHub PR squasher has bad titles,0,0,1
727,SPARK-1073,SPARK-1074,Fixed,GitHub PR squasher has bad titles,JavaPairRDD as Object File,0,0,1
728,SPARK-1074,SPARK-1075,Not A Problem,JavaPairRDD as Object File,Fix simple doc typo in Spark Streaming Custom Receiver,0,1,1
729,SPARK-1075,SPARK-1076,Fixed,Fix simple doc typo in Spark Streaming Custom Receiver,Adding zipWithIndex and zipWithUniqueId to RDD,0,0,1
730,SPARK-1076,SPARK-1077,Fixed,Adding zipWithIndex and zipWithUniqueId to RDD,Highlighted codes are not displayed properly in docs,0,0,1
731,SPARK-1077,SPARK-1078,Won't Fix,Highlighted codes are not displayed properly in docs,Replace lift-json with json4s-jackson,0,1,1
732,SPARK-1078,SPARK-1079,Fixed,Replace lift-json with json4s-jackson,EC2 scripts should allow mounting as XFS or EXT4,0,0,1
733,SPARK-1079,SPARK-1080,Won't Fix,EC2 scripts should allow mounting as XFS or EXT4,ZK PersistenceEngine does not respect zookeeper dir,0,1,1
734,SPARK-1080,SPARK-1081,Fixed,ZK PersistenceEngine does not respect zookeeper dir,Allow inferring number of cores with local[*],0,0,1
735,SPARK-1081,SPARK-1082,Fixed,Allow inferring number of cores with local[*],Use Curator for ZK interaction in standalone cluster,0,0,1
736,SPARK-1082,SPARK-1083,Won't Fix,Use Curator for ZK interaction in standalone cluster,Build fail,0,1,1
737,SPARK-1083,SPARK-1084,Cannot Reproduce,Build fail,Fix most build warnings,0,0,1
738,SPARK-1084,SPARK-1085,Fixed,Fix most build warnings,Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found),0,0,1
739,SPARK-1085,SPARK-1086,Fixed,Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found),Some corner case during HA master switching?,0,1,1
740,SPARK-1087,SPARK-1088,Fixed,Separate file for traceback and callsite related functions,Create a script for running tests,0,1,1
741,SPARK-1088,SPARK-1089,Fixed,Create a script for running tests,ADD_JARS regression in Spark 0.9.0,0,0,1
742,SPARK-1089,SPARK-1090,Fixed,ADD_JARS regression in Spark 0.9.0,spark-shell should print help information about parameters and should allow user to configure exe memory,0,0,1
743,SPARK-1090,SPARK-1091,Fixed,spark-shell should print help information about parameters and should allow user to configure exe memory,Cloudpickle does not work correctly for some methods that use a splat,0,1,1
744,SPARK-1092,SPARK-1093,Won't Fix,SparkContext should not read SPARK_MEM to set memory usage of executors,API Stability in Spark 1.X (Umbrella),1,0,1
745,SPARK-1093,SPARK-1094,Fixed,API Stability in Spark 1.X (Umbrella),Enforce Binary Compatibility in Spark Build,1,0,1
746,SPARK-1094,SPARK-1095,Fixed,Enforce Binary Compatibility in Spark Build,Ensure all public methods return explicit types,1,1,1
747,SPARK-1096,SPARK-1097,Fixed,Add Scalastyle Plug-in For Spaces after Comments,ConcurrentModificationException,0,1,1
748,SPARK-1098,SPARK-1099,Fixed,Cleanup and document ClassTag stuff in Java API,Add a new small files input for MLlib; which will return an RDD[(fileName; content)],0,0,1
749,SPARK-1099,SPARK-1100,Fixed,Add a new small files input for MLlib; which will return an RDD[(fileName; content)],saveAsTextFile shouldn't clobber by default,0,0,1
750,SPARK-1100,SPARK-1101,Fixed,saveAsTextFile shouldn't clobber by default,Umbrella for hardening Spark on YARN,0,1,1
751,SPARK-1101,SPARK-1102,Fixed,Umbrella for hardening Spark on YARN,Create a saveAsNewAPIHadoopDataset method,0,0,1
752,SPARK-1102,SPARK-1103,Fixed,Create a saveAsNewAPIHadoopDataset method,Garbage collect RDD information inside of Spark,0,0,1
753,SPARK-1103,SPARK-1104,Fixed,Garbage collect RDD information inside of Spark,Worker should not block while killing executors,0,0,1
754,SPARK-1104,SPARK-1105,Fixed,Worker should not block while killing executors,config.yml has an error version number,0,1,1
755,SPARK-1105,SPARK-1106,Fixed,config.yml has an error version number,check key name and identity file before launch a cluster,0,0,1
756,SPARK-1106,SPARK-1107,Fixed,check key name and identity file before launch a cluster,Add shutdown hook on executor stop to stop running tasks,0,0,1
757,SPARK-1107,SPARK-1108,Unresolved,Add shutdown hook on executor stop to stop running tasks,saveAsNewAPIHadoopFile throws NPE with TableOutputFormat,1,0,1
758,SPARK-1108,SPARK-1109,Fixed,saveAsNewAPIHadoopFile throws NPE with TableOutputFormat,wrong API docs for pyspark map function,0,0,1
759,SPARK-1109,SPARK-1110,Fixed,wrong API docs for pyspark map function,Clean up and clarify use of SPARK_HOME,0,1,1
760,SPARK-1110,SPARK-1111,Fixed,Clean up and clarify use of SPARK_HOME,URL Validation Throws Error for HDFS URL's,0,0,1
761,SPARK-1111,SPARK-1112,Fixed,URL Validation Throws Error for HDFS URL's,When spark.akka.frameSize > 10; task results bigger than 10MiB block execution,0,1,1
762,SPARK-1112,SPARK-1113,Fixed,When spark.akka.frameSize > 10; task results bigger than 10MiB block execution,External Spilling Bug - hash collision causes NoSuchElementException,0,1,1
763,SPARK-1114,SPARK-1115,Fixed,Patch to allow PySpark to use existing JVM and Gateway,Better error message when python worker process dies,1,0,1
764,SPARK-1115,SPARK-1116,Fixed,Better error message when python worker process dies,The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution.,0,1,1
765,SPARK-1116,SPARK-1117,Cannot Reproduce,The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution.,Update accumulator docs,1,0,1
766,SPARK-1117,SPARK-1118,Fixed,Update accumulator docs,Executor state shows as KILLED even the application is finished normally,0,0,1
767,SPARK-1119,SPARK-1120,Fixed,Make examples and assembly jar naming consistent between maven/sbt,Send all dependency logging through slf4j,0,1,1
768,SPARK-1121,SPARK-1122,Fixed,Only add avro if the build is for Hadoop 0.23.X and SPARK_YARN is set,Collect the RDD and send to each partition to form a new RDD,0,0,1
769,SPARK-1122,SPARK-1123,Won't Fix,Collect the RDD and send to each partition to form a new RDD,saveAsNewAPIHadoopFile throws java.lang.InstantiationException all the time,1,0,1
770,SPARK-1123,SPARK-1124,Cannot Reproduce,saveAsNewAPIHadoopFile throws java.lang.InstantiationException all the time,Infinite NullPointerException failures due to a null in map output locations,1,1,1
771,SPARK-1124,SPARK-1125,Fixed,Infinite NullPointerException failures due to a null in map output locations,The maven build error for Spark Examples,0,1,1
772,SPARK-1125,SPARK-1126,Fixed,The maven build error for Spark Examples,spark-submit script for running compiled binaries,0,0,1
773,SPARK-1126,SPARK-1127,Fixed,spark-submit script for running compiled binaries,Add saveAsHBase to PairRDDFunctions,1,0,1
774,SPARK-1128,SPARK-1129,Fixed,hadoop task properties not set while using InputFormat,use a predefined seed when seed is zero in XORShiftRandom,0,1,1
775,SPARK-1129,SPARK-1130,Fixed,use a predefined seed when seed is zero in XORShiftRandom,Clean up project documentation navigation menu,1,0,1
776,SPARK-1130,SPARK-1131,Fixed,Clean up project documentation navigation menu,Better document the --args option for yarn-standalone mode,0,0,1
777,SPARK-1131,SPARK-1132,Fixed,Better document the --args option for yarn-standalone mode,Persisting Web UI through refactoring the SparkListener interface,0,0,1
778,SPARK-1133,SPARK-1134,Fixed,Use Iterable[X] in co-group and group-by signatures,ipython won't run standalone python script,0,0,1
779,SPARK-1134,SPARK-1135,Fixed,ipython won't run standalone python script,Anchors broken in latest docs due to bad JavaScript code,0,0,1
780,SPARK-1135,SPARK-1136,Fixed,Anchors broken in latest docs due to bad JavaScript code,Fix FaultToleranceTest for Docker 0.8.1,0,1,1
781,SPARK-1136,SPARK-1137,Fixed,Fix FaultToleranceTest for Docker 0.8.1,ZK Persistence Engine crashes if stored data has wrong serialVersionUID,0,0,1
782,SPARK-1137,SPARK-1138,Fixed,ZK Persistence Engine crashes if stored data has wrong serialVersionUID,Spark 0.9.0 does not work with Hadoop / HDFS,0,1,1
783,SPARK-1138,SPARK-1139,Cannot Reproduce,Spark 0.9.0 does not work with Hadoop / HDFS,APIs like saveAsNewAPIHadoopFile are actually a mixture of old and new Hadoop API,0,1,1
784,SPARK-1139,SPARK-1140,Won't Fix,APIs like saveAsNewAPIHadoopFile are actually a mixture of old and new Hadoop API,Remove references to ClusterScheduler,0,0,1
785,SPARK-1140,SPARK-1141,Fixed,Remove references to ClusterScheduler,Parallelize Task Serialization,1,0,1
786,SPARK-1142,SPARK-1143,Not A Problem,Allow adding jars on app submission; outside of code,ClusterSchedulerSuite (soon to be TaskSchedulerImplSuite) does not actually test the ClusterScheduler/TaskSchedulerImpl,0,0,1
787,SPARK-1143,SPARK-1144,Fixed,ClusterSchedulerSuite (soon to be TaskSchedulerImplSuite) does not actually test the ClusterScheduler/TaskSchedulerImpl,Run Apache RAT In SBT Build to Catch License Errors,0,0,1
788,SPARK-1144,SPARK-1145,Fixed,Run Apache RAT In SBT Build to Catch License Errors,Memory mapping with many small blocks can cause JVM allocation failures,0,0,1
789,SPARK-1145,SPARK-1146,Fixed,Memory mapping with many small blocks can cause JVM allocation failures,Vagrant to setup Spark cluster locally,0,1,1
790,SPARK-1146,SPARK-1147,Won't Fix,Vagrant to setup Spark cluster locally,spark-project.org still goes to http://spark.incubator.apache.org/,0,1,1
791,SPARK-1147,SPARK-1148,Fixed,spark-project.org still goes to http://spark.incubator.apache.org/,Suggestions for exception handling (avoid potential bugs),0,1,1
792,SPARK-1149,SPARK-1150,Fixed,Bad partitioners can cause Spark to hang,repo location in create_release script out of date,0,1,1
793,SPARK-1150,SPARK-1151,Fixed,repo location in create_release script out of date,dev merge_spark_pr.py still references incubator-spark,0,0,1
794,SPARK-1151,SPARK-1152,Fixed,dev merge_spark_pr.py still references incubator-spark,ArrayStoreException on mapping RDD on cluster,0,0,1
795,SPARK-1155,SPARK-1156,Invalid,Clean up and document use of SparkEnv,Allow spark-ec2 to login to a cluster with 0 slaves,0,0,1
796,SPARK-1156,SPARK-1157,Fixed,Allow spark-ec2 to login to a cluster with 0 slaves,L-BFGS Optimizer,0,0,1
797,SPARK-1157,SPARK-1158,Fixed,L-BFGS Optimizer,Fix flaky RateLimitedOutputStreamSuite,1,1,1
798,SPARK-1158,SPARK-1159,Fixed,Fix flaky RateLimitedOutputStreamSuite,Add Shortest-path computations to graphx.lib,0,0,1
799,SPARK-1160,SPARK-1161,Fixed,Deprecate RDD.toArray,Add saveAsObjectFile and SparkContext.objectFile in Python,0,0,1
800,SPARK-1161,SPARK-1162,Fixed,Add saveAsObjectFile and SparkContext.objectFile in Python,Add top() and takeOrdered() to PySpark,1,0,1
801,SPARK-1162,SPARK-1163,Fixed,Add top() and takeOrdered() to PySpark,Miscellaneous missing PySpark methods,1,0,1
802,SPARK-1163,SPARK-1164,Fixed,Miscellaneous missing PySpark methods,Deprecate RDD.reduceByKeyToDriver,0,0,1
803,SPARK-1164,SPARK-1166,Fixed,Deprecate RDD.reduceByKeyToDriver,leftover vpc_id may block the creation of new ec2 cluster,0,0,1
804,SPARK-1166,SPARK-1167,Cannot Reproduce,leftover vpc_id may block the creation of new ec2 cluster,Remove metrics-ganglia from default build due to LGPL issue,0,0,1
805,SPARK-1167,SPARK-1168,Fixed,Remove metrics-ganglia from default build due to LGPL issue,Add foldByKey to PySpark,0,0,1
806,SPARK-1168,SPARK-1169,Fixed,Add foldByKey to PySpark,Add countApproxDistinctByKey to PySpark,1,1,1
807,SPARK-1169,SPARK-1170,Won't Fix,Add countApproxDistinctByKey to PySpark,Add histogram() to PySpark,1,1,1
808,SPARK-1171,SPARK-1172,Fixed,when executor is removed; we should reduce totalCores instead of just freeCores on that executor ,Improve naming of the BlockManager classes,0,0,1
809,SPARK-1172,SPARK-1173,Won't Fix,Improve naming of the BlockManager classes,Improve scala streaming docs,0,1,1
810,SPARK-1173,SPARK-1174,Fixed,Improve scala streaming docs,Adding port configuration for HttpFileServer,0,1,1
811,SPARK-1174,SPARK-1175,Duplicate,Adding port configuration for HttpFileServer,on shutting down a long running job; the cluster does not accept new jobs and gets hung,1,0,1
812,SPARK-1175,SPARK-1176,Fixed,on shutting down a long running job; the cluster does not accept new jobs and gets hung,Adding port configuration for HttpBroadcast,1,0,1
813,SPARK-1176,SPARK-1177,Fixed,Adding port configuration for HttpBroadcast,Allow SPARK_JAR to be set in system properties,1,1,1
814,SPARK-1177,SPARK-1178,Fixed,Allow SPARK_JAR to be set in system properties,missing document about spark.scheduler.revive.interval,0,0,1
815,SPARK-1178,SPARK-1179,Fixed,missing document about spark.scheduler.revive.interval,Allow SPARK_YARN_APP_JAR to be set in system properties,0,0,1
816,SPARK-1179,SPARK-1180,Won't Fix,Allow SPARK_YARN_APP_JAR to be set in system properties,Allow to provide a custom persistence engine,0,1,1
817,SPARK-1181,SPARK-1182,Won't Fix,'mvn test' fails out of the box since sbt assembly does not necessarily exist,Sort the configuration parameters in configuration.md,0,0,1
818,SPARK-1182,SPARK-1183,Fixed,Sort the configuration parameters in configuration.md,"Inconsistent meaning of ""worker"" in docs",1,0,1
819,SPARK-1183,SPARK-1184,Fixed,"Inconsistent meaning of ""worker"" in docs",Update the distribution tar.gz to include spark-assembly jar,0,1,1
820,SPARK-1184,SPARK-1185,Fixed,Update the distribution tar.gz to include spark-assembly jar,"In Spark Programming Guide; ""Master URLs"" should mention yarn-client",0,1,1
821,SPARK-1185,SPARK-1186,Fixed,"In Spark Programming Guide; ""Master URLs"" should mention yarn-client",Enrich the Spark Shell to support additional arguments.,0,1,1
822,SPARK-1186,SPARK-1187,Fixed,Enrich the Spark Shell to support additional arguments.,Missing Pyspark methods,0,1,1
823,SPARK-1187,SPARK-1188,Fixed,Missing Pyspark methods,GraphX triplets not working properly,0,1,1
824,SPARK-1189,SPARK-1190,Fixed,Support authentication between Spark components,Do not initialize log4j if slf4j log4j backend is not being used,1,0,1
825,SPARK-1191,SPARK-1192,Won't Fix,Convert configs to use SparkConf,Around 30 parameters in Spark are used but undocumented and some are having confusing name,0,1,1
826,SPARK-1192,SPARK-1193,Won't Fix,Around 30 parameters in Spark are used but undocumented and some are having confusing name,Inconsistent indendation between pom.xmls,0,1,1
827,SPARK-1193,SPARK-1194,Fixed,Inconsistent indendation between pom.xmls,The same-RDD rule for cache replacement is not properly implemented,0,1,1
828,SPARK-1194,SPARK-1195,Fixed,The same-RDD rule for cache replacement is not properly implemented,set map_input_file environment variable in PipedRDD,1,1,1
829,SPARK-1195,SPARK-1196,Fixed,set map_input_file environment variable in PipedRDD,val variables not available within RDD map on cluster app; are on shell or local,1,1,1
830,SPARK-1196,SPARK-1197,Cannot Reproduce,val variables not available within RDD map on cluster app; are on shell or local,Rename yarn-standalone and fix up docs for running on YARN,0,1,1
831,SPARK-1197,SPARK-1198,Fixed,Rename yarn-standalone and fix up docs for running on YARN,Allow pipes tasks to run in different sub-directories,0,1,1
832,SPARK-1198,SPARK-1199,Fixed,Allow pipes tasks to run in different sub-directories,Type mismatch in Spark shell when using case class defined in shell,1,0,1
833,SPARK-1200,SPARK-1201,Won't Fix,Make it possible to use unmanaged AM in yarn-client mode,Do not materialize partitions whenever possible in BlockManager,0,1,1
834,SPARK-1202,SPARK-1203,Fixed,"Add a ""cancel"" button in the UI for stages",spark-shell on yarn-client race in properly getting hdfs delegation tokens,0,0,1
835,SPARK-1203,SPARK-1204,Fixed,spark-shell on yarn-client race in properly getting hdfs delegation tokens,EC2 scripts upload private key,0,0,1
836,SPARK-1204,SPARK-1205,Fixed,EC2 scripts upload private key,Clean up callSite/origin/generator,0,0,1
837,SPARK-1205,SPARK-1206,Fixed,Clean up callSite/origin/generator,Add python support for  average and other summary satistics,0,0,1
838,SPARK-1206,SPARK-1207,Implemented,Add python support for  average and other summary satistics,Make python support for histograms,1,1,1
839,SPARK-1208,SPARK-1209,Fixed,after some hours of working the :4040 monitoring UI stops working.,SparkHadoop{MapRed;MapReduce}Util should not use package org.apache.hadoop,0,1,1
840,SPARK-1209,SPARK-1210,Fixed,SparkHadoop{MapRed;MapReduce}Util should not use package org.apache.hadoop,Prevent ContextClassLoader of Actor from becoming ClassLoader of Executor,1,0,1
841,SPARK-1211,SPARK-1212,Fixed,"In ApplicationMaster; set spark.master system property to ""yarn-cluster""",Support sparse data in MLlib,0,0,1
842,SPARK-1212,SPARK-1213,Fixed,Support sparse data in MLlib,loss function error of logistic loss,1,0,1
843,SPARK-1213,SPARK-1214,Fixed,loss function error of logistic loss,0-1 labels ,1,1,1
844,SPARK-1214,SPARK-1215,Fixed,0-1 labels ,Clustering: Index out of bounds error,1,1,1
845,SPARK-1217,SPARK-1218,Fixed,Add proximal gradient updater.,Minibatch SGD with random sampling,1,1,1
846,SPARK-1218,SPARK-1219,Fixed,Minibatch SGD with random sampling,Minibatch SGD with disjoint partitions,1,1,1
847,SPARK-1219,SPARK-1220,Fixed,Minibatch SGD with disjoint partitions,Principal Component Analysis,1,1,1
848,SPARK-1220,SPARK-1221,Fixed,Principal Component Analysis,SVMs (+ regularized variants),1,1,1
849,SPARK-1221,SPARK-1222,Fixed,SVMs (+ regularized variants),Logistic Regression (+ regularized variants),1,1,1
850,SPARK-1222,SPARK-1223,Fixed,Logistic Regression (+ regularized variants),Linear Regression (+ regularized variants),1,1,1
851,SPARK-1223,SPARK-1224,Fixed,Linear Regression (+ regularized variants),Improving Documentation for MLlib,1,1,1
852,SPARK-1224,SPARK-1225,Fixed,Improving Documentation for MLlib,ROC AUC and Average Precision for Binary classification models,1,0,1
853,SPARK-1225,SPARK-1226,Fixed,ROC AUC and Average Precision for Binary classification models,Import breeze to Spark mllib,1,0,1
854,SPARK-1226,SPARK-1227,Fixed,Import breeze to Spark mllib,Diagnostics for Classification&Regression,1,1,1
855,SPARK-1227,SPARK-1228,Won't Fix,Diagnostics for Classification&Regression,confusion matrix,1,1,1
856,SPARK-1228,SPARK-1229,Implemented,confusion matrix,train on array (in addition to RDD),1,1,1
857,SPARK-1229,SPARK-1230,Not A Problem,train on array (in addition to RDD),Enable SparkContext.addJars() to load classes not in CLASSPATH,0,1,1
858,SPARK-1230,SPARK-1231,Incomplete,Enable SparkContext.addJars() to load classes not in CLASSPATH,DEAD worker should recover automaticly,0,1,1
859,SPARK-1232,SPARK-1233,Fixed,maven hadoop 0.23 yarn-alpha build broken,spark on hadoop 0.23 yarn fails to run: java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH,0,1,1
860,SPARK-1233,SPARK-1234,Fixed,spark on hadoop 0.23 yarn fails to run: java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH,clean up typos and grammar issues in Spark on YARN page,0,0,1
861,SPARK-1234,SPARK-1235,Won't Fix,clean up typos and grammar issues in Spark on YARN page,DAGScheduler ignores exceptions thrown in handleTaskCompletion,0,0,1
862,SPARK-1235,SPARK-1236,Fixed,DAGScheduler ignores exceptions thrown in handleTaskCompletion,Update Jetty to 9,0,0,1
863,SPARK-1237,SPARK-1238,Fixed,Implicit ALS is not efficient in computing YtY,User should be able to set a random seed in ALS,1,1,1
864,SPARK-1238,SPARK-1239,Fixed,User should be able to set a random seed in ALS,Improve fetching of map output statuses,0,1,1
865,SPARK-1239,SPARK-1240,Fixed,Improve fetching of map output statuses,takeSample called on empty RDD never ends,0,1,1
866,SPARK-1240,SPARK-1241,Fixed,takeSample called on empty RDD never ends,Support sliding in RDD,0,1,1
867,SPARK-1241,SPARK-1242,Fixed,Support sliding in RDD,Add aggregate to python API,0,0,1
868,SPARK-1242,SPARK-1243,Fixed,Add aggregate to python API,spark compilation error,0,0,1
869,SPARK-1243,SPARK-1244,Fixed,spark compilation error,Log an exception if map output status message exceeds frame size,0,1,1
870,SPARK-1244,SPARK-1245,Fixed,Log an exception if map output status message exceeds frame size,Can't read EMR HBase cluster from properly built Cloudera Spark Cluster.,0,1,1
871,SPARK-1245,SPARK-1246,Not A Problem,Can't read EMR HBase cluster from properly built Cloudera Spark Cluster.,Add min max to the stat counter.,0,0,1
872,SPARK-1246,SPARK-1247,Fixed,Add min max to the stat counter.,Support some of the RDD double functions on float and int as well.,1,1,1
873,SPARK-1247,SPARK-1248,Not A Problem,Support some of the RDD double functions on float and int as well.,Spark build error with Apache Hadoop(Cloudera CDH4),0,0,1
874,SPARK-1248,SPARK-1249,Fixed,Spark build error with Apache Hadoop(Cloudera CDH4),Cannot create graphx.Graph with no edges,0,1,1
875,SPARK-1249,SPARK-1250,Won't Fix,Cannot create graphx.Graph with no edges,Misleading comments in Spark startup scripts,0,0,1
876,SPARK-1250,SPARK-1251,Fixed,Misleading comments in Spark startup scripts,Support for optimizing and executing structured queries,0,0,1
877,SPARK-1251,SPARK-1252,Fixed,Support for optimizing and executing structured queries,On YARN; use container-log4j.properties for executors,0,0,1
878,SPARK-1252,SPARK-1253,Fixed,On YARN; use container-log4j.properties for executors,Need to load mapred-site.xml for reading mapreduce.application.classpath,1,0,1
879,SPARK-1253,SPARK-1254,Cannot Reproduce,Need to load mapred-site.xml for reading mapreduce.application.classpath,Consolidate; order; and harmonize repository declarations in Maven/SBT builds,0,0,1
880,SPARK-1254,SPARK-1255,Fixed,Consolidate; order; and harmonize repository declarations in Maven/SBT builds,Allow user to pass Serializer object instead of class name for shuffle.,0,0,1
881,SPARK-1255,SPARK-1256,Fixed,Allow user to pass Serializer object instead of class name for shuffle.,Master web UI and Worker web UI returns a 404 error,1,1,1
882,SPARK-1257,SPARK-1258,Fixed,Endless running task when using pyspark with input file containing a long line,RDD.countByValue optimization,0,0,1
883,SPARK-1258,SPARK-1259,Won't Fix,RDD.countByValue optimization,Make RDD locally iterable,1,0,1
884,SPARK-1259,SPARK-1260,Fixed,Make RDD locally iterable,faster construction of features with intercept,0,0,1
885,SPARK-1260,SPARK-1261,Fixed,faster construction of features with intercept,Docs don't explain how to run Python examples,0,1,1
886,SPARK-1261,SPARK-1262,Fixed,Docs don't explain how to run Python examples,Numerical drift in computation of matrix inverse leads to invalid results in ALS,0,1,1
887,SPARK-1262,SPARK-1263,Cannot Reproduce,Numerical drift in computation of matrix inverse leads to invalid results in ALS,Implicit ALS unnecessarily recomputes factor matrices,1,1,1
888,SPARK-1264,SPARK-1265,Won't Fix,Documentation for setting heap sizes across all configurations,Fix 404 not found error in UI introduced in Jetty 9.0 upgrade,0,1,1
889,SPARK-1266,SPARK-1267,Fixed,Persist factors in implicit ALS,Add a pip installer for PySpark,0,0,1
890,SPARK-1267,SPARK-1268,Fixed,Add a pip installer for PySpark,Adding XOR and AND-NOT operations to spark.util.collection.BitSet,0,1,1
891,SPARK-1268,SPARK-1269,Fixed,Adding XOR and AND-NOT operations to spark.util.collection.BitSet,Add Ability to Make Distribution with Tachyon,0,0,1
892,SPARK-1269,SPARK-1270,Fixed,Add Ability to Make Distribution with Tachyon,An optimized gradient descent implementation,0,1,1
893,SPARK-1270,SPARK-1271,Won't Fix,An optimized gradient descent implementation,[STREAMING] Annotate developer and experimental API's,0,0,1
894,SPARK-1272,SPARK-1273,Unresolved,Don't fail job if some local directories are buggy,MLlib v0.9.1 release,0,1,1
895,SPARK-1273,SPARK-1274,Fixed,MLlib v0.9.1 release,Add dev scripts to merge PRs and create releases from master to branch-0.9,0,1,1
896,SPARK-1274,SPARK-1275,Fixed,Add dev scripts to merge PRs and create releases from master to branch-0.9,Made SPARK_HOME/dev/tests executable,1,1,1
897,SPARK-1275,SPARK-1276,Fixed,Made SPARK_HOME/dev/tests executable,Add a Simple History Server for the UI for Yarn / Mesos,0,0,1
898,SPARK-1276,SPARK-1277,Fixed,Add a Simple History Server for the UI for Yarn / Mesos,Automatically set the UI persistence directory based on cluster settings,1,1,1
899,SPARK-1277,SPARK-1278,Won't Fix,Automatically set the UI persistence directory based on cluster settings,Improper use of SimpleDateFormat,1,0,1
900,SPARK-1278,SPARK-1279,Fixed,Improper use of SimpleDateFormat,"Stage.name return  ""apply at Option.scala:120""",0,0,1
901,SPARK-1280,SPARK-1281,Fixed,"Stage.name return ""apply at Option.scala:120""",Partitioning in ALS,0,0,1
902,SPARK-1281,SPARK-1282,Fixed,Partitioning in ALS,Create spark-contrib repo for 1.0,0,0,1
903,SPARK-1284,SPARK-1285,Fixed,pyspark hangs after IOError on Executor,Back porting streaming doc updates to 0.9,0,0,1
904,SPARK-1285,SPARK-1286,Fixed,Back porting streaming doc updates to 0.9,Make usage of spark-env.sh idempotent,0,1,1
905,SPARK-1286,SPARK-1287,Fixed,Make usage of spark-env.sh idempotent,yarn alpha and stable Client calculateAMMemory routines are different,0,0,1
906,SPARK-1288,SPARK-1289,Fixed,yarn stable finishApplicationMaster incomplete,improve yarn stable error logging when passing bad arguments,1,1,1
907,SPARK-1289,SPARK-1290,Won't Fix,improve yarn stable error logging when passing bad arguments,PythonAccumulatorParam throws uncaught exception,0,1,1
908,SPARK-1290,SPARK-1291,Won't Fix,PythonAccumulatorParam throws uncaught exception,Link the spark UI to RM ui in yarn-client mode,0,1,1
909,SPARK-1291,SPARK-1292,Fixed,Link the spark UI to RM ui in yarn-client mode,In-Memory Columnar Representation for Catalyst,0,1,1
910,SPARK-1292,SPARK-1293,Fixed,In-Memory Columnar Representation for Catalyst,Support for reading/writing complex types in Parquet,1,1,1
911,SPARK-1293,SPARK-1294,Fixed,Support for reading/writing complex types in Parquet,Case insensitive resolution in HiveContext breaks the ability to access fields with upper case letters,1,1,1
912,SPARK-1294,SPARK-1295,Fixed,Case insensitive resolution in HiveContext breaks the ability to access fields with upper case letters,Progress values on Spark UI progress bar wander off-screen,0,0,1
913,SPARK-1295,SPARK-1296,Fixed,Progress values on Spark UI progress bar wander off-screen,Make RDDs Covariant,0,0,1
914,SPARK-1298,SPARK-1299,Fixed,Remove duplicate partition id checking,making comments of RDD.doCheckpoint consistent with its usage,1,0,1
915,SPARK-1299,SPARK-1300,Fixed,making comments of RDD.doCheckpoint consistent with its usage,Clean-up and clarify private vs public fields in MLLib,0,0,1
916,SPARK-1303,SPARK-1304,Won't Fix,Added discretization capability to MLlib.,Job fails with spot instances (due to IllegalStateException: Shutdown in progress),0,0,1
917,SPARK-1305,SPARK-1306,Fixed,Support persisting RDD's directly to Tachyon,no instructions provided for sbt assembly with Hadoop 2.2,0,0,1
918,SPARK-1306,SPARK-1307,Fixed,no instructions provided for sbt assembly with Hadoop 2.2,don't use term 'standalone' to refer to a Spark Application,1,1,1
919,SPARK-1307,SPARK-1308,Fixed,don't use term 'standalone' to refer to a Spark Application,Add getNumPartitions() method to PySpark RDDs,0,0,1
920,SPARK-1308,SPARK-1309,Fixed,Add getNumPartitions() method to PySpark RDDs,sbt assemble-deps no longer works,0,0,1
921,SPARK-1310,SPARK-1311,Fixed,Add support for  cross validation to MLLibb,Use map side distinct in collect vertex ids from edges graphx,0,1,1
922,SPARK-1311,SPARK-1312,Fixed,Use map side distinct in collect vertex ids from edges graphx,Batch should read based on the batch interval provided in the StreamingContext,0,0,1
923,SPARK-1312,SPARK-1313,Fixed,Batch should read based on the batch interval provided in the StreamingContext,Shark- JDBC driver ,0,0,1
924,SPARK-1313,SPARK-1314,Not A Problem,Shark- JDBC driver ,sbt assembly builds hive jar,0,0,1
925,SPARK-1314,SPARK-1315,Fixed,sbt assembly builds hive jar,spark on yarn-alpha with mvn on master branch won't build,0,0,1
926,SPARK-1316,SPARK-1317,Fixed,Remove use of Commons IO,sbt doesn't work for building Spark programs,0,0,1
927,SPARK-1317,SPARK-1318,Not A Problem,sbt doesn't work for building Spark programs,Alignment of the Spark Shell with Spark Submit.,0,1,1
928,SPARK-1318,SPARK-1319,Fixed,Alignment of the Spark Shell with Spark Submit.,The current code effectively ignores spark.task.cpus,0,1,1
929,SPARK-1319,SPARK-1320,Fixed,The current code effectively ignores spark.task.cpus,cogroup and groupby should pass an iterator,0,1,1
930,SPARK-1321,SPARK-1322,Fixed,Use Guava's top k implementation rather than our custom priority queue,Fix ordering of top() in Python,0,0,1
931,SPARK-1322,SPARK-1323,Fixed,Fix ordering of top() in Python,Job hangs with java.io.UTFDataFormatException when reading strings > 65536 bytes. ,1,0,1
932,SPARK-1323,SPARK-1324,Fixed,Job hangs with java.io.UTFDataFormatException when reading strings > 65536 bytes. ,Spark UI Should Not Try to Bind to SPARK_PUBLIC_DNS,0,0,1
933,SPARK-1324,SPARK-1325,Fixed,Spark UI Should Not Try to Bind to SPARK_PUBLIC_DNS,The maven build error for Spark Tools,0,0,1
934,SPARK-1326,SPARK-1327,Fixed,make-distribution.sh's Tachyon support relies on GNU sed,GLM needs to check addIntercept for intercept and weights,0,0,1
935,SPARK-1327,SPARK-1328,Fixed,GLM needs to check addIntercept for intercept and weights,Current implementation of Standard Deviation in MLUtils may cause catastrophic cancellation; and loss precision.,1,1,1
936,SPARK-1328,SPARK-1329,Fixed,Current implementation of Standard Deviation in MLUtils may cause catastrophic cancellation; and loss precision.,ArrayIndexOutOfBoundsException if graphx.Graph has more edge partitions than node partitions,0,1,1
937,SPARK-1329,SPARK-1330,Fixed,ArrayIndexOutOfBoundsException if graphx.Graph has more edge partitions than node partitions,compute_classpath.sh has extra echo which prevents spark-class from working,0,0,1
938,SPARK-1330,SPARK-1331,Fixed,compute_classpath.sh has extra echo which prevents spark-class from working,Graceful shutdown of Spark Streaming computation,0,1,1
939,SPARK-1331,SPARK-1332,Fixed,Graceful shutdown of Spark Streaming computation,Improve Spark Streaming's Network Receiver and InputDStream API for future stability,1,1,1
940,SPARK-1332,SPARK-1333,Fixed,Improve Spark Streaming's Network Receiver and InputDStream API for future stability,Java API for running SQL queries,0,1,1
941,SPARK-1333,SPARK-1334,Fixed,Java API for running SQL queries,RDD names should be settable from PySpark,0,0,1
942,SPARK-1334,SPARK-1335,Not A Problem,RDD names should be settable from PySpark,Also increase perm gen / code cache for scalatest when invoked via Maven build,0,0,1
943,SPARK-1335,SPARK-1336,Fixed,Also increase perm gen / code cache for scalatest when invoked via Maven build,Reduce Verbosity of QA Scripts,1,1,1
944,SPARK-1336,SPARK-1337,Fixed,Reduce Verbosity of QA Scripts,Application web UI garbage collects newest stages instead old ones,0,1,1
945,SPARK-1338,SPARK-1339,Fixed,Create Additional Style Rules,Build error: org.eclipse.paho:mqtt-client,0,1,1
946,SPARK-1339,SPARK-1340,Not A Problem,Build error: org.eclipse.paho:mqtt-client,Some Spark Streaming receivers are not restarted when worker fails,0,0,1
947,SPARK-1340,SPARK-1341,Fixed,Some Spark Streaming receivers are not restarted when worker fails,Ability to control the data rate in Spark Streaming ,1,0,1
948,SPARK-1341,SPARK-1342,Fixed,Ability to control the data rate in Spark Streaming ,Bump Scala version to 2.10.4,0,0,1
949,SPARK-1342,SPARK-1343,Done,Bump Scala version to 2.10.4,PySpark OOMs without caching,0,0,1
950,SPARK-1343,SPARK-1344,Fixed,PySpark OOMs without caching,Scala API docs for top methods,0,1,1
951,SPARK-1344,SPARK-1345,Fixed,Scala API docs for top methods,spark on yarn 0.23 using maven doesn't build,0,0,1
952,SPARK-1345,SPARK-1346,Fixed,spark on yarn 0.23 using maven doesn't build,Backport SPARK-1210 into 0.9 branch,0,0,1
953,SPARK-1346,SPARK-1347,Fixed,Backport SPARK-1210 into 0.9 branch,SHARK error when running in server mode: java.net.BindException: Address already in use,0,1,1
954,SPARK-1347,SPARK-1348,Won't Fix,SHARK error when running in server mode: java.net.BindException: Address already in use,Spark UI's do not bind to localhost interface anymore,0,0,1
955,SPARK-1348,SPARK-1349,Fixed,Spark UI's do not bind to localhost interface anymore,spark-shell's repl history is shared with the scala repl,1,0,1
956,SPARK-1349,SPARK-1350,Fixed,spark-shell's repl history is shared with the scala repl,YARN ContainerLaunchContext should use cluster's JAVA_HOME,0,0,1
957,SPARK-1350,SPARK-1351,Fixed,YARN ContainerLaunchContext should use cluster's JAVA_HOME,Documentation Improvements for Spark 1.0,0,0,1
958,SPARK-1351,SPARK-1352,Fixed,Documentation Improvements for Spark 1.0,Improve robustness of spark-submit script,0,0,1
959,SPARK-1352,SPARK-1353,Fixed,Improve robustness of spark-submit script,IllegalArgumentException when writing to disk,0,0,1
960,SPARK-1354,SPARK-1355,Fixed,Fail to resolve attribute when query with table name as a qualifer in SQLContext,Switch website to the Apache CMS,0,1,1
961,SPARK-1355,SPARK-1356,Invalid,Switch website to the Apache CMS,[MLLIB] Annotate developer and experimental API's,0,0,1
962,SPARK-1356,SPARK-1357,Fixed,[MLLIB] Annotate developer and experimental API's,[streaming] Add deployment subsection to streaming,0,0,1
963,SPARK-1358,SPARK-1359,Won't Fix,Continuous integrated test should be involved in Spark ecosystem ,SGD implementation is not efficient,0,1,1
964,SPARK-1359,SPARK-1360,Unresolved,SGD implementation is not efficient,Add Timestamp Support,0,0,1
965,SPARK-1360,SPARK-1361,Fixed,Add Timestamp Support,DAGScheduler throws NullPointerException occasionally,0,0,1
966,SPARK-1361,SPARK-1362,Fixed,DAGScheduler throws NullPointerException occasionally,Web UI should provide page of showing statistics and stage list for a given job,0,1,1
967,SPARK-1364,SPARK-1365,Fixed,DataTypes missing from ScalaReflection,Fix RateLimitedOutputStream Test,0,0,1
968,SPARK-1365,SPARK-1366,Fixed,Fix RateLimitedOutputStream Test,The sql function should be consistent between different types of SQLContext,0,0,1
969,SPARK-1366,SPARK-1367,Fixed,The sql function should be consistent between different types of SQLContext,NPE when joining Parquet Relations,1,1,1
970,SPARK-1367,SPARK-1368,Fixed,NPE when joining Parquet Relations,HiveTableScan is slow,1,0,1
971,SPARK-1368,SPARK-1369,Fixed,HiveTableScan is slow,HiveUDF wrappers are slow,1,1,1
972,SPARK-1369,SPARK-1370,Won't Fix,HiveUDF wrappers are slow,HashJoin should stream one relation,1,1,1
973,SPARK-1370,SPARK-1371,Fixed,HashJoin should stream one relation,HashAggregate should stream tuples and avoid doing an extra count,1,0,1
974,SPARK-1371,SPARK-1372,Fixed,HashAggregate should stream tuples and avoid doing an extra count,Expose in-memory columnar caching for tables.,1,1,1
975,SPARK-1372,SPARK-1373,Fixed,Expose in-memory columnar caching for tables.,Compression for In-Memory Columnar storage,1,1,1
976,SPARK-1373,SPARK-1374,Fixed,Compression for In-Memory Columnar storage,Python API for running SQL queries,1,1,1
977,SPARK-1374,SPARK-1375,Fixed,Python API for running SQL queries,spark-submit script additional cleanup,0,0,1
978,SPARK-1375,SPARK-1376,Fixed,spark-submit script additional cleanup,"In the yarn-cluster submitter; rename ""args"" option to ""arg""",0,1,1
979,SPARK-1376,SPARK-1377,Fixed,"In the yarn-cluster submitter; rename ""args"" option to ""arg""",Upgrade Jetty to 8.1.14v20131031,0,1,1
980,SPARK-1378,SPARK-1379,Not A Problem,Build error: org.eclipse.paho:mqtt-client,Calling .cache() on a SchemaRDD should do something more efficient than caching the individual row objects.,0,1,1
981,SPARK-1379,SPARK-1380,Fixed,Calling .cache() on a SchemaRDD should do something more efficient than caching the individual row objects.,Add sort-merge based cogroup/joins.,0,1,1
982,SPARK-1380,SPARK-1381,Won't Fix,Add sort-merge based cogroup/joins.,Spark to Shark direct streaming,0,1,1
983,SPARK-1381,SPARK-1382,Won't Fix,Spark to Shark direct streaming,NullPointerException when calling DStream.slice() before StreamingContext.start(),0,0,1
984,SPARK-1382,SPARK-1383,Fixed,NullPointerException when calling DStream.slice() before StreamingContext.start(),Spark-SQL: ParquetRelation improvements,0,0,1
985,SPARK-1383,SPARK-1384,Fixed,Spark-SQL: ParquetRelation improvements,spark-shell on yarn on spark 0.9 branch doesn't always work with secure hdfs,0,1,1
986,SPARK-1384,SPARK-1385,Fixed,spark-shell on yarn on spark 0.9 branch doesn't always work with secure hdfs,Use existing code-path for JSON de/serialization of BlockId,0,0,1
987,SPARK-1385,SPARK-1386,Fixed,Use existing code-path for JSON de/serialization of BlockId,Spark Streaming UI,0,0,1
988,SPARK-1386,SPARK-1387,Fixed,Spark Streaming UI,Update build plugins;  avoid plugin version warning; centralize versions,0,0,1
989,SPARK-1387,SPARK-1388,Fixed,Update build plugins;  avoid plugin version warning; centralize versions,ConcurrentModificationException in hadoop_common exposed by Spark,0,0,1
990,SPARK-1389,SPARK-1390,Fixed,Make numPartitions in Exchange configurable,Refactor RDD backed matrices,0,0,1
991,SPARK-1390,SPARK-1391,Fixed,Refactor RDD backed matrices,BlockManager cannot transfer blocks larger than 2G in size,0,0,1
992,SPARK-1393,SPARK-1394,Fixed,fix computePreferredLocations signature to not depend on underlying implementation,calling system.platform on worker raises IOError,0,1,1
993,SPARK-1394,SPARK-1395,Fixed,calling system.platform on worker raises IOError,"Cannot launch jobs on Yarn cluster with ""local:"" scheme in SPARK_JAR",0,1,1
994,SPARK-1395,SPARK-1396,Fixed,"Cannot launch jobs on Yarn cluster with ""local:"" scheme in SPARK_JAR",DAGScheduler has a memory leak for cancelled jobs,0,1,1
995,SPARK-1396,SPARK-1397,Fixed,DAGScheduler has a memory leak for cancelled jobs,SparkListeners should be notified when stages are cancelled,1,0,1
996,SPARK-1397,SPARK-1398,Fixed,SparkListeners should be notified when stages are cancelled,Remove FindBugs jsr305 dependency,0,0,1
997,SPARK-1398,SPARK-1399,Won't Fix,Remove FindBugs jsr305 dependency,Reason for Stage Failure should be shown in UI,0,0,1
998,SPARK-1399,SPARK-1400,Fixed,Reason for Stage Failure should be shown in UI,Spark Streaming's received data is not cleaned up from BlockManagers when not needed any more,0,1,1
999,SPARK-1402,SPARK-1403,Fixed,3 more compression algorithms for in-memory columnar storage,Spark on Mesos does not set Thread's context class loader,0,1,1
1000,SPARK-1403,SPARK-1404,Fixed,Spark on Mesos does not set Thread's context class loader,Non-exported spark-env.sh variables are no longer present in spark-shell,1,0,1
1001,SPARK-1404,SPARK-1405,Fixed,Non-exported spark-env.sh variables are no longer present in spark-shell,parallel Latent Dirichlet Allocation (LDA) atop of spark in MLlib,0,0,1
1002,SPARK-1406,SPARK-1407,Fixed,PMML model evaluation support via MLib,EventLogging to HDFS doesn't work properly on yarn,0,0,1
1003,SPARK-1408,SPARK-1409,Fixed,Modify Spark on Yarn to point to the history server when app finishes,"Flaky Test: ""actor input stream"" test in org.apache.spark.streaming.InputStreamsSuite",0,1,1
1004,SPARK-1409,SPARK-1410,Won't Fix,"Flaky Test: ""actor input stream"" test in org.apache.spark.streaming.InputStreamsSuite",Class not found exception with application launched from sbt 0.13.x,0,1,1
1005,SPARK-1410,SPARK-1411,Fixed,Class not found exception with application launched from sbt 0.13.x,When using spark.ui.retainedStages=n only the first n stages are kept; not the most recent.,0,1,1
1006,SPARK-1412,SPARK-1413,Unresolved,Disable partial aggregation automatically when reduction factor is low,Parquet messes up stdout and stdin when used in Spark REPL,1,0,1
1007,SPARK-1413,SPARK-1414,Fixed,Parquet messes up stdout and stdin when used in Spark REPL,Python API for SparkContext.wholeTextFiles,0,0,1
1008,SPARK-1414,SPARK-1415,Fixed,Python API for SparkContext.wholeTextFiles,Add a minSplits parameter to wholeTextFiles,0,1,1
1009,SPARK-1415,SPARK-1416,Fixed,Add a minSplits parameter to wholeTextFiles,Add support for SequenceFiles and binary Hadoop InputFormats in PySpark,1,1,1
1010,SPARK-1416,SPARK-1417,Fixed,Add support for SequenceFiles and binary Hadoop InputFormats in PySpark,Spark on Yarn - spark UI link from resourcemanager is broken,0,0,1
1011,SPARK-1417,SPARK-1418,Fixed,Spark on Yarn - spark UI link from resourcemanager is broken,Python MLlib's _get_unmangled_rdd should uncache RDDs when training is done,0,0,1
1012,SPARK-1418,SPARK-1419,Implemented,Python MLlib's _get_unmangled_rdd should uncache RDDs when training is done,Apache parent POM to version 14,0,1,1
1013,SPARK-1419,SPARK-1420,Fixed,Apache parent POM to version 14,The maven build error for Spark Catalyst,0,1,1
1014,SPARK-1420,SPARK-1421,Fixed,The maven build error for Spark Catalyst,Make MLlib work on Python 2.6,0,1,1
1015,SPARK-1421,SPARK-1422,Fixed,Make MLlib work on Python 2.6,Add scripts for launching Spark on Google Compute Engine,0,1,1
1016,SPARK-1423,SPARK-1424,Won't Fix,Add scripts for launching Spark on Windows Azure,InsertInto should work on JavaSchemaRDD as well.,0,0,1
1017,SPARK-1424,SPARK-1425,Fixed,InsertInto should work on JavaSchemaRDD as well.,PySpark can crash Executors if worker.py fails while serializing data,0,0,1
1018,SPARK-1425,SPARK-1426,Not A Problem,PySpark can crash Executors if worker.py fails while serializing data,Make MLlib work with NumPy versions older than 1.7,0,1,1
1019,SPARK-1426,SPARK-1427,Fixed,Make MLlib work with NumPy versions older than 1.7,HQL Examples Don't Work,0,1,1
1020,SPARK-1427,SPARK-1428,Fixed,HQL Examples Don't Work,MLlib should convert non-float64 NumPy arrays to float64 instead of complaining,0,0,1
1021,SPARK-1428,SPARK-1429,Fixed,MLlib should convert non-float64 NumPy arrays to float64 instead of complaining,"Spark shell fails to start after ""sbt clean assemble-deps package""",0,0,1
1022,SPARK-1430,SPARK-1431,Fixed,Support sparse data in Python MLlib,Allow merging pull requests that have conflicts,0,1,1
1023,SPARK-1431,SPARK-1432,Fixed,Allow merging pull requests that have conflicts,Potential memory leak in stageIdToExecutorSummaries in JobProgressTracker,0,1,1
1024,SPARK-1432,SPARK-1433,Fixed,Potential memory leak in stageIdToExecutorSummaries in JobProgressTracker,Upgrade Mesos dependency to 0.17.0,0,0,1
1025,SPARK-1434,SPARK-1435,Fixed,Make labelParser Java friendly.,Don't assume context class loader is set when creating classes via reflection,0,0,1
1026,SPARK-1436,SPARK-1437,Fixed,Compression code broke in-memory store,Jenkins should build with Java 6,0,0,1
1027,SPARK-1437,SPARK-1438,Implemented,Jenkins should build with Java 6,Update RDD.sample() API to make seed parameter optional,0,0,1
1028,SPARK-1438,SPARK-1439,Fixed,Update RDD.sample() API to make seed parameter optional,Aggregate Scaladocs across projects,0,0,1
1029,SPARK-1439,SPARK-1440,Fixed,Aggregate Scaladocs across projects,Generate JavaDoc instead of ScalaDoc for Java API,1,1,1
1030,SPARK-1440,SPARK-1441,Fixed,Generate JavaDoc instead of ScalaDoc for Java API,Compile Spark Core error with Hadoop 0.23.x when not using YARN,0,1,1
1031,SPARK-1441,SPARK-1442,Fixed,Compile Spark Core error with Hadoop 0.23.x when not using YARN,Add Window function support,0,0,1
1032,SPARK-1443,SPARK-1444,Done,Unable to Access MongoDB GridFS data with Spark using mongo-hadoop API,Update branch-0.9's SBT to 0.13.1 so that it works with Java 8,0,0,1
1033,SPARK-1445,SPARK-1446,Fixed,compute-classpath.sh should not print error if lib_managed not found,Spark examples should not do a System.exit,0,0,1
1034,SPARK-1446,SPARK-1447,Fixed,Spark examples should not do a System.exit,Update Java programming guide to explain Java 8 syntax,0,1,1
1035,SPARK-1447,SPARK-1448,Not A Problem,Update Java programming guide to explain Java 8 syntax,KEYS file to be added to dist directory,0,1,1
1036,SPARK-1448,SPARK-1449,Fixed,KEYS file to be added to dist directory,Please delete old releases from mirroring system,0,1,1
1037,SPARK-1449,SPARK-1450,Fixed,Please delete old releases from mirroring system,Specify the default zone in the EC2 script help,0,0,1
1038,SPARK-1450,SPARK-1451,Fixed,Specify the default zone in the EC2 script help,Multinomial Logistic Regression Support,0,0,1
1039,SPARK-1452,SPARK-1453,Won't Fix,dynamic partition creation not working on cached table,Improve the way Spark on Yarn waits for executors before starting,0,1,1
1040,SPARK-1454,SPARK-1455,Won't Fix,PySpark accumulators fail to update when runJob takes serialized/captured closures,Determine which test suites to run based on code changes,0,0,1
1041,SPARK-1456,SPARK-1457,Fixed,Clean up use of Ordered/Ordering in OrderedRDDFunctions,Change APIs for training algorithms to take optimizer as parameter ,0,0,1
1042,SPARK-1457,SPARK-1458,Won't Fix,Change APIs for training algorithms to take optimizer as parameter ,Expose sc.version in PySpark,0,1,1
1043,SPARK-1458,SPARK-1459,Fixed,Expose sc.version in PySpark,"EventLoggingListener does not work with ""file://"" target dir",0,0,1
1044,SPARK-1459,SPARK-1460,Fixed,"EventLoggingListener does not work with ""file://"" target dir",Set operations on SchemaRDDs are needlessly destructive of schema information.,0,1,1
1045,SPARK-1460,SPARK-1461,Fixed,Set operations on SchemaRDDs are needlessly destructive of schema information.,Support Short-circuit Expression Evaluation,0,1,1
1046,SPARK-1461,SPARK-1462,Fixed,Support Short-circuit Expression Evaluation,Examples of ML algorithms are using deprecated APIs,0,1,1
1047,SPARK-1463,SPARK-1464,Fixed,cleanup unnecessary dependency jars in the spark assembly jars,Update MLLib Examples to Use Breeze,0,0,1
1048,SPARK-1465,SPARK-1466,Fixed,Spark compilation is broken with the latest hadoop-2.4.0 release,Pyspark doesn't check if gateway process launches correctly,0,1,1
1049,SPARK-1466,SPARK-1467,Fixed,Pyspark doesn't check if gateway process launches correctly,Make StorageLevel.apply() factory methods developer API's,0,0,1
1050,SPARK-1467,SPARK-1468,Fixed,Make StorageLevel.apply() factory methods developer API's,The hash method used by partitionBy in Pyspark doesn't deal with None correctly.,0,1,1
1051,SPARK-1468,SPARK-1469,Fixed,The hash method used by partitionBy in Pyspark doesn't deal with None correctly.,Scheduler mode should accept lower-case definitions and have nicer error messages,0,1,1
1052,SPARK-1469,SPARK-1470,Fixed,Scheduler mode should accept lower-case definitions and have nicer error messages,Use the scala-logging wrapper instead of the directly sfl4j api,0,1,1
1053,SPARK-1470,SPARK-1471,Won't Fix,Use the scala-logging wrapper instead of the directly sfl4j api,Worker not  recognize Driver state at standalone mode,0,1,1
1054,SPARK-1471,SPARK-1472,Fixed,Worker not  recognize Driver state at standalone mode,Go through YARN api used in Spark to make sure we aren't using Private Apis,0,1,1
1055,SPARK-1472,SPARK-1473,Won't Fix,Go through YARN api used in Spark to make sure we aren't using Private Apis,Feature selection for high dimensional datasets,0,0,1
1056,SPARK-1473,SPARK-1474,Won't Fix,Feature selection for high dimensional datasets,Spark on yarn assembly doesn't include org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,0,0,1
1057,SPARK-1474,SPARK-1475,Fixed,Spark on yarn assembly doesn't include org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Drain event logging queue before stopping event logger,0,0,1
1058,SPARK-1475,SPARK-1476,Fixed,Drain event logging queue before stopping event logger,2GB limit in spark for blocks,1,0,1
1059,SPARK-1477,SPARK-1478,Won't Fix,Add the lifecycle interface,Upgrade FlumeInputDStream's FlumeReceiver to support FLUME-1915,0,0,1
1060,SPARK-1478,SPARK-1479,Fixed,Upgrade FlumeInputDStream's FlumeReceiver to support FLUME-1915,building spark on 2.0.0-cdh4.4.0 failed,0,0,1
1061,SPARK-1479,SPARK-1480,Won't Fix,building spark on 2.0.0-cdh4.4.0 failed,Choose classloader consistently inside of Spark codebase,0,0,1
1062,SPARK-1481,SPARK-1482,Fixed,Add Naive Bayes to MLlib documentation,Potential resource leaks in saveAsHadoopDataset and saveAsNewAPIHadoopDataset,0,0,1
1063,SPARK-1482,SPARK-1483,Fixed,Potential resource leaks in saveAsHadoopDataset and saveAsNewAPIHadoopDataset,Rename minSplits to minPartitions in public APIs,1,0,1
1064,SPARK-1483,SPARK-1484,Fixed,Rename minSplits to minPartitions in public APIs,MLlib should warn if you are using an iterative algorithm on non-cached data,0,0,1
1065,SPARK-1484,SPARK-1485,Fixed,MLlib should warn if you are using an iterative algorithm on non-cached data,Implement AllReduce,1,0,1
1066,SPARK-1485,SPARK-1486,Won't Fix,Implement AllReduce,Support multi-model training in MLlib,1,1,1
1067,SPARK-1486,SPARK-1487,Unresolved,Support multi-model training in MLlib,Support record filtering via predicate pushdown in Parquet,0,0,1
1068,SPARK-1487,SPARK-1488,Fixed,Support record filtering via predicate pushdown in Parquet,Resolve scalac feature warnings during build,0,0,1
1069,SPARK-1488,SPARK-1489,Fixed,Resolve scalac feature warnings during build,Fix the HistoryServer view acls,0,0,1
1070,SPARK-1489,SPARK-1490,Fixed,Fix the HistoryServer view acls,Add kerberos support to the HistoryServer,1,1,1
1071,SPARK-1490,SPARK-1491,Fixed,Add kerberos support to the HistoryServer,maven hadoop-provided profile fails to build,0,1,1
1072,SPARK-1491,SPARK-1492,Fixed,maven hadoop-provided profile fails to build,running-on-yarn doc should use spark-submit script for examples,0,0,1
1073,SPARK-1492,SPARK-1493,Fixed,running-on-yarn doc should use spark-submit script for examples,Apache RAT excludes don't work with file path (instead of file name),0,0,1
1074,SPARK-1493,SPARK-1494,Fixed,Apache RAT excludes don't work with file path (instead of file name),Hive Dependencies being checked by MIMA,0,0,1
1075,SPARK-1494,SPARK-1495,Fixed,Hive Dependencies being checked by MIMA,support leftsemijoin for sparkSQL,0,0,1
1076,SPARK-1495,SPARK-1496,Fixed,support leftsemijoin for sparkSQL,SparkContext.jarOfClass should return Option instead of a sequence,0,1,1
1077,SPARK-1496,SPARK-1497,Fixed,SparkContext.jarOfClass should return Option instead of a sequence,Spark YARN code isn't checked with Scalastyle and has many style violations,0,1,1
1078,SPARK-1497,SPARK-1498,Fixed,Spark YARN code isn't checked with Scalastyle and has many style violations,Spark can hang if pyspark tasks fail,0,1,1
1079,SPARK-1498,SPARK-1499,Fixed,Spark can hang if pyspark tasks fail,Workers continuously produce failing executors,0,1,1
1080,SPARK-1500,SPARK-1501,Fixed, add with-hive argument to make-distribution.sh,Assertions in Graph.apply test are never executed,0,0,1
1081,SPARK-1501,SPARK-1502,Fixed,Assertions in Graph.apply test are never executed,Spark on Yarn: add config option to not include yarn/mapred cluster classpath,0,0,1
1082,SPARK-1502,SPARK-1503,Unresolved,Spark on Yarn: add config option to not include yarn/mapred cluster classpath,Implement Nesterov's accelerated first-order method,0,1,1
1083,SPARK-1503,SPARK-1504,Unresolved,Implement Nesterov's accelerated first-order method,[streaming] Add 0.9 to 1.0 migration guide for streaming receiver,0,0,1
1084,SPARK-1504,SPARK-1505,Fixed,[streaming] Add 0.9 to 1.0 migration guide for streaming receiver,jblas's DoubleMatrix(double[]) ctor creates garbage; avoid,1,0,1
1085,SPARK-1505,SPARK-1506,Fixed,jblas's DoubleMatrix(double[]) ctor creates garbage; avoid,Documentation improvements for MLlib 1.0,0,0,1
1086,SPARK-1506,SPARK-1507,Fixed,Documentation improvements for MLlib 1.0,Spark on Yarn: Add support for user to specify # cores for ApplicationMaster,0,0,1
1087,SPARK-1507,SPARK-1508,Fixed,Spark on Yarn: Add support for user to specify # cores for ApplicationMaster,Add support for reading from SparkConf,0,1,1
1088,SPARK-1508,SPARK-1509,Fixed,Add support for reading from SparkConf,add zipWithIndex zipWithUniqueId methods to java api,0,0,1
1089,SPARK-1509,SPARK-1510,Fixed,add zipWithIndex zipWithUniqueId methods to java api,Add Spark Streaming metrics source for metrics system,0,0,1
1090,SPARK-1510,SPARK-1511,Fixed,Add Spark Streaming metrics source for metrics system,Update TestUtils.createCompiledClass() API to work with creating class file on different filesystem,0,0,1
1091,SPARK-1511,SPARK-1512,Fixed,Update TestUtils.createCompiledClass() API to work with creating class file on different filesystem,improve spark sql to support table with more than 22 fields,0,0,1
1092,SPARK-1512,SPARK-1513,Fixed,improve spark sql to support table with more than 22 fields,Specialized ColumnType for Timestamp,1,1,1
1093,SPARK-1514,SPARK-1515,Fixed,Standardize process for creating Spark packages,Specialized ColumnTypes for Array; Map and Struct,0,0,1
1094,SPARK-1515,SPARK-1516,Won't Fix,Specialized ColumnTypes for Array; Map and Struct,Yarn Client should not call System.exit; should throw exception instead.,0,1,1
1095,SPARK-1516,SPARK-1517,Fixed,Yarn Client should not call System.exit; should throw exception instead.,Publish nightly snapshots of documentation; maven artifacts; and binary builds,0,0,1
1096,SPARK-1518,SPARK-1519,Fixed,Spark master doesn't compile against hadoop-common trunk,support minPartitions parameter of wholeTextFiles() in pyspark,0,0,1
1097,SPARK-1519,SPARK-1520,Fixed,support minPartitions parameter of wholeTextFiles() in pyspark,Assembly Jar with more than 65536 files won't work when compiled on  JDK7 and run on JDK6,0,0,1
1098,SPARK-1521,SPARK-1522,Won't Fix,Take character set size into account when compressing in-memory string columns,YARN ClientBase will throw a NPE if there is no YARN application specific classpath.,0,0,1
1099,SPARK-1522,SPARK-1523,Fixed,YARN ClientBase will throw a NPE if there is no YARN application specific classpath.,improve the readability of code in AkkaUtil ,0,0,1
1100,SPARK-1523,SPARK-1524,Fixed,improve the readability of code in AkkaUtil ,TaskSetManager'd better not schedule tasks which has no preferred executorId using PROCESS_LOCAL in the first search process,0,0,1
1101,SPARK-1524,SPARK-1525,Won't Fix,TaskSetManager'd better not schedule tasks which has no preferred executorId using PROCESS_LOCAL in the first search process,TaskSchedulerImpl should decrease availableCpus by spark.task.cpus not 1,1,1,1
1102,SPARK-1525,SPARK-1526,Fixed,TaskSchedulerImpl should decrease availableCpus by spark.task.cpus not 1,Running spark driver program from my local machine,1,0,1
1103,SPARK-1526,SPARK-1527,Not A Problem,Running spark driver program from my local machine,rootDirs in DiskBlockManagerSuite doesn't get full path from rootDir0; rootDir1,1,0,1
1104,SPARK-1527,SPARK-1528,Fixed,rootDirs in DiskBlockManagerSuite doesn't get full path from rootDir0; rootDir1,Spark on Yarn: Add option for user to specify additional namenodes to get tokens from,0,0,1
1105,SPARK-1528,SPARK-1529,Fixed,Spark on Yarn: Add option for user to specify additional namenodes to get tokens from,Support DFS based shuffle in addition to Netty shuffle,0,1,1
1106,SPARK-1529,SPARK-1530,Won't Fix,Support DFS based shuffle in addition to Netty shuffle,Streaming UI test can hang indefinitely,0,1,1
1107,SPARK-1530,SPARK-1531,Fixed,Streaming UI test can hang indefinitely,GraphX should have messageRDD to enable OutOfCore messages,0,1,1
1108,SPARK-1531,SPARK-1532,Won't Fix,GraphX should have messageRDD to enable OutOfCore messages,provide option for more restrictive firewall rule in ec2/spark_ec2.py,0,0,1
1109,SPARK-1533,SPARK-1534,Won't Fix,The (kill) button in the web UI is visible to everyone.,spark-submit for yarn prints warnings even though calling as expected ,0,0,1
1110,SPARK-1535,SPARK-1536,Fixed,[streaming] Update custom receiver guide with new Receiver API,Add multiclass classification tree support to MLlib,1,0,1
1111,SPARK-1536,SPARK-1537,Fixed,Add multiclass classification tree support to MLlib,Add integration with Yarn's Application Timeline Server,0,0,1
1112,SPARK-1537,SPARK-1538,Unresolved,Add integration with Yarn's Application Timeline Server,SparkUI forgets about all persisted RDD's not directly associated with the Stage,0,0,1
1113,SPARK-1538,SPARK-1539,Fixed,SparkUI forgets about all persisted RDD's not directly associated with the Stage,RDDPage.scala contains RddPage,0,0,1
1114,SPARK-1539,SPARK-1540,Fixed,RDDPage.scala contains RddPage,Investigate whether we should require keys in PairRDD to be Comparable,0,0,1
1115,SPARK-1540,SPARK-1541,Fixed,Investigate whether we should require keys in PairRDD to be Comparable,sortByKey requires all data to fit in memory,0,0,1
1116,SPARK-1544,SPARK-1545,Fixed,Add support for deep decision trees.,Add Random Forest algorithm to MLlib,1,1,1
1117,SPARK-1545,SPARK-1546,Fixed,Add Random Forest algorithm to MLlib,Add AdaBoost algorithm to Spark MLlib,1,1,1
1118,SPARK-1548,SPARK-1549,Unresolved,Add Partial Random Forest algorithm to MLlib,Add python support to spark-submit script,0,0,1
1119,SPARK-1549,SPARK-1550,Fixed,Add python support to spark-submit script,Successive creation of spark context fails in pyspark; if the previous initialization of spark context had failed.,0,0,1
1120,SPARK-1550,SPARK-1551,Fixed,Successive creation of spark context fails in pyspark; if the previous initialization of spark context had failed.,Spark master does not build in sbt,0,1,1
1121,SPARK-1551,SPARK-1552,Not A Problem,Spark master does not build in sbt,GraphX performs type comparison incorrectly,0,1,1
1122,SPARK-1552,SPARK-1553,Fixed,GraphX performs type comparison incorrectly,Support alternating nonnegative least-squares,0,1,1
1123,SPARK-1553,SPARK-1554,Implemented,Support alternating nonnegative least-squares,Update doc overview page to not mention building if you get a pre-built distro,0,1,1
1124,SPARK-1554,SPARK-1555,Fixed,Update doc overview page to not mention building if you get a pre-built distro,enable ec2/spark_ec2.py to stop/delete cluster non-interactively,0,0,1
1125,SPARK-1555,SPARK-1556,Won't Fix,enable ec2/spark_ec2.py to stop/delete cluster non-interactively,jets3t dep doesn't update properly with newer Hadoop versions,0,0,1
1126,SPARK-1556,SPARK-1557,Fixed,jets3t dep doesn't update properly with newer Hadoop versions,Set permissions on event log files/directories,1,0,1
1127,SPARK-1557,SPARK-1558,Fixed,Set permissions on event log files/directories,[MLlib] ALS: Estimate communication and computation costs given a partitioner,0,0,1
1128,SPARK-1558,SPARK-1559,Fixed,[MLlib] ALS: Estimate communication and computation costs given a partitioner,Add conf dir to CLASSPATH in compute-classpath.sh dependent on whether SPARK_CONF_DIR is set,0,0,1
1129,SPARK-1560,SPARK-1561,Fixed,PySpark SQL depends on Java 7 only jars,sbt/sbt assembly generates too many local files,0,0,1
1130,SPARK-1561,SPARK-1562,Not A Problem,sbt/sbt assembly generates too many local files,Exclude internal catalyst classes from scaladoc; or make them package private,0,0,1
1131,SPARK-1562,SPARK-1563,Fixed,Exclude internal catalyst classes from scaladoc; or make them package private,Add package-info.java and package.scala files for all packages that appear in docs,0,0,1
1132,SPARK-1563,SPARK-1564,Fixed,Add package-info.java and package.scala files for all packages that appear in docs,Add JavaScript into Javadoc to turn ::Experimental:: and such into badges,1,0,1
1133,SPARK-1565,SPARK-1566,Fixed,Spark examples should be changed given spark-submit,Consolidate the Spark Programming Guide with tabs for all languages,0,1,1
1134,SPARK-1566,SPARK-1567,Fixed,Consolidate the Spark Programming Guide with tabs for all languages,Add language tabs to quick start guide,1,1,1
1135,SPARK-1567,SPARK-1568,Fixed,Add language tabs to quick start guide,Spark 0.9.0 hangs reading s3,0,1,1
1136,SPARK-1568,SPARK-1569,Fixed,Spark 0.9.0 hangs reading s3,Spark on Yarn; authentication broken by pr299,0,0,1
1137,SPARK-1569,SPARK-1570,Fixed,Spark on Yarn; authentication broken by pr299,Class loading issue when using Spark SQL Java API,0,1,1
1138,SPARK-1570,SPARK-1571,Fixed,Class loading issue when using Spark SQL Java API,UnresolvedException when running JavaSparkSQL example,0,1,1
1139,SPARK-1571,SPARK-1572,Fixed,UnresolvedException when running JavaSparkSQL example,Uncaught IO exceptions in Pyspark kill Executor,0,0,1
1140,SPARK-1572,SPARK-1573,Fixed,Uncaught IO exceptions in Pyspark kill Executor,slight modification with regards to sbt/sbt test,0,1,1
1141,SPARK-1573,SPARK-1574,Won't Fix,slight modification with regards to sbt/sbt test,ec2/spark_ec2.py should provide option to control number of attempts for ssh operations,0,0,1
1142,SPARK-1574,SPARK-1575,Fixed,ec2/spark_ec2.py should provide option to control number of attempts for ssh operations,failing tests with master branch ,0,1,1
1143,SPARK-1575,SPARK-1576,Cannot Reproduce,failing tests with master branch ,Passing of JAVA_OPTS to YARN on command line,1,0,1
1144,SPARK-1576,SPARK-1577,Not A Problem,Passing of JAVA_OPTS to YARN on command line,GraphX mapVertices with KryoSerialization,0,1,1
1145,SPARK-1577,SPARK-1578,Fixed,GraphX mapVertices with KryoSerialization,Do not require setting of cleaner TTL when creating StreamingContext,0,0,1
1146,SPARK-1578,SPARK-1579,Fixed,Do not require setting of cleaner TTL when creating StreamingContext,PySpark should distinguish expected IOExceptions from unexpected ones in the worker,0,0,1
1147,SPARK-1579,SPARK-1580,Fixed,PySpark should distinguish expected IOExceptions from unexpected ones in the worker,Compare Option[Partitioner] and Partitioner directly,0,0,1
1148,SPARK-1580,SPARK-1581,Fixed,Compare Option[Partitioner] and Partitioner directly,Allow One Flume Avro RPC Server for Each Worker rather than Just One Worker,0,1,1
1149,SPARK-1581,SPARK-1582,Won't Fix,Allow One Flume Avro RPC Server for Each Worker rather than Just One Worker,Job cancellation does not interrupt threads,0,0,1
1150,SPARK-1582,SPARK-1583,Fixed,Job cancellation does not interrupt threads,Use java.util.HashMap.remove by mistake in BlockManagerMasterActor.removeBlockManager,1,1,1
1151,SPARK-1583,SPARK-1584,Fixed,Use java.util.HashMap.remove by mistake in BlockManagerMasterActor.removeBlockManager,Upgrade Flume dependency to 1.4.0,0,1,1
1152,SPARK-1584,SPARK-1585,Fixed,Upgrade Flume dependency to 1.4.0,Not robust Lasso causes Infinity on weights and losses,0,1,1
1153,SPARK-1585,SPARK-1586,Fixed,Not robust Lasso causes Infinity on weights and losses,Fix issues with spark development under windows,0,1,1
1154,SPARK-1586,SPARK-1587,Fixed,Fix issues with spark development under windows,Fix thread leak in spark,1,1,1
1155,SPARK-1587,SPARK-1588,Fixed,Fix thread leak in spark,SPARK_JAVA_OPTS and SPARK_YARN_USER_ENV are not getting propagated,0,0,1
1156,SPARK-1588,SPARK-1589,Fixed,SPARK_JAVA_OPTS and SPARK_YARN_USER_ENV are not getting propagated,"EOFException when file size 0 exists when use sc.sequenceFile[K;V](""path"")",0,0,1
1157,SPARK-1589,SPARK-1590,Fixed,"EOFException when file size 0 exists when use sc.sequenceFile[K;V](""path"")",Recommend to use FindBugs,0,0,1
1158,SPARK-1590,SPARK-1591,Not A Problem,Recommend to use FindBugs,scala.MatchError executing custom UDTF,0,1,1
1159,SPARK-1591,SPARK-1592,Won't Fix,scala.MatchError executing custom UDTF,Old streaming input blocks not removed automatically from the BlockManagers,0,0,1
1160,SPARK-1592,SPARK-1593,Fixed,Old streaming input blocks not removed automatically from the BlockManagers,Add status command to Spark Daemons(master/worker),0,0,1
1161,SPARK-1593,SPARK-1594,Won't Fix,Add status command to Spark Daemons(master/worker),Cleaning up MLlib APIs and guide,0,0,1
1162,SPARK-1594,SPARK-1595,Fixed,Cleaning up MLlib APIs and guide,Remove VectorRDDs,1,0,1
1163,SPARK-1595,SPARK-1596,Fixed,Remove VectorRDDs,Re-arrange public methods in evaluation.,1,1,1
1164,SPARK-1596,SPARK-1597,Fixed,Re-arrange public methods in evaluation.,Add a version of reduceByKey that takes the Partitioner as a second argument,0,0,1
1165,SPARK-1597,SPARK-1598,Won't Fix,Add a version of reduceByKey that takes the Partitioner as a second argument,Mark main methods experimental,0,0,1
1166,SPARK-1599,SPARK-1600,Fixed,Allow to use intercept in Ridge and Lasso,"flaky ""recovery with file input stream"" test in streaming.CheckpointSuite",0,1,1
1167,SPARK-1600,SPARK-1601,Fixed,"flaky ""recovery with file input stream"" test in streaming.CheckpointSuite",CacheManager#getOrCompute() does not return an InterruptibleIterator,0,1,1
1168,SPARK-1601,SPARK-1602,Fixed,CacheManager#getOrCompute() does not return an InterruptibleIterator,Cancelled jobs can lead to corrupted cached partitions,1,0,1
1169,SPARK-1602,SPARK-1603,Fixed,Cancelled jobs can lead to corrupted cached partitions,Flaky test: o.a.s.streaming.StreamingContextSuite,0,1,1
1170,SPARK-1603,SPARK-1604,Cannot Reproduce,Flaky test: o.a.s.streaming.StreamingContextSuite,Couldn't run spark-submit with yarn cluster mode when built with assemble-deps,0,0,1
1171,SPARK-1604,SPARK-1605,Won't Fix,Couldn't run spark-submit with yarn cluster mode when built with assemble-deps,Improve mllib.linalg.Vector,0,1,1
1172,SPARK-1605,SPARK-1606,Won't Fix,Improve mllib.linalg.Vector,spark-submit needs `--arg` for every application parameter,0,0,1
1173,SPARK-1606,SPARK-1607,Fixed,spark-submit needs `--arg` for every application parameter,Remove use of octal literals; deprecated in Scala 2.10 / removed in 2.11,0,0,1
1174,SPARK-1607,SPARK-1608,Fixed,Remove use of octal literals; deprecated in Scala 2.10 / removed in 2.11,Cast.nullable should be true when cast from StringType to NumericType/TimestampType,0,0,1
1175,SPARK-1608,SPARK-1609,Fixed,Cast.nullable should be true when cast from StringType to NumericType/TimestampType,Executor fails to start when Command.extraJavaOptions contains multiple Java options,0,0,1
1176,SPARK-1610,SPARK-1611,Fixed,Cast from BooleanType to NumericType should use exact type value.,Incorrect initialization order in AppendOnlyMap,0,0,1
1177,SPARK-1611,SPARK-1612,Fixed,Incorrect initialization order in AppendOnlyMap,Potential resource leaks in Utils.copyStream and Utils.offsetBytes,1,1,1
1178,SPARK-1612,SPARK-1613,Fixed,Potential resource leaks in Utils.copyStream and Utils.offsetBytes,Difficulty starting up cluster on Amazon EC2,0,0,1
1179,SPARK-1613,SPARK-1614,Incomplete,Difficulty starting up cluster on Amazon EC2,Move Mesos protobufs out of TaskState,0,0,1
1180,SPARK-1614,SPARK-1615,Won't Fix,Move Mesos protobufs out of TaskState,Very subtle race condition in SparkListenerSuite,0,1,1
1181,SPARK-1615,SPARK-1616,Fixed,Very subtle race condition in SparkListenerSuite,input file not found issue ,0,0,1
1182,SPARK-1616,SPARK-1617,Not A Problem,input file not found issue ,Exposing receiver state and errors in the streaming UI,0,1,1
1183,SPARK-1617,SPARK-1618,Fixed,Exposing receiver state and errors in the streaming UI,Socket receiver not restarting properly when connection is refused,0,1,1
1184,SPARK-1618,SPARK-1619,Fixed,Socket receiver not restarting properly when connection is refused,Spark shell should use spark-submit,0,0,1
1185,SPARK-1619,SPARK-1620,Fixed,Spark shell should use spark-submit,Uncaught exception from Akka scheduler,0,1,1
1186,SPARK-1620,SPARK-1621,Fixed,Uncaught exception from Akka scheduler,Update Chill to 0.3.6,0,0,1
1187,SPARK-1621,SPARK-1622,Fixed,Update Chill to 0.3.6,Expose input split(s) accessed by a task in UI or logs,0,0,1
1188,SPARK-1622,SPARK-1623,Won't Fix,Expose input split(s) accessed by a task in UI or logs,SPARK-1623. Broadcast cleaner should use getCanonicalPath when deleting files by name,0,1,1
1189,SPARK-1623,SPARK-1624,Fixed,SPARK-1623. Broadcast cleaner should use getCanonicalPath when deleting files by name,addShutdownHook in DiskBlockManager Doesn't work the way it is supposed to work,1,0,1
1190,SPARK-1624,SPARK-1625,Not A Problem,addShutdownHook in DiskBlockManager Doesn't work the way it is supposed to work,Ensure all legacy YARN options are supported with spark-submit,0,1,1
1191,SPARK-1625,SPARK-1626,Fixed,Ensure all legacy YARN options are supported with spark-submit,Update Spark YARN docs to use spark-submit,1,1,1
1192,SPARK-1628,SPARK-1629,Fixed,Missing hashCode methods in Partitioner subclasses,Spark should inline use of commons-lang `SystemUtils.IS_OS_WINDOWS` ,1,1,1
1193,SPARK-1629,SPARK-1630,Fixed,Spark should inline use of commons-lang `SystemUtils.IS_OS_WINDOWS` ,PythonRDDs don't handle nulls gracefully,0,0,1
1194,SPARK-1630,SPARK-1631,Won't Fix,PythonRDDs don't handle nulls gracefully,App name set in SparkConf (not in JVM properties) not respected by Yarn backend,0,0,1
1195,SPARK-1631,SPARK-1632,Fixed,App name set in SparkConf (not in JVM properties) not respected by Yarn backend,Avoid boxing in ExternalAppendOnlyMap compares,0,0,1
1196,SPARK-1632,SPARK-1633,Fixed,Avoid boxing in ExternalAppendOnlyMap compares,Various examples for Scala and Java custom receiver; etc. ,0,1,1
1197,SPARK-1633,SPARK-1634,Fixed,Various examples for Scala and Java custom receiver; etc. ,Java API docs contain test cases,0,0,1
1198,SPARK-1634,SPARK-1635,Not A Problem,Java API docs contain test cases,Java API docs do not show annotation.,1,0,1
1199,SPARK-1637,SPARK-1638,Fixed,Clean up examples for 1.0,"Executors fail to come up if ""spark.executor.extraJavaOptions"" is set ",0,0,1
1200,SPARK-1639,SPARK-1640,Fixed,Some tidying of Spark on YARN code,In yarn-client mode; pass preferred node locations to AM,1,1,1
1201,SPARK-1640,SPARK-1641,Invalid,In yarn-client mode; pass preferred node locations to AM,Spark submit warning tells the user to use spark-submit,0,0,1
1202,SPARK-1642,SPARK-1643,Unresolved,Upgrade FlumeInputDStream's FlumeReceiver to support FLUME-2083,"hql(""show tables"") throw an exception",0,0,1
1203,SPARK-1643,SPARK-1644,Cannot Reproduce,"hql(""show tables"") throw an exception",The org.datanucleus:*  should not be packaged into spark-assembly-*.jar,1,1,1
1204,SPARK-1644,SPARK-1645,Fixed,The org.datanucleus:*  should not be packaged into spark-assembly-*.jar,Improve Spark Streaming compatibility with Flume,0,1,1
1205,SPARK-1645,SPARK-1646,Fixed,Improve Spark Streaming compatibility with Flume,ALS micro-optimisation,0,0,1
1206,SPARK-1646,SPARK-1647,Implemented,ALS micro-optimisation,Prevent data loss when Streaming driver goes down,0,0,1
1207,SPARK-1648,SPARK-1649,Fixed,Support closing JIRA's as part of merge script,Figure out Nullability semantics for Array elements and Map values,0,0,1
1208,SPARK-1649,SPARK-1650,Done,Figure out Nullability semantics for Array elements and Map values,Correctly identify maven project version in distribution script,0,0,1
1209,SPARK-1650,SPARK-1651,Fixed,Correctly identify maven project version in distribution script,Delete existing (tar) deployment directory in distribution script,0,0,1
1210,SPARK-1651,SPARK-1652,Fixed,Delete existing (tar) deployment directory in distribution script,Fixes and improvements for spark-submit/configs,0,0,1
1211,SPARK-1653,SPARK-1654,Fixed,Spark submit should not use SPARK_CLASSPATH internally (creates warnings),Spark shell doesn't correctly pass quoted arguments to spark-submit,0,1,1
1212,SPARK-1654,SPARK-1655,Fixed,Spark shell doesn't correctly pass quoted arguments to spark-submit,In naive Bayes; store conditional probabilities distributively.,0,1,1
1213,SPARK-1655,SPARK-1656,Unresolved,In naive Bayes; store conditional probabilities distributively.,Potential resource leak in HttpBroadcast; SparkSubmitArguments; FileSystemPersistenceEngine and DiskStore,0,1,1
1214,SPARK-1656,SPARK-1657,Fixed,Potential resource leak in HttpBroadcast; SparkSubmitArguments; FileSystemPersistenceEngine and DiskStore,Spark submit should fail gracefully if YARN support not enabled,0,0,1
1215,SPARK-1657,SPARK-1658,Fixed,Spark submit should fail gracefully if YARN support not enabled,Correctly identify if maven is installed and working,0,0,1
1216,SPARK-1658,SPARK-1659,Fixed,Correctly identify if maven is installed and working,improvements spark-submit usage,0,0,1
1217,SPARK-1659,SPARK-1660,Fixed,improvements spark-submit usage,Centralize the definition of property names and default values,0,1,1
1218,SPARK-1660,SPARK-1661,Won't Fix,Centralize the definition of property names and default values,the result of querying table created with RegexSerDe is all null,0,1,1
1219,SPARK-1661,SPARK-1662,Won't Fix,the result of querying table created with RegexSerDe is all null,PySpark fails if python class is used as a data container,0,0,1
1220,SPARK-1662,SPARK-1663,Not A Problem,PySpark fails if python class is used as a data container,Spark Streaming docs code has several small errors,0,1,1
1221,SPARK-1663,SPARK-1664,Fixed,Spark Streaming docs code has several small errors,spark-submit --name doesn't work in yarn-client mode,0,0,1
1222,SPARK-1666,SPARK-1667,Done,document examples,Jobs never finish successfully once bucket file missing occurred,0,1,1
1223,SPARK-1667,SPARK-1668,Fixed,Jobs never finish successfully once bucket file missing occurred,Add implicit preference as an option to examples/MovieLensALS,0,0,1
1224,SPARK-1668,SPARK-1669,Implemented,Add implicit preference as an option to examples/MovieLensALS,SQLContext.cacheTable() should be idempotent,0,0,1
1225,SPARK-1669,SPARK-1670,Fixed,SQLContext.cacheTable() should be idempotent,PySpark Fails to Create SparkContext Due To Debugging Options in conf/java-opts,0,1,1
1226,SPARK-1670,SPARK-1671,Fixed,PySpark Fails to Create SparkContext Due To Debugging Options in conf/java-opts,Cached tables should follow write-through policy,0,1,1
1227,SPARK-1671,SPARK-1672,Fixed,Cached tables should follow write-through policy,Support separate partitioners (and numbers of partitions) for users and products,0,0,1
1228,SPARK-1672,SPARK-1673,Implemented,Support separate partitioners (and numbers of partitions) for users and products,GLMNET implementation in Spark,1,0,1
1229,SPARK-1674,SPARK-1675,Fixed,Interrupted system call error in pyspark's RDD.pipe,Make clear whether computePrincipalComponents requires centered data,0,0,1
1230,SPARK-1675,SPARK-1676,Fixed,Make clear whether computePrincipalComponents requires centered data,HDFS FileSystems continually pile up in the FS cache,0,0,1
1231,SPARK-1676,SPARK-1677,Fixed,HDFS FileSystems continually pile up in the FS cache,Allow users to avoid Hadoop output checks if desired,1,0,1
1232,SPARK-1677,SPARK-1678,Fixed,Allow users to avoid Hadoop output checks if desired,Compression loses repeated values.,0,0,1
1233,SPARK-1678,SPARK-1679,Fixed,Compression loses repeated values.,In-Memory compression needs to be configurable.,1,1,1
1234,SPARK-1679,SPARK-1680,Fixed,In-Memory compression needs to be configurable.,Clean up use of setExecutorEnvs in SparkConf ,0,1,1
1235,SPARK-1680,SPARK-1681,Fixed,Clean up use of setExecutorEnvs in SparkConf ,Handle hive support correctly in ./make-distribution.sh,0,1,1
1236,SPARK-1681,SPARK-1682,Fixed,Handle hive support correctly in ./make-distribution.sh,Add gradient descent w/o sampling and RDA L1 updater,0,0,1
1237,SPARK-1682,SPARK-1683,Later,Add gradient descent w/o sampling and RDA L1 updater,Display filesystem read statistics with each task,0,1,1
1238,SPARK-1683,SPARK-1684,Fixed,Display filesystem read statistics with each task,Merge script should standardize SPARK-XXX prefix,0,0,1
1239,SPARK-1684,SPARK-1685,Fixed,Merge script should standardize SPARK-XXX prefix,retryTimer not canceled on actor restart in Worker and AppClient,0,0,1
1240,SPARK-1685,SPARK-1686,Fixed,retryTimer not canceled on actor restart in Worker and AppClient,Master switches thread when ElectedLeader,1,1,1
1241,SPARK-1686,SPARK-1687,Fixed,Master switches thread when ElectedLeader,Support NamedTuples in RDDs,0,1,1
1242,SPARK-1687,SPARK-1688,Fixed,Support NamedTuples in RDDs,PySpark throws unhelpful exception when pyspark cannot be loaded,0,1,1
1243,SPARK-1688,SPARK-1689,Fixed,PySpark throws unhelpful exception when pyspark cannot be loaded,AppClient does not respond correctly to RemoveApplication,0,1,1
1244,SPARK-1689,SPARK-1690,Fixed,AppClient does not respond correctly to RemoveApplication,RDD.saveAsTextFile throws scala.MatchError if RDD contains empty elements,0,0,1
1245,SPARK-1690,SPARK-1691,Fixed,RDD.saveAsTextFile throws scala.MatchError if RDD contains empty elements,Support quoted arguments inside of spark-submit,0,0,1
1246,SPARK-1691,SPARK-1692,Fixed,Support quoted arguments inside of spark-submit,Enable external sorting in Spark SQL aggregates,0,0,1
1247,SPARK-1693,SPARK-1694,Fixed,Dependent on multiple versions of servlet-api jars lead to throw an SecurityException when Spark built for hadoop 2.3.0 ; 2.4.0 ,Simplify ColumnBuilder/Accessor class hierarchy,0,0,1
1248,SPARK-1694,SPARK-1695,Fixed,Simplify ColumnBuilder/Accessor class hierarchy,java8-tests compiler error: package com.google.common.collections does not exist,0,0,1
1249,SPARK-1695,SPARK-1696,Fixed,java8-tests compiler error: package com.google.common.collections does not exist,RowMatrix.dspr is not using parameter alpha for DenseVector,0,0,1
1250,SPARK-1696,SPARK-1697,Fixed,RowMatrix.dspr is not using parameter alpha for DenseVector,Driver error org.apache.spark.scheduler.TaskSetManager - Loss was due to java.io.FileNotFoundException,0,0,1
1251,SPARK-1698,SPARK-1699,Won't Fix,Improve spark integration,Python relative should be independence from the core; becomes subprojects,0,1,1
1252,SPARK-1699,SPARK-1700,Unresolved,Python relative should be independence from the core; becomes subprojects,PythonRDD leaks socket descriptors during cancellation,0,1,1
1253,SPARK-1700,SPARK-1701,Fixed,PythonRDD leaks socket descriptors during cancellation,"Inconsistent naming: ""slice"" or ""partition""",0,0,1
1254,SPARK-1701,SPARK-1702,Fixed,"Inconsistent naming: ""slice"" or ""partition""",Mesos executor won't start because of a ClassNotFoundException,0,0,1
1255,SPARK-1702,SPARK-1703,Cannot Reproduce,Mesos executor won't start because of a ClassNotFoundException,Warn users if Spark is run on JRE6 but compiled with JDK7,0,0,1
1256,SPARK-1703,SPARK-1704,Fixed,Warn users if Spark is run on JRE6 but compiled with JDK7,Support EXPLAIN in Spark SQL,0,0,1
1257,SPARK-1704,SPARK-1705,Fixed,Support EXPLAIN in Spark SQL,Allow multiple instances per node with SPARK-EC2,0,1,1
1258,SPARK-1705,SPARK-1706,Fixed,Allow multiple instances per node with SPARK-EC2,Allow multiple executors per worker in Standalone mode,0,1,1
1259,SPARK-1706,SPARK-1707,Fixed,Allow multiple executors per worker in Standalone mode,Remove 3 second sleep before starting app on YARN,0,1,1
1260,SPARK-1707,SPARK-1708,Fixed,Remove 3 second sleep before starting app on YARN,Add ClassTag parameter on accumulator and broadcast methods,0,0,1
1261,SPARK-1708,SPARK-1709,Fixed,Add ClassTag parameter on accumulator and broadcast methods,"spark-submit should use ""main class"" attribute of JAR if no --class is given",0,0,1
1262,SPARK-1709,SPARK-1710,Fixed,"spark-submit should use ""main class"" attribute of JAR if no --class is given","spark-submit should print better errors than ""InvocationTargetException""",1,1,1
1263,SPARK-1710,SPARK-1711,Fixed,"spark-submit should print better errors than ""InvocationTargetException""",We should not use two build tools at the same time; should remove one of them,0,0,1
1264,SPARK-1711,SPARK-1712,Duplicate,We should not use two build tools at the same time; should remove one of them,ParallelCollectionRDD operations hanging forever without any error messages ,0,1,1
1265,SPARK-1712,SPARK-1713,Fixed,ParallelCollectionRDD operations hanging forever without any error messages ,YarnAllocationHandler starts a thread for every executor it runs,0,1,1
1266,SPARK-1713,SPARK-1714,Fixed,YarnAllocationHandler starts a thread for every executor it runs,Take advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler,1,1,1
1267,SPARK-1715,SPARK-1716,Won't Fix,Ensure actor is self-contained in DAGScheduler,EC2 script should exit with non-zero code on UsageError,0,1,1
1268,SPARK-1716,SPARK-1717,Fixed,EC2 script should exit with non-zero code on UsageError,spark-ec2.py sometimes doesn't wait long enough for EC2 to stand up,1,0,1
1269,SPARK-1717,SPARK-1718,Fixed,spark-ec2.py sometimes doesn't wait long enough for EC2 to stand up,pyspark doesn't work with assembly jar containing over 65536 files/dirs built on redhat ,0,0,1
1270,SPARK-1719,SPARK-1721,Fixed,spark.executor.extraLibraryPath isn't applied on yarn,Classloaders not used correctly in Mesos,0,0,1
1271,SPARK-1721,SPARK-1722,Fixed,Classloaders not used correctly in Mesos,Unable to create KafkaStream,0,0,1
1272,SPARK-1722,SPARK-1723,Invalid,Unable to create KafkaStream,Add saveAsLibSVMFile,0,1,1
1273,SPARK-1723,SPARK-1724,Fixed,Add saveAsLibSVMFile,Add appendBias,1,1,1
1274,SPARK-1724,SPARK-1725,Fixed,Add appendBias,Can't use broadcast variables in pyspark on Mesos because pyspark isn't added to PYTHONPATH,0,0,1
1275,SPARK-1725,SPARK-1726,Fixed,Can't use broadcast variables in pyspark on Mesos because pyspark isn't added to PYTHONPATH,Tasks that fail to serialize remain in active stages forever.,0,0,1
1276,SPARK-1726,SPARK-1727,Fixed,Tasks that fail to serialize remain in active stages forever.,Correct small compile errors; typos; and markdown issues in (primarly) MLlib docs,0,0,1
1277,SPARK-1727,SPARK-1728,Fixed,Correct small compile errors; typos; and markdown issues in (primarly) MLlib docs,JavaRDDLike.mapPartitionsWithIndex requires ClassTag,0,0,1
1278,SPARK-1728,SPARK-1729,Fixed,JavaRDDLike.mapPartitionsWithIndex requires ClassTag,Make Flume pull data from source; rather than the current push model,0,0,1
1279,SPARK-1729,SPARK-1730,Fixed,Make Flume pull data from source; rather than the current push model,Make receiver store data reliably to avoid data-loss on executor failures,1,1,1
1280,SPARK-1730,SPARK-1731,Fixed,Make receiver store data reliably to avoid data-loss on executor failures,PySpark broadcast values with custom classes won't depickle properly,0,1,1
1281,SPARK-1731,SPARK-1732,Fixed,PySpark broadcast values with custom classes won't depickle properly,Support for primitive nulls in SparkSQL,0,1,1
1282,SPARK-1732,SPARK-1733,Fixed,Support for primitive nulls in SparkSQL,Pluggable storage support for BlockManager,0,1,1
1283,SPARK-1733,SPARK-1734,Won't Fix,Pluggable storage support for BlockManager,"spark-submit throws an exception: Exception in thread ""main"" java.lang.ClassNotFoundException: org.apache.spark.broadcast.TorrentBroadcastFactory",0,0,1
1284,SPARK-1734,SPARK-1735,Fixed,"spark-submit throws an exception: Exception in thread ""main"" java.lang.ClassNotFoundException: org.apache.spark.broadcast.TorrentBroadcastFactory",Make distribution script has missing profiles for special hadoop versions,0,0,1
1285,SPARK-1735,SPARK-1736,Fixed,Make distribution script has missing profiles for special hadoop versions,spark-submit on Windows,0,0,1
1286,SPARK-1736,SPARK-1737,Fixed,spark-submit on Windows,Warn rather than fail when Java 7+ is used to create distributions,0,0,1
1287,SPARK-1737,SPARK-1738,Fixed,Warn rather than fail when Java 7+ is used to create distributions,Is spark-debugger still available?,0,0,1
1288,SPARK-1738,SPARK-1739,Fixed,Is spark-debugger still available?,Close PR's after period of inactivity,0,0,1
1289,SPARK-1739,SPARK-1740,Unresolved,Close PR's after period of inactivity,Pyspark cancellation kills unrelated pyspark workers,0,0,1
1290,SPARK-1740,SPARK-1741,Fixed,Pyspark cancellation kills unrelated pyspark workers,Add predict(JavaRDD) to predictive models,0,0,1
1291,SPARK-1741,SPARK-1742,Fixed,Add predict(JavaRDD) to predictive models,Profiler for Spark,0,1,1
1292,SPARK-1742,SPARK-1743,Not A Problem,Profiler for Spark,Add mllib.util.MLUtils.{loadLibSVMFile; saveAsLibSVMFile} to pyspark,0,1,1
1293,SPARK-1743,SPARK-1744,Fixed,Add mllib.util.MLUtils.{loadLibSVMFile; saveAsLibSVMFile} to pyspark,Document how to pass in preferredNodeLocationData,0,1,1
1294,SPARK-1744,SPARK-1745,Won't Fix,Document how to pass in preferredNodeLocationData,TaskContext.interrupted should probably not be a constructor argument,0,0,1
1295,SPARK-1745,SPARK-1746,Fixed,TaskContext.interrupted should probably not be a constructor argument,Support setting SPARK_JAVA_OPTS on executors for backwards compatibility,0,0,1
1296,SPARK-1746,SPARK-1747,Fixed,Support setting SPARK_JAVA_OPTS on executors for backwards compatibility,check for Spark on Yarn ApplicationMaster split brain,0,0,1
1297,SPARK-1747,SPARK-1748,Unresolved,check for Spark on Yarn ApplicationMaster split brain,I installed the spark_standalone;but I did not know how to use sbt to compile the programme of spark?,0,1,1
1298,SPARK-1748,SPARK-1749,Done,I installed the spark_standalone;but I did not know how to use sbt to compile the programme of spark?,DAGScheduler supervisor strategy broken with Mesos,0,0,1
1299,SPARK-1749,SPARK-1750,Fixed,DAGScheduler supervisor strategy broken with Mesos,EdgePartition is not serialized properly,0,0,1
1300,SPARK-1750,SPARK-1751,Fixed,EdgePartition is not serialized properly,spark ec2 scripts should check for SSh to be up,0,0,1
1301,SPARK-1751,SPARK-1752,Fixed,spark ec2 scripts should check for SSh to be up,Standardize input/output format for vectors and labeled points,0,0,1
1302,SPARK-1752,SPARK-1753,Fixed,Standardize input/output format for vectors and labeled points,PySpark on YARN does not work on assembly jar built on Red Hat based OS,0,0,1
1303,SPARK-1753,SPARK-1754,Fixed,PySpark on YARN does not work on assembly jar built on Red Hat based OS,Add missing arithmetic DSL operations.,0,1,1
1304,SPARK-1754,SPARK-1755,Fixed,Add missing arithmetic DSL operations.,Spark-submit --name does not resolve to application name on YARN,0,0,1
1305,SPARK-1757,SPARK-1758,Fixed,Support saving null primitives with .saveAsParquetFile(),failing test org.apache.spark.JavaAPISuite.wholeTextFiles,0,1,1
1306,SPARK-1758,SPARK-1759,Cannot Reproduce,failing test org.apache.spark.JavaAPISuite.wholeTextFiles,sbt/sbt package fail cause by directory,0,1,1
1307,SPARK-1759,SPARK-1760,Not A Problem,sbt/sbt package fail cause by directory, mvn  -Dsuites=*  test throw an ClassNotFoundException,0,1,1
1308,SPARK-1760,SPARK-1761,Fixed, mvn  -Dsuites=*  test throw an ClassNotFoundException,Add broadcast information on SparkUI storage tab,0,1,1
1309,SPARK-1762,SPARK-1763,Won't Fix,Add functionality to pin RDDs in cache,SparkSubmit arguments do not propagate to python files on YARN,0,0,1
1310,SPARK-1763,SPARK-1764,Cannot Reproduce,SparkSubmit arguments do not propagate to python files on YARN,EOF reached before Python server acknowledged,0,1,1
1311,SPARK-1764,SPARK-1765,Fixed,EOF reached before Python server acknowledged,Modify a typo in monitoring.md,0,0,1
1312,SPARK-1765,SPARK-1766,Fixed,Modify a typo in monitoring.md,Move reduceByKey definitions next to each other in PairRDDFunctions,0,0,1
1313,SPARK-1766,SPARK-1767,Fixed,Move reduceByKey definitions next to each other in PairRDDFunctions,Prefer HDFS-cached replicas when scheduling data-local tasks,1,0,1
1314,SPARK-1767,SPARK-1768,Fixed,Prefer HDFS-cached replicas when scheduling data-local tasks,History Server enhancements,1,1,1
1315,SPARK-1768,SPARK-1769,Fixed,History Server enhancements,Executor loss can cause race condition in Pool,1,1,1
1316,SPARK-1769,SPARK-1770,Fixed,Executor loss can cause race condition in Pool,repartition and coalesce(shuffle=true) put objects with the same key in the same bucket,0,0,1
1317,SPARK-1770,SPARK-1771,Fixed,repartition and coalesce(shuffle=true) put objects with the same key in the same bucket,CoarseGrainedSchedulerBackend is not resilient to Akka restarts,0,0,1
1318,SPARK-1771,SPARK-1772,Won't Fix,CoarseGrainedSchedulerBackend is not resilient to Akka restarts,Spark executors do not successfully die on OOM,1,1,1
1319,SPARK-1772,SPARK-1773,Fixed,Spark executors do not successfully die on OOM,Standalone cluster docs should be updated to reflect Spark Submit,0,0,1
1320,SPARK-1773,SPARK-1774,Fixed,Standalone cluster docs should be updated to reflect Spark Submit,SparkSubmit --jars not working for yarn-client,0,1,1
1321,SPARK-1774,SPARK-1775,Fixed,SparkSubmit --jars not working for yarn-client,Unneeded lock in ShuffleMapTask.deserializeInfo,0,0,1
1322,SPARK-1775,SPARK-1776,Fixed,Unneeded lock in ShuffleMapTask.deserializeInfo,Have Spark's SBT build read dependencies from Maven,0,0,1
1323,SPARK-1776,SPARK-1777,Fixed,Have Spark's SBT build read dependencies from Maven,"Pass ""cached"" blocks directly to disk if memory is not large enough",0,0,1
1324,SPARK-1778,SPARK-1779,Fixed,Add 'limit' transformation to SchemaRDD.,Warning when spark.storage.memoryFraction is not between 0 and 1,0,1,1
1325,SPARK-1779,SPARK-1780,Fixed,Warning when spark.storage.memoryFraction is not between 0 and 1,Non-existent SPARK_DAEMON_OPTS is referred to in a few places,0,1,1
1326,SPARK-1780,SPARK-1781,Fixed,Non-existent SPARK_DAEMON_OPTS is referred to in a few places,Generalized validity checking for configuration parameters,0,0,1
1327,SPARK-1781,SPARK-1782,Won't Fix,Generalized validity checking for configuration parameters,svd for sparse matrix using ARPACK,0,0,1
1328,SPARK-1782,SPARK-1783,Fixed,svd for sparse matrix using ARPACK,Title contains html code in MLlib guide,0,0,1
1329,SPARK-1783,SPARK-1784,Fixed,Title contains html code in MLlib guide,Add a partitioner which partitions an RDD with each partition having specified # of keys,0,1,1
1330,SPARK-1784,SPARK-1785,Invalid,Add a partitioner which partitions an RDD with each partition having specified # of keys,Streaming requires receivers to be serializable,0,0,1
1331,SPARK-1785,SPARK-1786,Fixed,Streaming requires receivers to be serializable,Kryo Serialization Error in GraphX,0,1,1
1332,SPARK-1786,SPARK-1787,Fixed,Kryo Serialization Error in GraphX,Build failure on JDK8 :: SBT fails to load build configuration file,0,0,1
1333,SPARK-1788,SPARK-1789,Fixed,Upgrade Parquet to 1.4.3,Multiple versions of Netty dependencies cause FlumeStreamSuite failure,0,1,1
1334,SPARK-1789,SPARK-1790,Fixed,Multiple versions of Netty dependencies cause FlumeStreamSuite failure,Update EC2 scripts to support r3 instance types,0,1,1
1335,SPARK-1790,SPARK-1791,Fixed,Update EC2 scripts to support r3 instance types,SVM implementation does not use threshold parameter,0,1,1
1336,SPARK-1791,SPARK-1792,Fixed,SVM implementation does not use threshold parameter,Missing Spark-Shell Configure Options,0,1,1
1337,SPARK-1792,SPARK-1793,Fixed,Missing Spark-Shell Configure Options,Heavily duplicated test setup code in SVMSuite,0,0,1
1338,SPARK-1793,SPARK-1794,Fixed,Heavily duplicated test setup code in SVMSuite,Generic ADMM implementation for SVM; lasso; and L1-regularized logistic regression,1,1,1
1339,SPARK-1797,SPARK-1798,Not A Problem,streaming on hdfs can detected all new file; but the sum of all the rdd.count() not equals which had detected,Tests should clean up temp files,0,0,1
1340,SPARK-1798,SPARK-1799,Fixed,Tests should clean up temp files,Add init script to the debian packaging,0,0,1
1341,SPARK-1799,SPARK-1800,Won't Fix,Add init script to the debian packaging,Add broadcast hash join operator,0,1,1
1342,SPARK-1801,SPARK-1802,Fixed,Open up some private APIs related to creating new RDDs for developers,Audit dependency graph when Spark is built with -Phive,0,0,1
1343,SPARK-1803,SPARK-1804,Fixed,Rename test resources to be compatible with Windows FS,Mark 0.9.1 as released in JIRA,0,1,1
1344,SPARK-1804,SPARK-1805,Fixed,Mark 0.9.1 as released in JIRA,Error launching cluster when master and slave machines are of different virtualization types,0,0,1
1345,SPARK-1807,SPARK-1808,Won't Fix,Modify SPARK_EXECUTOR_URI to allow for script execution in Mesos.,bin/pyspark does not load default configuration properties,0,1,1
1346,SPARK-1808,SPARK-1809,Fixed,bin/pyspark does not load default configuration properties,Mesos backend doesn't respect HADOOP_CONF_DIR,0,1,1
1347,SPARK-1809,SPARK-1810,Unresolved,Mesos backend doesn't respect HADOOP_CONF_DIR,The spark tar ball does not unzip into a separate folder when un-tarred.,0,0,1
1348,SPARK-1810,SPARK-1811,Cannot Reproduce,The spark tar ball does not unzip into a separate folder when un-tarred.,Support resizable output buffer for kryo serializer,0,1,1
1349,SPARK-1814,SPARK-1815,Fixed,Splash page should include correct syntax for launching examples,SparkContext shouldn't be marked DeveloperApi,0,1,1
1350,SPARK-1815,SPARK-1816,Fixed,SparkContext shouldn't be marked DeveloperApi,LiveListenerBus dies if a listener throws an exception,1,0,1
1351,SPARK-1816,SPARK-1817,Fixed,LiveListenerBus dies if a listener throws an exception,RDD zip erroneous when partitions do not divide RDD count,1,0,1
1352,SPARK-1817,SPARK-1818,Fixed,RDD zip erroneous when partitions do not divide RDD count,Freshen Mesos docs,0,1,1
1353,SPARK-1818,SPARK-1819,Fixed,Freshen Mesos docs,Fix GetField.nullable.,0,1,1
1354,SPARK-1819,SPARK-1820,Fixed,Fix GetField.nullable.,Make GenerateMimaIgnore @DeveloperApi annotation aware.,0,1,1
1355,SPARK-1820,SPARK-1821,Fixed,Make GenerateMimaIgnore @DeveloperApi annotation aware.,Document History Server,0,1,1
1356,SPARK-1821,SPARK-1822,Implemented,Document History Server,SchemaRDD.count() should use the optimizer.,0,1,1
1357,SPARK-1822,SPARK-1823,Fixed,SchemaRDD.count() should use the optimizer.,ExternalAppendOnlyMap can still OOM if one key is very large,0,1,1
1358,SPARK-1823,SPARK-1824,Unresolved,ExternalAppendOnlyMap can still OOM if one key is very large,Python examples still take in <master>,0,1,1
1359,SPARK-1824,SPARK-1825,Fixed,Python examples still take in <master>,Windows Spark fails to work with Linux YARN,0,1,1
1360,SPARK-1826,SPARK-1827,Fixed,Some bad head notations in sparksql ,LICENSE and NOTICE files need a refresh to contain transitive dependency info,0,0,1
1361,SPARK-1827,SPARK-1828,Fixed,LICENSE and NOTICE files need a refresh to contain transitive dependency info,Created forked version of hive-exec that doesn't bundle other dependencies,0,1,1
1362,SPARK-1829,SPARK-1830,Fixed,"Sub-second durations shouldn't round to ""0 s""",Deploy failover; Make Persistence engine and LeaderAgent Pluggable.,0,0,1
1363,SPARK-1831,SPARK-1832,Invalid,"add the security guide to the ""More"" drop down menu",Executor UI improvement suggestions,0,1,1
1364,SPARK-1833,SPARK-1834,Fixed,Have an empty SparkContext constructor instead of relying on new SparkContext(new SparkConf()),NoSuchMethodError when invoking JavaPairRDD.reduce() in Java,1,1,1
1365,SPARK-1835,SPARK-1836,Won't Fix,sbt gen-idea includes both mesos and mesos with shaded-protobuf into dependencies,REPL $outer type mismatch causes lookup() and equals() problems,0,0,1
1366,SPARK-1837,SPARK-1838,Fixed,NumericRange should be partitioned in the same way as other sequences,On a YARN cluster; Spark doesn't run on local mode,0,1,1
1367,SPARK-1838,SPARK-1839,Not A Problem,On a YARN cluster; Spark doesn't run on local mode,PySpark take() does not launch a Spark job when it has to,0,1,1
1368,SPARK-1839,SPARK-1840,Fixed,PySpark take() does not launch a Spark job when it has to,SparkListenerBus prints out scary error message when terminating normally,0,1,1
1369,SPARK-1840,SPARK-1841,Fixed,SparkListenerBus prints out scary error message when terminating normally,update scalatest to version 2.1.5,0,1,1
1370,SPARK-1841,SPARK-1842,Fixed,update scalatest to version 2.1.5,update scala-logging-slf4j to version 2.1.2,1,1,1
1371,SPARK-1842,SPARK-1843,Won't Fix,update scala-logging-slf4j to version 2.1.2,Provide a simpler alternative to assemble-deps,0,1,1
1372,SPARK-1843,SPARK-1844,Fixed,Provide a simpler alternative to assemble-deps,Support maven-style dependency resolution in sbt build,1,1,1
1373,SPARK-1845,SPARK-1846,Fixed,Use AllScalaRegistrar for SparkSqlSerializer to register serializers of Scala collections.,RAT checks should exclude logs/ directory,0,1,1
1374,SPARK-1846,SPARK-1847,Fixed,RAT checks should exclude logs/ directory,Pushdown filters on non-required parquet columns,0,1,1
1375,SPARK-1847,SPARK-1848,Fixed,Pushdown filters on non-required parquet columns,Executors are mysteriously dying when using Spark on Mesos,0,1,1
1376,SPARK-1848,SPARK-1849,Cannot Reproduce,Executors are mysteriously dying when using Spark on Mesos,sc.textFile does not support non UTF-8 encodings,0,1,1
1377,SPARK-1850,SPARK-1851,Fixed,Bad exception if multiple jars exist when running PySpark,Upgrade Avro dependency to 1.7.6 so Spark can read Avro files,0,0,1
1378,SPARK-1851,SPARK-1852,Fixed,Upgrade Avro dependency to 1.7.6 so Spark can read Avro files,SparkSQL Queries with Sorts run before the user asks them to,0,0,1
1379,SPARK-1852,SPARK-1853,Fixed,SparkSQL Queries with Sorts run before the user asks them to,Show Streaming application code context (file; line number) in Spark Stages UI,0,1,1
1380,SPARK-1853,SPARK-1854,Fixed,Show Streaming application code context (file; line number) in Spark Stages UI,Add a version of StreamingContext.fileStream that take hadoop conf object,1,1,1
1381,SPARK-1854,SPARK-1855,Won't Fix,Add a version of StreamingContext.fileStream that take hadoop conf object,Provide memory-and-local-disk RDD checkpointing,0,1,1
1382,SPARK-1856,SPARK-1857,Fixed,Standardize MLlib interfaces,map() with lookup() causes exception,0,0,1
1383,SPARK-1857,SPARK-1858,Not A Problem,map() with lookup() causes exception,"Update ""third-party Hadoop distros"" doc to list more distros",0,1,1
1384,SPARK-1858,SPARK-1859,Fixed,"Update ""third-party Hadoop distros"" doc to list more distros",Linear; Ridge and Lasso Regressions with SGD yield unexpected results,0,1,1
1385,SPARK-1859,SPARK-1860,Not A Problem,Linear; Ridge and Lasso Regressions with SGD yield unexpected results,Standalone Worker cleanup should not clean up running executors,0,0,1
1386,SPARK-1861,SPARK-1862,Implemented,ArrayIndexOutOfBoundsException when reading bzip2 files,Add build support for MapR,0,1,1
1387,SPARK-1862,SPARK-1863,Fixed,Add build support for MapR,Allowing user jars to take precedence over Spark jars does not work as expected,0,0,1
1388,SPARK-1864,SPARK-1865,Fixed,Classpath not correctly sent to executors.,Improve behavior of cleanup of disk state,0,0,1
1389,SPARK-1865,SPARK-1866,Won't Fix,Improve behavior of cleanup of disk state,Closure cleaner does not null shadowed fields when outer scope is referenced,0,0,1
1390,SPARK-1866,SPARK-1867,Unresolved,Closure cleaner does not null shadowed fields when outer scope is referenced,Spark Documentation Error causes java.lang.IllegalStateException: unread block data,1,0,1
1391,SPARK-1867,SPARK-1868,Not A Problem,Spark Documentation Error causes java.lang.IllegalStateException: unread block data,Users should be allowed to cogroup at least 4 RDDs,0,1,1
1392,SPARK-1868,SPARK-1869,Fixed,Users should be allowed to cogroup at least 4 RDDs,`spark-shell --help` fails if called from outside spark home,0,0,1
1393,SPARK-1869,SPARK-1870,Fixed,`spark-shell --help` fails if called from outside spark home,Jars specified via --jars in spark-submit are not added to executor classpath for YARN,0,1,1
1394,SPARK-1870,SPARK-1871,Fixed,Jars specified via --jars in spark-submit are not added to executor classpath for YARN,Improve MLlib guide for v1.0,0,0,1
1395,SPARK-1871,SPARK-1872,Done,Improve MLlib guide for v1.0,Update api links for unidoc,1,1,1
1396,SPARK-1872,SPARK-1873,Fixed,Update api links for unidoc,Add README.md file when making distributions,0,0,1
1397,SPARK-1873,SPARK-1874,Fixed,Add README.md file when making distributions,Clean up MLlib sample data,0,0,1
1398,SPARK-1874,SPARK-1875,Fixed,Clean up MLlib sample data,NoClassDefFoundError: StringUtils when building against Hadoop 1,0,0,1
1399,SPARK-1875,SPARK-1876,Fixed,NoClassDefFoundError: StringUtils when building against Hadoop 1,Update Windows scripts to deal with latest distribution layout changes,0,1,1
1400,SPARK-1876,SPARK-1877,Fixed,Update Windows scripts to deal with latest distribution layout changes,ClassNotFoundException when loading RDD with serialized objects,0,0,1
1401,SPARK-1877,SPARK-1878,Fixed,ClassNotFoundException when loading RDD with serialized objects,Incorrect initialization order in JavaStreamingContext,0,1,1
1402,SPARK-1878,SPARK-1879,Fixed,Incorrect initialization order in JavaStreamingContext,Default PermGen size too small when using Hadoop2 and Hive,0,0,1
1403,SPARK-1879,SPARK-1880,Fixed,Default PermGen size too small when using Hadoop2 and Hive,Eliminate unnecessary job executions.,0,0,1
1404,SPARK-1880,SPARK-1881,Won't Fix,Eliminate unnecessary job executions.,Executor caching,0,0,1
1405,SPARK-1881,SPARK-1882,Not A Problem,Executor caching,Support dynamic memory sharing in Mesos,1,0,1
1406,SPARK-1882,SPARK-1883,Won't Fix,Support dynamic memory sharing in Mesos,spark graph.triplets does not return correct values,0,1,1
1407,SPARK-1884,SPARK-1885,Won't Fix,Shark failed to start,GraphX reduce function not working properly -- returns only 1 element,1,0,1
1408,SPARK-1885,SPARK-1886,Not A Problem,GraphX reduce function not working properly -- returns only 1 element,workers keep dying for uncaught exception of executor id not found ,0,1,1
1409,SPARK-1886,SPARK-1887,Fixed,workers keep dying for uncaught exception of executor id not found ,"Update ""Contributors Guide"" with useful data from past threads",0,0,1
1410,SPARK-1887,SPARK-1888,Unresolved,"Update ""Contributors Guide"" with useful data from past threads",enhance MEMORY_AND_DISK mode by dropping blocks in parallel,0,0,1
1411,SPARK-1889,SPARK-1890,Fixed,Apply splitConjunctivePredicates to join condition while finding join keys.,"add modify acls to the web UI for the ""kill"" button",0,0,1
1412,SPARK-1890,SPARK-1891,Fixed,"add modify acls to the web UI for the ""kill"" button",Add admin acls to the Web UI,1,0,1
1413,SPARK-1891,SPARK-1892,Fixed,Add admin acls to the Web UI,Add an OWL-QN optimizer for L1 regularized optimizations.,0,1,1
1414,SPARK-1892,SPARK-1893,Duplicate,Add an OWL-QN optimizer for L1 regularized optimizations.,Explain why running Spark Hive locally freezes,0,1,1
1415,SPARK-1893,SPARK-1894,Cannot Reproduce,Explain why running Spark Hive locally freezes,The default ec2 set-up ignores --driver-class-path,0,1,1
1416,SPARK-1894,SPARK-1895,Fixed,The default ec2 set-up ignores --driver-class-path,Run tests on windows,0,0,1
1417,SPARK-1895,SPARK-1896,Cannot Reproduce,Run tests on windows,MASTER masks spark.master in spark-shell,0,0,1
1418,SPARK-1896,SPARK-1897,Fixed,MASTER masks spark.master in spark-shell,Spark shell --jars (or spark.jars) doesn't work,1,1,1
1419,SPARK-1897,SPARK-1898,Fixed,Spark shell --jars (or spark.jars) doesn't work,In deploy.yarn.Client; use YarnClient rather than YarnClientImpl,0,1,1
1420,SPARK-1898,SPARK-1899,Fixed,In deploy.yarn.Client; use YarnClient rather than YarnClientImpl,Default log4j.properties incorrectly sends all output to stderr and none to stdout,0,1,1
1421,SPARK-1899,SPARK-1900,Won't Fix,Default log4j.properties incorrectly sends all output to stderr and none to stdout,Fix running PySpark files on YARN ,0,0,1
1422,SPARK-1900,SPARK-1901,Fixed,Fix running PySpark files on YARN ,Standalone worker update exector's state ahead of executor process exit,0,0,1
1423,SPARK-1901,SPARK-1902,Fixed,Standalone worker update exector's state ahead of executor process exit,Spark shell prints error when :4040 port already in use,0,1,1
1424,SPARK-1902,SPARK-1903,Fixed,Spark shell prints error when :4040 port already in use,Document Spark's network connections,0,1,1
1425,SPARK-1903,SPARK-1904,Fixed,Document Spark's network connections,ZooKeeper URI in spark-env.sh no longer working w/ bin/pyspark,0,1,1
1426,SPARK-1904,SPARK-1905,Not A Problem,ZooKeeper URI in spark-env.sh no longer working w/ bin/pyspark,Issues with `spark-submit`,0,1,1
1427,SPARK-1906,SPARK-1907,Fixed,spark-submit doesn't send master URL to Driver in standalone cluster mode,spark-submit: add exec at the end of the script,1,1,1
1428,SPARK-1907,SPARK-1908,Fixed,spark-submit: add exec at the end of the script,Support local app jar in standalone cluster mode,1,1,1
1429,SPARK-1908,SPARK-1909,Won't Fix,Support local app jar in standalone cluster mode,#NAME?,1,1,1
1430,SPARK-1909,SPARK-1910,Fixed,#NAME?,Add onBlockComplete API to receiver,0,0,1
1431,SPARK-1910,SPARK-1911,Unresolved,Add onBlockComplete API to receiver,Warn users if their assembly jars are not built with Java 6,0,1,1
1432,SPARK-1911,SPARK-1912,Fixed,Warn users if their assembly jars are not built with Java 6,Compression memory issue during reduce,0,1,1
1433,SPARK-1912,SPARK-1913,Fixed,Compression memory issue during reduce,Parquet table column pruning error caused by filter pushdown,0,1,1
1434,SPARK-1913,SPARK-1914,Fixed,Parquet table column pruning error caused by filter pushdown,Simplify CountFunction not to traverse to evaluate all child expressions.,1,1,1
1435,SPARK-1914,SPARK-1915,Fixed,Simplify CountFunction not to traverse to evaluate all child expressions.,AverageFunction should not count if the evaluated value is null.,1,1,1
1436,SPARK-1915,SPARK-1916,Fixed,AverageFunction should not count if the evaluated value is null.,SparkFlumeEvent with body bigger than 1020 bytes are not read properly,0,1,1
1437,SPARK-1916,SPARK-1917,Fixed,SparkFlumeEvent with body bigger than 1020 bytes are not read properly,PySpark fails to import functions from {{scipy.special}},0,1,1
1438,SPARK-1917,SPARK-1918,Fixed,PySpark fails to import functions from {{scipy.special}},PySpark shell --py-files does not work for zip files,1,1,1
1439,SPARK-1918,SPARK-1919,Fixed,PySpark shell --py-files does not work for zip files,In Windows; Spark shell cannot load classes in spark.jars (--jars),0,1,1
1440,SPARK-1919,SPARK-1920,Fixed,In Windows; Spark shell cannot load classes in spark.jars (--jars),Spark JAR compiled with Java 7 leads to PySpark not working in YARN,0,0,1
1441,SPARK-1921,SPARK-1922,Unresolved,Allow duplicate jar files among the app jar and secondary jars in yarn-cluster mode,"hql query throws ""RuntimeException: Unsupported dataType"" if struct field of a table has a column with underscore in name",0,0,1
1442,SPARK-1922,SPARK-1923,Fixed,"hql query throws ""RuntimeException: Unsupported dataType"" if struct field of a table has a column with underscore in name",ClassNotFoundException when running with sbt and Scala 2.10.3,0,1,1
1443,SPARK-1923,SPARK-1924,Fixed,ClassNotFoundException when running with sbt and Scala 2.10.3,Make local:/ scheme work in more deploy modes,0,0,1
1444,SPARK-1924,SPARK-1925,Unresolved,Make local:/ scheme work in more deploy modes,Typo in org.apache.spark.mllib.tree.DecisionTree.isSampleValid,0,1,1
1445,SPARK-1925,SPARK-1926,Fixed,Typo in org.apache.spark.mllib.tree.DecisionTree.isSampleValid,Nullability of Max/Min/First should be true.,0,0,1
1446,SPARK-1926,SPARK-1927,Fixed,Nullability of Max/Min/First should be true.,Implicits declared in companion objects not found in Spark shell,0,1,1
1447,SPARK-1928,SPARK-1929,Fixed,DAGScheduler suspended by local task OOM,DAGScheduler suspended by local task OOM,0,1,1
1448,SPARK-1930,SPARK-1931,Fixed,The Container is running beyond physical memory limits; so as to be killed.,Graph.partitionBy does not reconstruct routing tables,0,1,1
1449,SPARK-1931,SPARK-1932,Fixed,Graph.partitionBy does not reconstruct routing tables,Race conditions in BlockManager.cachedPeers and ConnectionManager.onReceiveCallback,0,0,1
1450,SPARK-1932,SPARK-1933,Fixed,Race conditions in BlockManager.cachedPeers and ConnectionManager.onReceiveCallback,FileNotFoundException when a directory is passed to SparkContext.addJar/addFile,1,0,1
1451,SPARK-1933,SPARK-1934,Fixed,FileNotFoundException when a directory is passed to SparkContext.addJar/addFile,"this reference escape to ""selectorThread"" during construction in ConnectionManager",1,0,1
1452,SPARK-1934,SPARK-1935,Fixed,"this reference escape to ""selectorThread"" during construction in ConnectionManager",Explicitly add commons-codec 1.5 as a dependency,0,1,1
1453,SPARK-1935,SPARK-1936,Fixed,Explicitly add commons-codec 1.5 as a dependency,Add apache header and remove author tags,0,1,1
1454,SPARK-1936,SPARK-1937,Won't Fix,Add apache header and remove author tags,Tasks can be submitted before executors are registered,0,0,1
1455,SPARK-1937,SPARK-1938,Fixed,Tasks can be submitted before executors are registered,ApproxCountDistinctMergeFunction should return Int value.,0,1,1
1456,SPARK-1938,SPARK-1939,Fixed,ApproxCountDistinctMergeFunction should return Int value.,Refactor takeSample method in RDD to use ScaSRS,0,1,1
1457,SPARK-1939,SPARK-1940,Fixed,Refactor takeSample method in RDD to use ScaSRS,Enable rolling of executor logs (stdout / stderr),1,1,1
1458,SPARK-1940,SPARK-1941,Fixed,Enable rolling of executor logs (stdout / stderr),Update streamlib to 2.7.0 and use HyperLogLogPlus instead of HyperLogLog,1,1,1
1459,SPARK-1941,SPARK-1942,Implemented,Update streamlib to 2.7.0 and use HyperLogLogPlus instead of HyperLogLog,Stop clearing spark.driver.port in unit tests,1,0,1
1460,SPARK-1942,SPARK-1943,Fixed,Stop clearing spark.driver.port in unit tests,Testing use of target version field,0,0,1
1461,SPARK-1943,SPARK-1944,Not A Problem,Testing use of target version field,Document --verbose in spark-shell -h,0,0,1
1462,SPARK-1944,SPARK-1945,Fixed,Document --verbose in spark-shell -h,Add full Java examples in MLlib docs,0,0,1
1463,SPARK-1945,SPARK-1946,Fixed,Add full Java examples in MLlib docs,Submit stage after executors have been registered,0,1,1
1464,SPARK-1947,SPARK-1948,Fixed,Child of SumDistinct or Average should be widened to prevent overflows the same as Sum.,Scalac crashes when building Spark in IntelliJ IDEA,0,0,1
1465,SPARK-1948,SPARK-1949,Not A Problem,Scalac crashes when building Spark in IntelliJ IDEA,Servlet 2.5 vs 3.0 conflict in SBT build,1,1,1
1466,SPARK-1949,SPARK-1950,Won't Fix,Servlet 2.5 vs 3.0 conflict in SBT build,spark on yarn can't start ,0,0,1
1467,SPARK-1951,SPARK-1952,Invalid,spark on yarn can't start ,slf4j version conflicts with pig,0,0,1
1468,SPARK-1952,SPARK-1953,Not A Problem,slf4j version conflicts with pig,yarn client mode Application Master memory size is same as driver memory size,0,1,1
1469,SPARK-1955,SPARK-1956,Fixed,VertexRDD can incorrectly assume index sharing,Enable shuffle consolidation by default,0,0,1
1470,SPARK-1956,SPARK-1957,Won't Fix,Enable shuffle consolidation by default,Pluggable disk store for BlockManager,0,1,1
1471,SPARK-1957,SPARK-1958,Won't Fix,Pluggable disk store for BlockManager,Calling .collect() on a SchemaRDD should call executeCollect() on the underlying query plan.,0,1,1
1472,SPARK-1958,SPARK-1959,Fixed,Calling .collect() on a SchemaRDD should call executeCollect() on the underlying query plan.,"String ""NULL"" is interpreted as null value",1,1,1
1473,SPARK-1959,SPARK-1960,Fixed,"String ""NULL"" is interpreted as null value",[Duplicate] Support cross-building with Scala 2.11,0,1,1
1474,SPARK-1960,SPARK-1961,Not A Problem,[Duplicate] Support cross-building with Scala 2.11,when data return from map is about 10 kb; reduce(_ + _) would always pending,0,1,1
1475,SPARK-1961,SPARK-1962,Invalid,when data return from map is about 10 kb; reduce(_ + _) would always pending,Add RDD cache reference counting,0,0,1
1476,SPARK-1962,SPARK-1963,Won't Fix,Add RDD cache reference counting,Job aborted with NullPointerException from DAGScheduler.scala:1020,0,0,1
1477,SPARK-1963,SPARK-1964,Invalid,Job aborted with NullPointerException from DAGScheduler.scala:1020,Timestamp missing from HiveMetastore types parser,0,1,1
1478,SPARK-1964,SPARK-1965,Fixed,Timestamp missing from HiveMetastore types parser,Spark UI throws NPE on trying to load the app page for non-existent app,0,0,1
1479,SPARK-1965,SPARK-1966,Fixed,Spark UI throws NPE on trying to load the app page for non-existent app,Cannot cancel tasks running locally,0,0,1
1480,SPARK-1966,SPARK-1967,Won't Fix,Cannot cancel tasks running locally,Using parallelize method to create RDD; wordcount app just hanging there without errors or warnings,0,1,1
1481,SPARK-1967,SPARK-1968,Cannot Reproduce,Using parallelize method to create RDD; wordcount app just hanging there without errors or warnings,SQL commands for caching tables,0,0,1
1482,SPARK-1968,SPARK-1969,Fixed,SQL commands for caching tables,Public available online summarizer for mean; variance; min; and max,0,0,1
1483,SPARK-1969,SPARK-1970,Done,Public available online summarizer for mean; variance; min; and max,Update unit test in XORShiftRandomSuite to use ChiSquareTest from commons-math3,0,0,1
1484,SPARK-1970,SPARK-1971,Fixed,Update unit test in XORShiftRandomSuite to use ChiSquareTest from commons-math3,Update MIMA to compare against Spark 1.0.0,0,0,1
1485,SPARK-1971,SPARK-1972,Fixed,Update MIMA to compare against Spark 1.0.0,Add support for setting and visualizing custom task-related metrics,0,1,1
1486,SPARK-1972,SPARK-1973,Won't Fix,Add support for setting and visualizing custom task-related metrics,Add randomSplit to JavaRDD (with tests; and tidy Java tests),1,0,1
1487,SPARK-1973,SPARK-1974,Implemented,Add randomSplit to JavaRDD (with tests; and tidy Java tests),Most examples fail at startup because spark.master is not set,0,0,1
1488,SPARK-1974,SPARK-1975,Not A Problem,Most examples fail at startup because spark.master is not set,Spark streaming with kafka source stuck at runJob at ReceiverTracker.scala:275,0,1,1
1489,SPARK-1975,SPARK-1976,Not A Problem,Spark streaming with kafka source stuck at runJob at ReceiverTracker.scala:275,misleading streaming document,0,1,1
1490,SPARK-1976,SPARK-1977,Fixed,misleading streaming document,mutable.BitSet in ALS not serializable with KryoSerializer,0,0,1
1491,SPARK-1977,SPARK-1978,Fixed,mutable.BitSet in ALS not serializable with KryoSerializer,In some cases; spark-yarn does not automatically restart the failed container,0,0,1
1492,SPARK-1978,SPARK-1979,Fixed,In some cases; spark-yarn does not automatically restart the failed container,Error message needs to be updated when user specifies --arg ( which is no longer required to run spark submit jobs).Spark documentation needs to be updated to reflect this change.,0,0,1
1493,SPARK-1979,SPARK-1980,Not A Problem,Error message needs to be updated when user specifies --arg ( which is no longer required to run spark submit jobs).Spark documentation needs to be updated to reflect this change.,problems introduced by broadcast,0,1,1
1494,SPARK-1980,SPARK-1981,Invalid,problems introduced by broadcast,Add AWS Kinesis streaming support,0,1,1
1495,SPARK-1981,SPARK-1982,Fixed,Add AWS Kinesis streaming support,saveToParquetFile doesn't support ByteType,0,1,1
1496,SPARK-1982,SPARK-1983,Fixed,saveToParquetFile doesn't support ByteType,Expose private `inferSchema` method in SQLContext for Scala and Java API,1,1,1
1497,SPARK-1983,SPARK-1984,Fixed,Expose private `inferSchema` method in SQLContext for Scala and Java API,Maven build requires SCALA_HOME to be set even though it's not needed,0,1,1
1498,SPARK-1984,SPARK-1985,Fixed,Maven build requires SCALA_HOME to be set even though it's not needed,SPARK_HOME shouldn't be required when spark.executor.uri is provided,0,1,1
1499,SPARK-1985,SPARK-1986,Not A Problem,SPARK_HOME shouldn't be required when spark.executor.uri is provided,lib.Analytics should be in org.apache.spark.examples,0,1,1
1500,SPARK-1986,SPARK-1987,Fixed,lib.Analytics should be in org.apache.spark.examples,More memory-efficient graph construction,1,1,1
1501,SPARK-1987,SPARK-1988,Won't Fix,More memory-efficient graph construction,Enable storing edges out-of-core,1,0,1
1502,SPARK-1988,SPARK-1989,Fixed,Enable storing edges out-of-core,Exit executors faster if they get into a cycle of heavy GC,0,1,1
1503,SPARK-1990,SPARK-1991,Fixed,spark-ec2 should only need Python 2.6; not 2.7,Support custom StorageLevels for vertices and edges,0,1,1
1504,SPARK-1991,SPARK-1992,Fixed,Support custom StorageLevels for vertices and edges,Support for Pivotal HD in the Maven build,0,1,1
1505,SPARK-1992,SPARK-1993,Fixed,Support for Pivotal HD in the Maven build,Let users skip checking output directory,0,1,1
1506,SPARK-1993,SPARK-1994,Duplicate,Let users skip checking output directory,Aggregates return incorrect results on first execution,0,0,1
1507,SPARK-1994,SPARK-1995,Fixed,Aggregates return incorrect results on first execution,Add native support for UPPER() LOWER() and MIN() MAX(),1,0,1
1508,SPARK-1995,SPARK-1996,Fixed,Add native support for UPPER() LOWER() and MIN() MAX(),Remove use of special Maven repo for Akka,0,1,1
1509,SPARK-1996,SPARK-1997,Fixed,Remove use of special Maven repo for Akka,Update breeze to version 0.9,0,1,1
1510,SPARK-1997,SPARK-1998,Fixed,Update breeze to version 0.9,SparkFlumeEvent with body bigger than 1020 bytes are not read properly,0,1,1
1511,SPARK-1998,SPARK-1999,Fixed,SparkFlumeEvent with body bigger than 1020 bytes are not read properly,UI : StorageLevel in storage tab and RDD Storage Info never changes ,0,1,1
1512,SPARK-1999,SPARK-2000,Fixed,UI : StorageLevel in storage tab and RDD Storage Info never changes ,cannot connect to cluster in Standalone mode when run spark-shell in one of the cluster node without specify master,0,1,1
1513,SPARK-2000,SPARK-2001,Not A Problem,cannot connect to cluster in Standalone mode when run spark-shell in one of the cluster node without specify master,Remove docs/spark-debugger.md from master since it is obsolete,0,1,1
1514,SPARK-2001,SPARK-2002,Fixed,Remove docs/spark-debugger.md from master since it is obsolete,Race condition in accessing cache locations in DAGScheduler,0,1,1
1515,SPARK-2003,SPARK-2004,Won't Fix,SparkContext(SparkConf) doesn't work in pyspark,Automate QA of Spark Build/Deploy Matrix,0,1,1
1516,SPARK-2004,SPARK-2005,Later,Automate QA of Spark Build/Deploy Matrix,Investigate linux container-based solution,0,1,1
1517,SPARK-2005,SPARK-2006,Later,Investigate linux container-based solution,Spark Sql example throws ClassCastException: Long -> Int,0,0,1
1518,SPARK-2006,SPARK-2007,Fixed,Spark Sql example throws ClassCastException: Long -> Int,Spark on YARN picks up hadoop log4j.properties even if SPARK_LOG4J_CONF is set,0,0,1
1519,SPARK-2007,SPARK-2008,Fixed,Spark on YARN picks up hadoop log4j.properties even if SPARK_LOG4J_CONF is set,Enhance spark-ec2 to be able to add and remove slaves to an existing cluster,0,0,1
1520,SPARK-2008,SPARK-2009,Won't Fix,Enhance spark-ec2 to be able to add and remove slaves to an existing cluster,Key not found exception when slow receiver starts,0,0,1
1521,SPARK-2009,SPARK-2010,Fixed,Key not found exception when slow receiver starts,Support for nested data in PySpark SQL,0,0,1
1522,SPARK-2010,SPARK-2011,Fixed,Support for nested data in PySpark SQL,Eliminate duplicate join in Pregel,0,0,1
1523,SPARK-2011,SPARK-2012,Won't Fix,Eliminate duplicate join in Pregel,PySpark StatCounter with numpy arrays,0,1,1
1524,SPARK-2012,SPARK-2013,Fixed,PySpark StatCounter with numpy arrays,Add Python pickleFile to programming guide,0,0,1
1525,SPARK-2013,SPARK-2014,Fixed,Add Python pickleFile to programming guide,Make PySpark store RDDs in MEMORY_ONLY_SER with compression by default,0,0,1
1526,SPARK-2014,SPARK-2015,Fixed,Make PySpark store RDDs in MEMORY_ONLY_SER with compression by default,Spark UI issues at scale,0,1,1
1527,SPARK-2016,SPARK-2017,Fixed,rdd in-memory storage UI becomes unresponsive when the number of RDD partitions is large,web ui stage page becomes unresponsive when the number of tasks is large,1,1,1
1528,SPARK-2018,SPARK-2019,Fixed,Big-Endian (IBM Power7)  Spark Serialization issue,Spark workers die/disappear when job fails for nearly any reason,0,1,1
1529,SPARK-2019,SPARK-2020,Incomplete,Spark workers die/disappear when job fails for nearly any reason,Spark 1.0.0 fails to run in coarse-grained mesos mode,0,1,1
1530,SPARK-2023,SPARK-2024,Cannot Reproduce,PySpark reduce does a map side reduce and then sends the results to the driver for final reduce; instead do this more like Scala Spark.,Add saveAsSequenceFile to PySpark,1,1,1
1531,SPARK-2024,SPARK-2025,Fixed,Add saveAsSequenceFile to PySpark,EdgeRDD persists after pregel iteration,0,1,1
1532,SPARK-2025,SPARK-2026,Fixed,EdgeRDD persists after pregel iteration,"Maven ""hadoop*"" Profiles Should Set the expected Hadoop Version.",0,1,1
1533,SPARK-2028,SPARK-2029,Fixed,Let users of HadoopRDD access the partition InputSplits,Bump pom.xml version number of master branch to 1.1.0-SNAPSHOT.,0,1,1
1534,SPARK-2029,SPARK-2030,Fixed,Bump pom.xml version number of master branch to 1.1.0-SNAPSHOT.,Bump SparkBuild.scala version number of branch-1.0 to 1.0.1-SNAPSHOT.,1,1,1
1535,SPARK-2030,SPARK-2031,Fixed,Bump SparkBuild.scala version number of branch-1.0 to 1.0.1-SNAPSHOT.,DAGScheduler supports pluggable clock,0,1,1
1536,SPARK-2031,SPARK-2032,Fixed,DAGScheduler supports pluggable clock,Add an RDD.samplePartitions method for partition-level sampling,1,0,1
1537,SPARK-2032,SPARK-2033,Won't Fix,Add an RDD.samplePartitions method for partition-level sampling,Automatically cleanup checkpoint ,1,0,1
1538,SPARK-2033,SPARK-2034,Fixed,Automatically cleanup checkpoint ,KafkaInputDStream doesn't close resources and may prevent JVM shutdown,0,1,1
1539,SPARK-2034,SPARK-2035,Fixed,KafkaInputDStream doesn't close resources and may prevent JVM shutdown,Make a stage's call stack available on the UI,0,0,1
1540,SPARK-2035,SPARK-2036,Fixed,Make a stage's call stack available on the UI,CaseConversionExpression should check if the evaluated value is null.,0,0,1
1541,SPARK-2036,SPARK-2037,Fixed,CaseConversionExpression should check if the evaluated value is null.,yarn client mode doesn't support spark.yarn.max.executor.failures,0,1,1
1542,SPARK-2037,SPARK-2038,Fixed,yarn client mode doesn't support spark.yarn.max.executor.failures,"Don't shadow ""conf"" variable in saveAsHadoop functions",0,0,1
1543,SPARK-2038,SPARK-2039,Fixed,"Don't shadow ""conf"" variable in saveAsHadoop functions",Run hadoop output checks for all formats,1,0,1
1544,SPARK-2039,SPARK-2040,Fixed,Run hadoop output checks for all formats,[MLLIB] Univariate kernel density estimation,0,1,1
1545,SPARK-2041,SPARK-2042,Fixed,Exception when querying when tableName == columnName,Take triggers unneeded shuffle.,1,1,1
1546,SPARK-2042,SPARK-2043,Fixed,Take triggers unneeded shuffle.,ExternalAppendOnlyMap doesn't always find matching keys,0,0,1
1547,SPARK-2043,SPARK-2044,Fixed,ExternalAppendOnlyMap doesn't always find matching keys,Pluggable interface for shuffles,0,0,1
1548,SPARK-2044,SPARK-2045,Fixed,Pluggable interface for shuffles,Sort-based shuffle implementation,1,1,1
1549,SPARK-2045,SPARK-2046,Fixed,Sort-based shuffle implementation,Support config properties that are changeable across tasks/stages within a job,0,1,1
1550,SPARK-2046,SPARK-2047,Won't Fix,Support config properties that are changeable across tasks/stages within a job,Use less memory in AppendOnlyMap.destructiveSortedIterator,1,1,1
1551,SPARK-2047,SPARK-2048,Fixed,Use less memory in AppendOnlyMap.destructiveSortedIterator,Optimizations to CPU usage of external spilling code,1,1,1
1552,SPARK-2048,SPARK-2049,Fixed,Optimizations to CPU usage of external spilling code,avg function in aggregation may cause overflow ,0,1,1
1553,SPARK-2050,SPARK-2051,Fixed,LIKE; RLIKE; IN; BETWEEN and DIV in HQL should not be case sensitive,spark.yarn.dist.* configs are not supported in yarn-cluster mode,0,1,1
1554,SPARK-2052,SPARK-2053,Fixed,Add optimization for CaseConversionExpression's.,Add Catalyst expression for CASE WHEN,1,1,1
1555,SPARK-2053,SPARK-2054,Fixed,Add Catalyst expression for CASE WHEN,Code Generation for Expression Evaluation,1,0,1
1556,SPARK-2054,SPARK-2055,Fixed,Code Generation for Expression Evaluation,bin$ ./run-example is bad.  must run SPARK_HOME$ bin/run-example. look at the file run-example at line 54.,0,0,1
1557,SPARK-2055,SPARK-2056,Invalid,bin$ ./run-example is bad.  must run SPARK_HOME$ bin/run-example. look at the file run-example at line 54.,Set RDD name to input path,0,0,1
1558,SPARK-2056,SPARK-2057,Fixed,Set RDD name to input path,run-example can only be run within spark_home,0,0,1
1559,SPARK-2057,SPARK-2058,Fixed,run-example can only be run within spark_home,SPARK_CONF_DIR should override all present configs,0,0,1
1560,SPARK-2059,SPARK-2060,Fixed,Unresolved Attributes should cause a failure before execution time,Querying JSON Datasets with SQL and DSL in Spark SQL,1,1,1
1561,SPARK-2060,SPARK-2061,Fixed,Querying JSON Datasets with SQL and DSL in Spark SQL,Deprecate `splits` in JavaRDDLike and add `partitions`,0,0,1
1562,SPARK-2061,SPARK-2062,Fixed,Deprecate `splits` in JavaRDDLike and add `partitions`,VertexRDD.apply does not use the mergeFunc,0,0,1
1563,SPARK-2062,SPARK-2063,Fixed,VertexRDD.apply does not use the mergeFunc,Creating a SchemaRDD via sql() does not correctly resolve nested types,0,1,1
1564,SPARK-2065,SPARK-2066,Fixed,Have spark-ec2 set EC2 instance names,Better error message for non-aggregated attributes with aggregates,0,0,1
1565,SPARK-2066,SPARK-2067,Fixed,Better error message for non-aggregated attributes with aggregates,Spark logo in application UI uses absolute path,0,0,1
1566,SPARK-2067,SPARK-2068,Fixed,Spark logo in application UI uses absolute path,Remove other uses of @transient lazy val in physical plan nodes,0,0,1
1567,SPARK-2068,SPARK-2069,Won't Fix,Remove other uses of @transient lazy val in physical plan nodes,MIMA false positives (umbrella),0,0,1
1568,SPARK-2069,SPARK-2070,Fixed,MIMA false positives (umbrella),Package private methods are not excluded correctly,1,0,1
1569,SPARK-2070,SPARK-2071,Fixed,Package private methods are not excluded correctly,Package private classes that are deleted from an older version of Spark trigger errors,1,1,1
1570,SPARK-2071,SPARK-2072,Fixed,Package private classes that are deleted from an older version of Spark trigger errors,Streaming not processing a file with particular number of entries,0,0,1
1571,SPARK-2072,SPARK-2073,Cannot Reproduce,Streaming not processing a file with particular number of entries,Streaming not processing a file with particular number of entries,1,1,1
1572,SPARK-2073,SPARK-2074,Duplicate,Streaming not processing a file with particular number of entries,Streaming not processing a file with particular number of entries,1,1,1
1573,SPARK-2074,SPARK-2075,Duplicate,Streaming not processing a file with particular number of entries,Anonymous classes are missing from Spark distribution,0,0,1
1574,SPARK-2075,SPARK-2076,Fixed,Anonymous classes are missing from Spark distribution,Push Down the Predicate & Join Filter for OuterJoin,0,0,1
1575,SPARK-2076,SPARK-2077,Fixed,Push Down the Predicate & Join Filter for OuterJoin,Log serializer in use on application startup,0,0,1
1576,SPARK-2077,SPARK-2078,Fixed,Log serializer in use on application startup,Use ISO8601 date formats in logging,1,1,1
1577,SPARK-2078,SPARK-2079,Won't Fix,Use ISO8601 date formats in logging,Support batching when serializing SchemaRDD to Python,0,1,1
1578,SPARK-2079,SPARK-2080,Fixed,Support batching when serializing SchemaRDD to Python,Yarn: history UI link missing; wrong reported user,0,1,1
1579,SPARK-2080,SPARK-2081,Fixed,Yarn: history UI link missing; wrong reported user,Undefine output() from the abstract class Command and implement it in concrete subclasses,0,0,1
1580,SPARK-2081,SPARK-2082,Fixed,Undefine output() from the abstract class Command and implement it in concrete subclasses,Stratified sampling implementation in PairRDDFunctions,0,0,1
1581,SPARK-2083,SPARK-2084,Won't Fix,Allow local task to retry after failure.,Mention SPARK_JAR in env var section on configuration page,0,0,1
1582,SPARK-2084,SPARK-2085,Won't Fix,Mention SPARK_JAR in env var section on configuration page,Apply user-specific regularization instead of uniform regularization in Alternating Least Squares (ALS),0,0,1
1583,SPARK-2085,SPARK-2086,Implemented,Apply user-specific regularization instead of uniform regularization in Alternating Least Squares (ALS),Improve output of toDebugString to make shuffle boundaries more clear,0,1,1
1584,SPARK-2086,SPARK-2087,Fixed,Improve output of toDebugString to make shuffle boundaries more clear,Clean Multi-user semantics for thrift JDBC/ODBC server.,0,1,1
1585,SPARK-2088,SPARK-2089,Fixed,NPE in toString when creationSiteInfo is null after deserialization,With YARN; preferredNodeLocalityData isn't honored ,0,1,1
1586,SPARK-2089,SPARK-2090,Won't Fix,With YARN; preferredNodeLocalityData isn't honored ,spark-shell input text entry not showing on REPL,0,1,1
1587,SPARK-2090,SPARK-2091,Not A Problem,spark-shell input text entry not showing on REPL,pyspark/mllib is not compatible with numpy-1.4,0,0,1
1588,SPARK-2091,SPARK-2092,Fixed,pyspark/mllib is not compatible with numpy-1.4,This is a test issue,0,1,1
1589,SPARK-2092,SPARK-2093,Not A Problem,This is a test issue,NullPropagation should use exact type value.,1,1,1
1590,SPARK-2093,SPARK-2094,Fixed,NullPropagation should use exact type value.,Ensure exactly once semantics for DDL / Commands,0,1,1
1591,SPARK-2094,SPARK-2095,Fixed,Ensure exactly once semantics for DDL / Commands,sc.getExecutorCPUCounts(),0,0,1
1592,SPARK-2095,SPARK-2096,Won't Fix,sc.getExecutorCPUCounts(),Correctly parse dot notations for accessing an array of structs,0,1,1
1593,SPARK-2096,SPARK-2097,Fixed,Correctly parse dot notations for accessing an array of structs,UDF Support,1,0,1
1594,SPARK-2097,SPARK-2098,Fixed,UDF Support,All Spark processes should support spark-defaults.conf; config file,0,0,1
1595,SPARK-2098,SPARK-2099,Fixed,All Spark processes should support spark-defaults.conf; config file,Report TaskMetrics for running tasks,1,0,1
1596,SPARK-2099,SPARK-2100,Fixed,Report TaskMetrics for running tasks,Allow users to disable Jetty Spark UI in local mode,0,0,1
1597,SPARK-2101,SPARK-2102,Fixed,Python unit tests fail on Python 2.6 because of lack of unittest.skipIf(),Caching with GENERIC column type causes query execution to slow down significantly,0,1,1
1598,SPARK-2102,SPARK-2103,Fixed,Caching with GENERIC column type causes query execution to slow down significantly,Java + Kafka + Spark Streaming NoSuchMethodError in java.lang.Object.<init>,0,1,1
1599,SPARK-2103,SPARK-2104,Fixed,Java + Kafka + Spark Streaming NoSuchMethodError in java.lang.Object.<init>,RangePartitioner should use user specified serializer to serialize range bounds,0,1,1
1600,SPARK-2104,SPARK-2105,Fixed,RangePartitioner should use user specified serializer to serialize range bounds,SparkUI doesn't remove active stages that failed,0,1,1
1601,SPARK-2105,SPARK-2106,Fixed,SparkUI doesn't remove active stages that failed,Unify the HiveContext,0,1,1
1602,SPARK-2106,SPARK-2107,Won't Fix,Unify the HiveContext,FilterPushdownSuite imports Junit and leads to compilation error,0,0,1
1603,SPARK-2107,SPARK-2108,Fixed,FilterPushdownSuite imports Junit and leads to compilation error,Mark SparkContext methods that return block information as developer API's,0,0,1
1604,SPARK-2108,SPARK-2109,Fixed,Mark SparkContext methods that return block information as developer API's,Setting SPARK_MEM for bin/pyspark does not work. ,1,0,1
1605,SPARK-2109,SPARK-2110,Fixed,Setting SPARK_MEM for bin/pyspark does not work. ,Misleading help displayed for interactive mode pyspark --help,0,0,1
1606,SPARK-2110,SPARK-2111,Fixed,Misleading help displayed for interactive mode pyspark --help,pyspark errors when SPARK_PRINT_LAUNCH_COMMAND=1,0,0,1
1607,SPARK-2111,SPARK-2112,Fixed,pyspark errors when SPARK_PRINT_LAUNCH_COMMAND=1,ParquetTypesConverter should not create its own conf,0,1,1
1608,SPARK-2112,SPARK-2113,Fixed,ParquetTypesConverter should not create its own conf,awaitTermination() after stop() will hang in Spark Stremaing,0,1,1
1609,SPARK-2113,SPARK-2114,Fixed,awaitTermination() after stop() will hang in Spark Stremaing,groupByKey and joins on raw data,0,1,1
1610,SPARK-2115,SPARK-2116,Fixed,Stage kill link is too close to stage details link,Load spark-defaults.conf from directory specified by SPARK_CONF_DIR,0,0,1
1611,SPARK-2116,SPARK-2117,Fixed,Load spark-defaults.conf from directory specified by SPARK_CONF_DIR,UTF8 Characters Break PySpark,0,0,1
1612,SPARK-2117,SPARK-2118,Won't Fix,UTF8 Characters Break PySpark,If tools jar is not present; MIMA build should exit with an exception,0,1,1
1613,SPARK-2118,SPARK-2119,Fixed,If tools jar is not present; MIMA build should exit with an exception,Reading Parquet InputSplits dominates query execution time when reading off S3,0,0,1
1614,SPARK-2119,SPARK-2120,Fixed,Reading Parquet InputSplits dominates query execution time when reading off S3,Not fully cached when there is enough memory,0,0,1
1615,SPARK-2120,SPARK-2121,Duplicate,Not fully cached when there is enough memory,Not fully cached when there is enough memory in ALS,1,1,1
1616,SPARK-2121,SPARK-2122,Not A Problem,Not fully cached when there is enough memory in ALS,Move aggregation into shuffle implementation,0,1,1
1617,SPARK-2123,SPARK-2124,Fixed,Basic pluggable interface for shuffle,Move aggregation into ShuffleManager implementations,1,1,1
1618,SPARK-2125,SPARK-2126,Fixed,Add sorting flag to ShuffleManager; and implement it in HashShuffleManager,Move MapOutputTracker behind ShuffleManager interface,1,1,1
1619,SPARK-2126,SPARK-2127,Won't Fix,Move MapOutputTracker behind ShuffleManager interface,Use application specific folders to dump metrics via CsvSink,0,0,1
1620,SPARK-2128,SPARK-2129,Fixed,No plan for DESCRIBE,NPE thrown while lookup a view,1,1,1
1621,SPARK-2129,SPARK-2130,Won't Fix,NPE thrown while lookup a view,Clarify PySpark docs for RDD.getStorageLevel,0,0,1
1622,SPARK-2130,SPARK-2131,Fixed,Clarify PySpark docs for RDD.getStorageLevel,Collect per-task filesystem-bytes-read/written metrics,0,0,1
1623,SPARK-2131,SPARK-2132,Duplicate,Collect per-task filesystem-bytes-read/written metrics,Color GC time red when over a percentage of task time,0,0,1
1624,SPARK-2133,SPARK-2134,Cannot Reproduce,FileNotFoundException in BlockObjectWriter,Report metrics before application finishes,1,0,1
1625,SPARK-2134,SPARK-2135,Fixed,Report metrics before application finishes,InMemoryColumnarScan does not get planned correctly,0,0,1
1626,SPARK-2135,SPARK-2136,Fixed,InMemoryColumnarScan does not get planned correctly,Spark SQL does not disply the job description on web ui/ event log,1,0,1
1627,SPARK-2136,SPARK-2137,Fixed,Spark SQL does not disply the job description on web ui/ event log,Timestamp UDFs broken,1,0,1
1628,SPARK-2137,SPARK-2138,Fixed,Timestamp UDFs broken,The KMeans algorithm in the MLlib can lead to the Serialized Task size become bigger and bigger,0,1,1
1629,SPARK-2138,SPARK-2139,Not A Problem,The KMeans algorithm in the MLlib can lead to the Serialized Task size become bigger and bigger,spark.yarn.dist.* configs are not documented,0,1,1
1630,SPARK-2141,SPARK-2142,Unresolved,Add sc.getPersistentRDDs() to PySpark,Give better indicator of how GC cuts into task time,0,1,1
1631,SPARK-2142,SPARK-2143,Not A Problem,Give better indicator of how GC cuts into task time,Display Spark version on Driver web page,1,0,1
1632,SPARK-2143,SPARK-2144,Fixed,Display Spark version on Driver web page,SparkUI Executors tab displays incorrect RDD blocks,0,0,1
1633,SPARK-2144,SPARK-2145,Fixed,SparkUI Executors tab displays incorrect RDD blocks,Add lower bound on sampling rate to guarantee sampling performance,0,0,1
1634,SPARK-2145,SPARK-2146,Fixed,Add lower bound on sampling rate to guarantee sampling performance,Fix the takeOrdered doc,0,1,1
1635,SPARK-2146,SPARK-2147,Fixed,Fix the takeOrdered doc,Master UI forgets about Executors when application exits cleanly,0,0,1
1636,SPARK-2147,SPARK-2148,Fixed,Master UI forgets about Executors when application exits cleanly,Document custom class as key needing equals() AND hashcode(),0,1,1
1637,SPARK-2148,SPARK-2149,Fixed,Document custom class as key needing equals() AND hashcode(),[Core] Disable partial aggregation automatically when reduction factor is low,0,1,1
1638,SPARK-2149,SPARK-2150,Fixed,[Core] Disable partial aggregation automatically when reduction factor is low,Provide direct link to finished application UI in yarn resource manager UI,0,0,1
1639,SPARK-2150,SPARK-2151,Fixed,Provide direct link to finished application UI in yarn resource manager UI,spark-submit issue (int format expected for memory parameter),0,0,1
1640,SPARK-2151,SPARK-2152,Fixed,spark-submit issue (int format expected for memory parameter),the error of comput rightNodeAgg about  Decision tree algorithm  in Spark MLlib ,1,1,1
